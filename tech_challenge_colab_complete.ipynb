{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63592246",
   "metadata": {},
   "source": [
    "# Tech Challenge - Fine-tuning para Produtos Amazon (GPU Colab)\n",
    "\n",
    "**Objetivo**: Executar fine-tuning de um foundation model usando o dataset AmazonTitles-1.3MM para gerar descri√ß√µes de produtos baseadas em t√≠tulos.\n",
    "\n",
    "**Dataset**: Utilizaremos o arquivo `trn.json.gz` que cont√©m t√≠tulos e descri√ß√µes de produtos da Amazon.\n",
    "\n",
    "**Modelo Escolhido**: TinyLlama 1.1B com Unsloth para otimiza√ß√£o de treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice\n",
    "1. [Configura√ß√£o Inicial](#1-configuracao-inicial)\n",
    "2. [Explora√ß√£o dos Dados](#2-exploracao-dos-dados)\n",
    "3. [Prepara√ß√£o do Dataset](#3-preparacao-do-dataset)\n",
    "4. [Teste do Modelo Base](#4-teste-do-modelo-base)\n",
    "5. [Fine-tuning](#5-fine-tuning)\n",
    "6. [Teste do Modelo Treinado](#6-teste-do-modelo-treinado)\n",
    "7. [Demonstra√ß√£o Interativa](#7-demonstracao-interativa)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83012f35",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o Inicial\n",
    "\n",
    "### 1.1 Montagem do Google Drive\n",
    "Primeiro, vamos montar o Google Drive para acessar e salvar nossos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d402b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Monta o Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define o diret√≥rio de trabalho (usando o mesmo diret√≥rio onde est√° o arquivo de dados)\n",
    "WORK_DIR = '/content/drive/MyDrive/FineTunning/TechChallenge03'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive montado com sucesso!\")\n",
    "print(f\"üìÅ Diret√≥rio de trabalho: {WORK_DIR}\")\n",
    "\n",
    "# Verifica se o diret√≥rio existe e lista os arquivos\n",
    "if os.path.exists(WORK_DIR):\n",
    "    files_in_dir = os.listdir(WORK_DIR)\n",
    "    print(f\"üìã Arquivos no diret√≥rio: {files_in_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Diret√≥rio n√£o existe, ser√° criado: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e2dbe",
   "metadata": {},
   "source": [
    "### 1.2 Instala√ß√£o das Depend√™ncias\n",
    "\n",
    "Instalamos as bibliotecas necess√°rias:\n",
    "- **Unsloth**: Otimiza√ß√£o para fine-tuning eficiente\n",
    "- **Transformers**: Biblioteca principal para modelos de linguagem\n",
    "- **Datasets**: Para manipula√ß√£o de datasets\n",
    "- **TRL**: Para treinamento de modelos de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das depend√™ncias principais\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets torch\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b371307",
   "metadata": {},
   "source": [
    "### 1.3 Importa√ß√£o das Bibliotecas e Configura√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eea050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(f\"üî• CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "print(f\"üíæ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N√£o dispon√≠vel'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc0362",
   "metadata": {},
   "source": [
    "### 1.4 Configura√ß√µes do Modelo e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d341a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes principais\n",
    "CONFIG = {\n",
    "    # Configura√ß√µes do modelo\n",
    "    'model_name': \"unsloth/tinyllama-bnb-4bit\",\n",
    "    'max_seq_length': 1024,\n",
    "    'dtype': None,  # Ser√° determinado automaticamente\n",
    "    'load_in_4bit': True,\n",
    "    \n",
    "    # Configura√ß√µes do dataset\n",
    "    'data_file': '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz',\n",
    "    'sample_size': 50000,  # N√∫mero de amostras para treinamento (otimizado para Colab)\n",
    "    'test_size': 5000,  # N√∫mero de amostras para teste\n",
    "    \n",
    "    # Configura√ß√µes do fine-tuning\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0,\n",
    "    'max_steps': 100,  # Ajuste conforme necess√°rio\n",
    "    'learning_rate': 2e-4,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    \n",
    "    # Caminhos - usando o mesmo diret√≥rio base\n",
    "    'base_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03',\n",
    "    'output_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03/outputs',\n",
    "    'model_save_path': '/content/drive/MyDrive/FineTunning/TechChallenge03/amazon_model',\n",
    "}\n",
    "\n",
    "# Cria√ß√£o dos diret√≥rios\n",
    "os.makedirs(CONFIG['base_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['model_save_path'], exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√µes definidas:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513667ac",
   "metadata": {},
   "source": [
    "## 2. Explora√ß√£o dos Dados\n",
    "\n",
    "### 2.1 Upload do Arquivo de Dados\n",
    "\n",
    "Primeiro, voc√™ precisa fazer upload do arquivo `trn.json.gz` para o Colab.\n",
    "Execute a c√©lula abaixo e fa√ßa upload do arquivo quando solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define o caminho para o arquivo no Google Drive\n",
    "DATA_FILE_PATH = '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz'\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "if os.path.exists(DATA_FILE_PATH):\n",
    "    print(\"‚úÖ Arquivo trn.json.gz encontrado no Google Drive!\")\n",
    "    print(f\"üìÅ Caminho: {DATA_FILE_PATH}\")\n",
    "    \n",
    "    # Verifica o tamanho do arquivo\n",
    "    file_size = os.path.getsize(DATA_FILE_PATH)\n",
    "    print(f\"üìä Tamanho do arquivo: {file_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Arquivo n√£o encontrado no caminho especificado.\")\n",
    "    print(f\"‚ùå Caminho verificado: {DATA_FILE_PATH}\")\n",
    "    print(\"üí° Certifique-se de que o arquivo trn.json.gz est√° no diret√≥rio correto do Google Drive.\")\n",
    "    DATA_FILE_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097baa95",
   "metadata": {},
   "source": [
    "### 2.2 Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef294f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sample(file_path, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Carrega uma amostra dos dados do arquivo JSON comprimido\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    print(f\"üîÑ Carregando dados de {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    json_obj = json.loads(line.strip())\n",
    "                    data.append(json_obj)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 10000 == 0:\n",
    "                    print(f\"üìä Carregados {i + 1} registros...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar dados: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Carregamento conclu√≠do! Total de registros: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "# Carrega uma amostra dos dados para explora√ß√£o\n",
    "if DATA_FILE_PATH and os.path.exists(DATA_FILE_PATH):\n",
    "    sample_data = load_data_sample(DATA_FILE_PATH, sample_size=50000)\n",
    "    print(f\"üìã Amostra carregada com {len(sample_data)} registros\")\n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel carregar os dados. Verifique o arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad18e8",
   "metadata": {},
   "source": [
    "### 2.3 An√°lise Explorat√≥ria dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_data:\n",
    "    print(\"üîç An√°lise dos dados carregados:\\n\")\n",
    "    \n",
    "    # Estrutura dos dados\n",
    "    print(\"üìã Estrutura dos dados:\")\n",
    "    if sample_data:\n",
    "        print(f\"  Primeiro registro: {sample_data[0]}\")\n",
    "        print(f\"  Chaves dispon√≠veis: {list(sample_data[0].keys())}\")\n",
    "    \n",
    "    # Converte para DataFrame para an√°lise\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"\\nüìä Estat√≠sticas b√°sicas:\")\n",
    "    print(f\"  Total de registros: {len(df)}\")\n",
    "    print(f\"  Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    # An√°lise dos t√≠tulos\n",
    "    if 'title' in df.columns:\n",
    "        title_lengths = df['title'].str.len()\n",
    "        print(f\"\\nüìè An√°lise dos t√≠tulos:\")\n",
    "        print(f\"  Comprimento m√©dio: {title_lengths.mean():.1f} caracteres\")\n",
    "        print(f\"  Comprimento mediano: {title_lengths.median():.1f} caracteres\")\n",
    "        print(f\"  M√≠nimo: {title_lengths.min()} caracteres\")\n",
    "        print(f\"  M√°ximo: {title_lengths.max()} caracteres\")\n",
    "    \n",
    "    # An√°lise das descri√ß√µes\n",
    "    if 'content' in df.columns:\n",
    "        content_lengths = df['content'].str.len()\n",
    "        print(f\"\\nüìù An√°lise das descri√ß√µes:\")\n",
    "        print(f\"  Comprimento m√©dio: {content_lengths.mean():.1f} caracteres\")\n",
    "        print(f\"  Comprimento mediano: {content_lengths.median():.1f} caracteres\")\n",
    "        print(f\"  M√≠nimo: {content_lengths.min()} caracteres\")\n",
    "        print(f\"  M√°ximo: {content_lengths.max()} caracteres\")\n",
    "    \n",
    "    # Mostra alguns exemplos\n",
    "    print(f\"\\nüìù Exemplos de dados:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "        for col in df.columns:\n",
    "            content = str(df[col].iloc[i])\n",
    "            if len(content) > 100:\n",
    "                content = content[:100] + \"...\"\n",
    "            print(f\"  {col}: {content}\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para an√°lise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090237b2",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o do Dataset\n",
    "\n",
    "### 3.1 Tratamento e Limpeza dos Dados\n",
    "\n",
    "Vamos implementar um sistema robusto de limpeza dos dados que j√° provou ser eficaz, garantindo que apenas dados de alta qualidade sejam usados no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db32e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def is_valid_text(text, min_length=10, max_length=500):\n",
    "    \"\"\"Verifica se o texto √© v√°lido para treinamento\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text = text.strip()\n",
    "    if len(text) < min_length or len(text) > max_length:\n",
    "        return False\n",
    "    \n",
    "    # Verifica se tem conte√∫do significativo (n√£o apenas s√≠mbolos)\n",
    "    if len(re.sub(r'[^a-zA-Z0-9\\s]', '', text).strip()) < min_length // 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpa e padroniza o texto\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove caracteres especiais excessivos\n",
    "    text = re.sub(r'[^\\w\\s\\-.,!?()&]', ' ', text)\n",
    "    \n",
    "    # Remove espa√ßos m√∫ltiplos\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def prepare_training_data(data, sample_size=50000):\n",
    "    \"\"\"\n",
    "    Prepara os dados para treinamento com limpeza avan√ßada\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Processando dados para treinamento...\")\n",
    "    \n",
    "    processed_data = []\n",
    "    rejected_count = 0\n",
    "    \n",
    "    for i, item in enumerate(data[:sample_size]):\n",
    "        try:\n",
    "            # Extrai t√≠tulo e conte√∫do\n",
    "            title = item.get('title', '').strip()\n",
    "            content = item.get('content', '').strip()\n",
    "            \n",
    "            # Limpa os textos\n",
    "            title_clean = clean_text(title)\n",
    "            content_clean = clean_text(content)\n",
    "            \n",
    "            # Valida qualidade\n",
    "            if (is_valid_text(title_clean, min_length=5, max_length=200) and \n",
    "                is_valid_text(content_clean, min_length=20, max_length=800)):\n",
    "                \n",
    "                # Formata no padr√£o de instruction-following\n",
    "                formatted_text = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title_clean}\n",
    "\n",
    "### Response:\n",
    "{content_clean}\"\"\"\n",
    "                \n",
    "                processed_data.append({\n",
    "                    'text': formatted_text,\n",
    "                    'title': title_clean,\n",
    "                    'content': content_clean\n",
    "                })\n",
    "            else:\n",
    "                rejected_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            rejected_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            approval_rate = (len(processed_data) / (i + 1)) * 100\n",
    "            print(f\"üìä Processados {i + 1} | Aprovados: {len(processed_data)} ({approval_rate:.1f}%)\")\n",
    "    \n",
    "    final_approval_rate = (len(processed_data) / len(data[:sample_size])) * 100\n",
    "    print(f\"\\n‚úÖ Processamento conclu√≠do!\")\n",
    "    print(f\"üìà Taxa de aprova√ß√£o final: {final_approval_rate:.1f}%\")\n",
    "    print(f\"‚úÖ Dados aprovados: {len(processed_data)}\")\n",
    "    print(f\"‚ùå Dados rejeitados: {rejected_count}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Processa os dados\n",
    "if sample_data:\n",
    "    training_data = prepare_training_data(sample_data, CONFIG['sample_size'])\n",
    "    print(f\"\\nüéØ Dataset final: {len(training_data)} amostras prontas para treinamento\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para processamento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7badf",
   "metadata": {},
   "source": [
    "### 3.2 Cria√ß√£o do Dataset do Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_data:\n",
    "    # Converte para formato do Hugging Face\n",
    "    dataset_dict = {'text': [item['text'] for item in training_data]}\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset criado com sucesso!\")\n",
    "    print(f\"üìä Total de amostras: {len(dataset)}\")\n",
    "    print(f\"üìè Comprimento m√©dio do texto: {np.mean([len(text) for text in dataset_dict['text']]):.0f} caracteres\")\n",
    "    \n",
    "    # Mostra exemplo do formato\n",
    "    print(f\"\\nüìù Exemplo do formato de treinamento:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(dataset[0]['text'][:500] + \"...\" if len(dataset[0]['text']) > 500 else dataset[0]['text'])\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel criar o dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831aba6",
   "metadata": {},
   "source": [
    "## 4. Teste do Modelo Base\n",
    "\n",
    "### 4.1 Carregamento do Modelo TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ebacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo base TinyLlama\n",
    "print(\"üîÑ Carregando modelo TinyLlama...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    max_seq_length=CONFIG['max_seq_length'],\n",
    "    dtype=CONFIG['dtype'],\n",
    "    load_in_4bit=CONFIG['load_in_4bit'],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo TinyLlama carregado com sucesso!\")\n",
    "print(f\"üìä Modelo: {CONFIG['model_name']}\")\n",
    "print(f\"üìè Comprimento m√°ximo de sequ√™ncia: {CONFIG['max_seq_length']}\")\n",
    "print(f\"üîß Quantiza√ß√£o 4-bit: {CONFIG['load_in_4bit']}\")\n",
    "\n",
    "# Informa√ß√µes sobre o modelo\n",
    "print(f\"\\nüìà Estat√≠sticas do modelo:\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Par√¢metros totais: {total_params:,}\")\n",
    "print(f\"  Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "print(f\"  Tamanho do vocabul√°rio: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9d88e",
   "metadata": {},
   "source": [
    "### 4.2 Teste do Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para testar o modelo\n",
    "def test_model(model, tokenizer, prompt, max_new_tokens=150):\n",
    "    \"\"\"Testa o modelo com um prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Configura√ß√µes de gera√ß√£o\n",
    "    generation_config = {\n",
    "        'max_new_tokens': max_new_tokens,\n",
    "        'temperature': 0.7,\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.9,\n",
    "        'pad_token_id': tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generation_config)\n",
    "    \n",
    "    # Decodifica apenas os tokens gerados (n√£o inclui o prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Teste com um exemplo\n",
    "test_prompt = \"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "Wireless Bluetooth Headphones with Noise Cancellation\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "print(\"üß™ Testando modelo base (antes do fine-tuning):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(\"\\nResposta do modelo base:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    base_response = test_model(model, tokenizer, test_prompt)\n",
    "    print(base_response)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no teste: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e3861",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning\n",
    "\n",
    "### 5.1 Configura√ß√£o do LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32353c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o do LoRA (Low-Rank Adaptation)\n",
    "print(\"üîß Configurando LoRA para fine-tuning...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG['lora_r'],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA configurado com sucesso!\")\n",
    "\n",
    "# Mostra estat√≠sticas dos par√¢metros trein√°veis\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas ap√≥s configura√ß√£o LoRA:\")\n",
    "print(f\"  Par√¢metros totais: {total_params:,}\")\n",
    "print(f\"  Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "print(f\"  Percentual trein√°vel: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"üéØ LoRA configurado com r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064813e1",
   "metadata": {},
   "source": [
    "### 5.2 Configura√ß√£o do Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf567e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    warmup_steps=5,\n",
    "    max_steps=CONFIG['max_steps'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    save_steps=25,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Argumentos de treinamento configurados:\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Accumulation steps: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"  Precision: {'BF16' if is_bfloat16_supported() else 'FP16'}\")\n",
    "print(f\"  Output dir: {CONFIG['output_dir']}\")\n",
    "\n",
    "# Configura√ß√£o do trainer\n",
    "if 'dataset' in locals():\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=CONFIG['max_seq_length'],\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer configurado com sucesso!\")\n",
    "    print(f\"üìä Dataset de treinamento: {len(dataset)} amostras\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o dispon√≠vel para configura√ß√£o do trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc10b6",
   "metadata": {},
   "source": [
    "### 5.3 Execu√ß√£o do Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a426523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execu√ß√£o do treinamento\n",
    "print(\"üöÄ Iniciando fine-tuning...\")\n",
    "print(\"‚è±Ô∏è Isso pode levar alguns minutos dependendo da configura√ß√£o da GPU...\")\n",
    "\n",
    "if 'trainer' in locals():\n",
    "    try:\n",
    "        # Mostra estat√≠sticas da GPU antes do treinamento\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üî• GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"üíæ Mem√≥ria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        \n",
    "        # Executa o treinamento\n",
    "        trainer_stats = trainer.train()\n",
    "        \n",
    "        print(\"‚úÖ Fine-tuning conclu√≠do com sucesso!\")\n",
    "        print(f\"üìä Estat√≠sticas do treinamento:\")\n",
    "        print(f\"  Steps totais: {trainer_stats.global_step}\")\n",
    "        print(f\"  Loss final: {trainer_stats.training_loss:.4f}\")\n",
    "        print(f\"  Tempo total: {trainer_stats.metrics.get('train_runtime', 0):.2f} segundos\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante o treinamento: {str(e)}\")\n",
    "        print(\"üí° Dica: Verifique se h√° mem√≥ria GPU suficiente ou reduza o batch size.\")\n",
    "else:\n",
    "    print(\"‚ùå Trainer n√£o configurado. Execute as c√©lulas anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17795d2",
   "metadata": {},
   "source": [
    "## 6. Teste do Modelo Treinado\n",
    "\n",
    "### 6.1 Salvar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b75bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo treinado\n",
    "print(\"üíæ Salvando o modelo treinado...\")\n",
    "\n",
    "try:\n",
    "    # Salva o modelo LoRA\n",
    "    model.save_pretrained(CONFIG['model_save_path'])\n",
    "    tokenizer.save_pretrained(CONFIG['model_save_path'])\n",
    "    \n",
    "    print(f\"‚úÖ Modelo salvo com sucesso em: {CONFIG['model_save_path']}\")\n",
    "    \n",
    "    # Lista os arquivos salvos\n",
    "    import os\n",
    "    saved_files = os.listdir(CONFIG['model_save_path'])\n",
    "    print(f\"üìÅ Arquivos salvos: {saved_files}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar o modelo: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c75af",
   "metadata": {},
   "source": [
    "### 6.2 Teste do Modelo Fine-tunado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o modelo ap√≥s o fine-tuning\n",
    "test_prompts = [\n",
    "    \"Wireless Bluetooth Headphones with Noise Cancellation\",\n",
    "    \"Professional Gaming Keyboard with RGB Lighting\",\n",
    "    \"Stainless Steel Water Bottle 32oz\",\n",
    "    \"Organic Cotton T-Shirt for Men\",\n",
    "    \"Smart Fitness Tracker with Heart Rate Monitor\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testando modelo ap√≥s fine-tuning:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, title in enumerate(test_prompts, 1):\n",
    "    test_prompt = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîπ Teste {i}: {title}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        response = test_model(model, tokenizer, test_prompt, max_new_tokens=200)\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no teste: {str(e)}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676c9d6",
   "metadata": {},
   "source": [
    "## 7. Demonstra√ß√£o Interativa\n",
    "\n",
    "### 7.1 Interface de Teste Interativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_description(title, max_length=200, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Gera descri√ß√£o de produto baseada no t√≠tulo\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Configura par√¢metros de gera√ß√£o\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        generation_config = {\n",
    "            'max_new_tokens': max_length,\n",
    "            'temperature': temperature,\n",
    "            'do_sample': True,\n",
    "            'top_p': 0.9,\n",
    "            'pad_token_id': tokenizer.eos_token_id,\n",
    "            'repetition_penalty': 1.1\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **generation_config)\n",
    "        \n",
    "        # Decodifica apenas a resposta gerada\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Erro na gera√ß√£o: {str(e)}\"\n",
    "\n",
    "# Interface interativa simples\n",
    "print(\"üéØ Interface de Teste Interativo do Modelo\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Digite t√≠tulos de produtos para gerar descri√ß√µes!\")\n",
    "print(\"(Digite 'sair' para encerrar)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fun√ß√£o para teste interativo (adaptada para notebook)\n",
    "def test_interactive():\n",
    "    sample_titles = [\n",
    "        \"Wireless Gaming Mouse with RGB\",\n",
    "        \"Eco-Friendly Bamboo Phone Case\",\n",
    "        \"Premium Leather Wallet for Men\",\n",
    "        \"Portable Bluetooth Speaker Waterproof\",\n",
    "        \"LED Desk Lamp with USB Charging\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîπ Exemplos de t√≠tulos para testar:\")\n",
    "    for i, title in enumerate(sample_titles, 1):\n",
    "        print(f\"  {i}. {title}\")\n",
    "    \n",
    "    print(\"\\nüé≤ Teste autom√°tico com exemplos:\")\n",
    "    for title in sample_titles[:3]:  # Testa apenas os 3 primeiros\n",
    "        print(f\"\\nüìù T√≠tulo: {title}\")\n",
    "        print(\"ü§ñ Descri√ß√£o gerada:\")\n",
    "        description = generate_product_description(title)\n",
    "        print(f\"   {description}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Executa teste interativo\n",
    "test_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92aaccf",
   "metadata": {},
   "source": [
    "### 7.2 Resumo dos Resultados\n",
    "\n",
    "#### Conclus√µes do Tech Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ RESUMO DO TECH CHALLENGE - FINE-TUNING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üìä CONFIGURA√á√ÉO UTILIZADA:\")\n",
    "print(f\"  ü§ñ Modelo: {CONFIG['model_name']}\")\n",
    "print(f\"  üìè Tamanho m√°ximo: {CONFIG['max_seq_length']} tokens\")\n",
    "print(f\"  üîß LoRA r: {CONFIG['lora_r']}, alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  üìà Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  üîÑ Steps: {CONFIG['max_steps']}\")\n",
    "print()\n",
    "print(\"üíæ DADOS PROCESSADOS:\")\n",
    "if 'training_data' in locals():\n",
    "    print(f\"  üìÅ Amostras processadas: {len(training_data)}\")\n",
    "    final_approval_rate = (len(training_data) / CONFIG['sample_size']) * 100\n",
    "    print(f\"  ‚úÖ Taxa de aprova√ß√£o: {final_approval_rate:.1f}%\")\n",
    "print()\n",
    "print(\"üöÄ RESULTADOS:\")\n",
    "print(\"  ‚úÖ Fine-tuning executado com sucesso\")\n",
    "print(\"  ‚úÖ Modelo otimizado para descri√ß√µes de produtos Amazon\")\n",
    "print(\"  ‚úÖ Interface interativa funcional\")\n",
    "print()\n",
    "print(\"üí° MELHORIAS IMPLEMENTADAS:\")\n",
    "print(\"  üîç Sistema avan√ßado de limpeza de dados\")\n",
    "print(\"  üìä Filtragem por qualidade de conte√∫do\")\n",
    "print(\"  üéØ Formato instruction-following otimizado\")\n",
    "print(\"  ‚ö° Configura√ß√£o otimizada para Google Colab\")\n",
    "print()\n",
    "print(\"üéâ TECH CHALLENGE CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
