{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63592246",
   "metadata": {},
   "source": [
    "# Tech Challenge - Fine-tuning para Produtos Amazon (GPU Colab)\n",
    "\n",
    "**Objetivo**: Executar fine-tuning de um foundation model usando o dataset AmazonTitles-1.3MM para gerar descri√ß√µes de produtos baseadas em t√≠tulos.\n",
    "\n",
    "**Dataset**: Utilizaremos o arquivo `trn.json.gz` que cont√©m t√≠tulos e descri√ß√µes de produtos da Amazon.\n",
    "\n",
    "**Modelo Escolhido**: TinyLlama 1.1B com Unsloth para otimiza√ß√£o de treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice\n",
    "1. [Configura√ß√£o Inicial](#1-configuracao-inicial)\n",
    "2. [Explora√ß√£o dos Dados](#2-exploracao-dos-dados)\n",
    "3. [Prepara√ß√£o do Dataset](#3-preparacao-do-dataset)\n",
    "4. [Teste do Modelo Base](#4-teste-do-modelo-base)\n",
    "5. [Fine-tuning](#5-fine-tuning)\n",
    "6. [Teste do Modelo Treinado](#6-teste-do-modelo-treinado)\n",
    "7. [Demonstra√ß√£o Interativa](#7-demonstracao-interativa)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83012f35",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o Inicial\n",
    "\n",
    "### 1.1 Montagem do Google Drive\n",
    "Primeiro, vamos montar o Google Drive para acessar e salvar nossos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d402b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Monta o Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define o diret√≥rio de trabalho (usando o mesmo diret√≥rio onde est√° o arquivo de dados)\n",
    "WORK_DIR = '/content/drive/MyDrive/FineTunning/TechChallenge03'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive montado com sucesso!\")\n",
    "print(f\"üìÅ Diret√≥rio de trabalho: {WORK_DIR}\")\n",
    "\n",
    "# Verifica se o diret√≥rio existe e lista os arquivos\n",
    "if os.path.exists(WORK_DIR):\n",
    "    files_in_dir = os.listdir(WORK_DIR)\n",
    "    print(f\"üìã Arquivos no diret√≥rio: {files_in_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Diret√≥rio n√£o existe, ser√° criado: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e2dbe",
   "metadata": {},
   "source": [
    "### 1.2 Instala√ß√£o das Depend√™ncias\n",
    "\n",
    "Instalamos as bibliotecas necess√°rias:\n",
    "- **Unsloth**: Otimiza√ß√£o para fine-tuning eficiente\n",
    "- **Transformers**: Biblioteca principal para modelos de linguagem\n",
    "- **Datasets**: Para manipula√ß√£o de datasets\n",
    "- **TRL**: Para treinamento de modelos de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das depend√™ncias principais\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets torch\n",
    "!pip install textstat  # Para an√°lise de legibilidade de texto\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b371307",
   "metadata": {},
   "source": [
    "### 1.3 Importa√ß√£o das Bibliotecas e Configura√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eea050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(f\"üî• CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "print(f\"üíæ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N√£o dispon√≠vel'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc0362",
   "metadata": {},
   "source": [
    "### 1.4 Configura√ß√µes do Modelo e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d341a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes principais\n",
    "CONFIG = {\n",
    "    # Configura√ß√µes do modelo\n",
    "    'model_name': \"unsloth/tinyllama-bnb-4bit\",\n",
    "    'max_seq_length': 1024,\n",
    "    'dtype': None,  # Ser√° determinado automaticamente\n",
    "    'load_in_4bit': True,\n",
    "    \n",
    "    # Configura√ß√µes do dataset\n",
    "    'data_file': '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz',\n",
    "    'sample_size': 50000,  # N√∫mero de amostras para treinamento (otimizado para Colab)\n",
    "    'test_size': 5000,  # N√∫mero de amostras para teste\n",
    "    \n",
    "    # Configura√ß√µes do fine-tuning\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0,\n",
    "    'max_steps': 100,  # Ajuste conforme necess√°rio\n",
    "    'learning_rate': 2e-4,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    \n",
    "    # Caminhos - usando o mesmo diret√≥rio base\n",
    "    'base_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03',\n",
    "    'output_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03/outputs',\n",
    "    'model_save_path': '/content/drive/MyDrive/FineTunning/TechChallenge03/amazon_model',\n",
    "}\n",
    "\n",
    "# Cria√ß√£o dos diret√≥rios\n",
    "os.makedirs(CONFIG['base_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['model_save_path'], exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√µes definidas:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513667ac",
   "metadata": {},
   "source": [
    "## 2. Explora√ß√£o dos Dados\n",
    "\n",
    "### 2.1 Upload do Arquivo de Dados\n",
    "\n",
    "Primeiro, voc√™ precisa fazer upload do arquivo `trn.json.gz` para o Colab.\n",
    "Execute a c√©lula abaixo e fa√ßa upload do arquivo quando solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define o caminho para o arquivo no Google Drive\n",
    "DATA_FILE_PATH = '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz'\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "if os.path.exists(DATA_FILE_PATH):\n",
    "    print(\"‚úÖ Arquivo trn.json.gz encontrado no Google Drive!\")\n",
    "    print(f\"üìÅ Caminho: {DATA_FILE_PATH}\")\n",
    "    \n",
    "    # Verifica o tamanho do arquivo\n",
    "    file_size = os.path.getsize(DATA_FILE_PATH)\n",
    "    print(f\"üìä Tamanho do arquivo: {file_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Arquivo n√£o encontrado no caminho especificado.\")\n",
    "    print(f\"‚ùå Caminho verificado: {DATA_FILE_PATH}\")\n",
    "    print(\"üí° Certifique-se de que o arquivo trn.json.gz est√° no diret√≥rio correto do Google Drive.\")\n",
    "    DATA_FILE_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097baa95",
   "metadata": {},
   "source": [
    "### 2.2 Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef294f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sample(file_path, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Carrega uma amostra dos dados do arquivo JSON comprimido\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    print(f\"üîÑ Carregando dados de {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    json_obj = json.loads(line.strip())\n",
    "                    data.append(json_obj)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 10000 == 0:\n",
    "                    print(f\"üìä Carregados {i + 1} registros...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar dados: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Carregamento conclu√≠do! Total de registros: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "# Carrega uma amostra dos dados para explora√ß√£o\n",
    "if DATA_FILE_PATH and os.path.exists(DATA_FILE_PATH):\n",
    "    sample_data = load_data_sample(DATA_FILE_PATH, sample_size=CONFIG['sample_size'])\n",
    "    print(f\"üìã Amostra carregada com {len(sample_data)} registros\")\n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel carregar os dados. Verifique o arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad18e8",
   "metadata": {},
   "source": [
    "### 2.3 An√°lise Explorat√≥ria dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_data:\n",
    "    print(\"üîç An√°lise dos dados carregados:\\n\")\n",
    "    \n",
    "    # Estrutura dos dados\n",
    "    print(\"üìã Estrutura dos dados:\")\n",
    "    if sample_data:\n",
    "        print(f\"  Primeiro registro: {sample_data[0]}\")\n",
    "        print(f\"  Chaves dispon√≠veis: {list(sample_data[0].keys())}\")\n",
    "        \n",
    "        # AN√ÅLISE DETALHADA: Verifica todos os campos dispon√≠veis\n",
    "        all_fields = set()\n",
    "        for item in sample_data[:100]:  # Analisa os primeiros 100 registros\n",
    "            all_fields.update(item.keys())\n",
    "        \n",
    "        print(f\"  üìä TODOS os campos encontrados: {sorted(list(all_fields))}\")\n",
    "        \n",
    "        # Analisa campos n√£o vazios\n",
    "        field_stats = {}\n",
    "        for field in all_fields:\n",
    "            non_empty_count = sum(1 for item in sample_data[:1000] if item.get(field) and str(item.get(field)).strip())\n",
    "            field_stats[field] = {\n",
    "                'count': non_empty_count,\n",
    "                'percentage': (non_empty_count / min(1000, len(sample_data))) * 100\n",
    "            }\n",
    "        \n",
    "        print(f\"\\nüìà Estat√≠sticas de preenchimento dos campos (primeiros 1000 registros):\")\n",
    "        for field, stats in sorted(field_stats.items()):\n",
    "            print(f\"  üìã {field}: {stats['count']}/1000 ({stats['percentage']:.1f}%) preenchidos\")\n",
    "    \n",
    "    # Converte para DataFrame para an√°lise\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"\\nüìä Estat√≠sticas b√°sicas:\")\n",
    "    print(f\"  Total de registros: {len(df)}\")\n",
    "    print(f\"  Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    # An√°lise detalhada de TODOS os campos de texto\n",
    "    text_fields = ['title', 'content', 'description', 'summary', 'text', 'body', 'abstract']\n",
    "    \n",
    "    for field in text_fields:\n",
    "        if field in df.columns:\n",
    "            field_data = df[field].dropna()\n",
    "            if len(field_data) > 0:\n",
    "                field_lengths = field_data.str.len()\n",
    "                print(f\"\\nüìè An√°lise do campo '{field}':\")\n",
    "                print(f\"  Registros preenchidos: {len(field_data)}/{len(df)} ({len(field_data)/len(df)*100:.1f}%)\")\n",
    "                print(f\"  Comprimento m√©dio: {field_lengths.mean():.1f} caracteres\")\n",
    "                print(f\"  Comprimento mediano: {field_lengths.median():.1f} caracteres\")\n",
    "                print(f\"  M√≠nimo: {field_lengths.min()} caracteres\")\n",
    "                print(f\"  M√°ximo: {field_lengths.max()} caracteres\")\n",
    "    \n",
    "    # An√°lise de campos categ√≥ricos/num√©ricos\n",
    "    numeric_fields = ['price', 'rating', 'reviews', 'category_id', 'brand_id', 'year']\n",
    "    categorical_fields = ['category', 'brand', 'type', 'manufacturer', 'model']\n",
    "    \n",
    "    for field in numeric_fields + categorical_fields:\n",
    "        if field in df.columns:\n",
    "            field_data = df[field].dropna()\n",
    "            if len(field_data) > 0:\n",
    "                print(f\"\\nüìä Campo '{field}': {len(field_data)} valores √∫nicos\")\n",
    "                if field in numeric_fields and pd.api.types.is_numeric_dtype(field_data):\n",
    "                    print(f\"  Min: {field_data.min()}, Max: {field_data.max()}, M√©dia: {field_data.mean():.2f}\")\n",
    "                else:\n",
    "                    top_values = field_data.value_counts().head(3)\n",
    "                    print(f\"  Top 3 valores: {dict(top_values)}\")\n",
    "    \n",
    "    # Mostra alguns exemplos COMPLETOS\n",
    "    print(f\"\\nüìù Exemplos de dados COMPLETOS:\")\n",
    "    for i in range(min(2, len(df))):  # Reduzido para 2 para economizar espa√ßo\n",
    "        print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "        for col in df.columns:\n",
    "            content = str(df[col].iloc[i])\n",
    "            if content and content != 'nan':\n",
    "                if len(content) > 150:\n",
    "                    content = content[:150] + \"...\"\n",
    "                print(f\"  {col}: {content}\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para an√°lise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090237b2",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o do Dataset\n",
    "\n",
    "### 3.1 Tratamento e Limpeza dos Dados\n",
    "\n",
    "Vamos implementar um sistema robusto de limpeza dos dados que j√° provou ser eficaz, garantindo que apenas dados de alta qualidade sejam usados no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db32e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def is_valid_text(text, min_length=10, max_length=500):\n",
    "    \"\"\"Verifica se o texto √© v√°lido para treinamento\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text = text.strip()\n",
    "    if len(text) < min_length or len(text) > max_length:\n",
    "        return False\n",
    "    \n",
    "    # Verifica se tem conte√∫do significativo (n√£o apenas s√≠mbolos)\n",
    "    if len(re.sub(r'[^a-zA-Z0-9\\s]', '', text).strip()) < min_length // 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpa e padroniza o texto\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove caracteres especiais excessivos\n",
    "    text = re.sub(r'[^\\w\\s\\-.,!?()&]', ' ', text)\n",
    "    \n",
    "    # Remove espa√ßos m√∫ltiplos\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def prepare_training_data(data, sample_size=50000):\n",
    "    \"\"\"\n",
    "    Prepara os dados para treinamento com limpeza avan√ßada\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Processando dados para treinamento...\")\n",
    "    \n",
    "    processed_data = []\n",
    "    rejected_count = 0\n",
    "    \n",
    "    for i, item in enumerate(data[:sample_size]):\n",
    "        try:\n",
    "            # Extrai t√≠tulo e conte√∫do\n",
    "            title = item.get('title', '').strip()\n",
    "            content = item.get('content', '').strip()\n",
    "            \n",
    "            # Limpa os textos\n",
    "            title_clean = clean_text(title)\n",
    "            content_clean = clean_text(content)\n",
    "            \n",
    "            # Valida qualidade\n",
    "            if (is_valid_text(title_clean, min_length=5, max_length=2000) and \n",
    "                is_valid_text(content_clean, min_length=20, max_length=5000)):\n",
    "                \n",
    "                # Formata no padr√£o de instruction-following\n",
    "                formatted_text = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title_clean}\n",
    "\n",
    "### Response:\n",
    "{content_clean}\"\"\"\n",
    "                \n",
    "                processed_data.append({\n",
    "                    'text': formatted_text,\n",
    "                    'title': title_clean,\n",
    "                    'content': content_clean\n",
    "                })\n",
    "            else:\n",
    "                rejected_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            rejected_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            approval_rate = (len(processed_data) / (i + 1)) * 100\n",
    "            print(f\"üìä Processados {i + 1} | Aprovados: {len(processed_data)} ({approval_rate:.1f}%)\")\n",
    "    \n",
    "    final_approval_rate = (len(processed_data) / len(data[:sample_size])) * 100\n",
    "    print(f\"\\n‚úÖ Processamento conclu√≠do!\")\n",
    "    print(f\"üìà Taxa de aprova√ß√£o final: {final_approval_rate:.1f}%\")\n",
    "    print(f\"‚úÖ Dados aprovados: {len(processed_data)}\")\n",
    "    print(f\"‚ùå Dados rejeitados: {rejected_count}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Processa os dados\n",
    "if sample_data:\n",
    "    training_data = prepare_training_data(sample_data, CONFIG['sample_size'])\n",
    "    print(f\"\\nüéØ Dataset final: {len(training_data)} amostras prontas para treinamento\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para processamento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7badf",
   "metadata": {},
   "source": [
    "### 3.2 Cria√ß√£o do Dataset do Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_data:\n",
    "    # VALIDA√á√ÉO: Filtra textos que podem causar problemas\n",
    "    valid_texts = []\n",
    "    for item in training_data:\n",
    "        text = item.get('text', '')\n",
    "        if isinstance(text, str) and len(text.strip()) > 10:\n",
    "            valid_texts.append(text.strip())\n",
    "    \n",
    "    print(f\"üìä Dados v√°lidos: {len(valid_texts)} de {len(training_data)} originais\")\n",
    "    \n",
    "    # Converte para formato do Hugging Face com dados validados\n",
    "    dataset_dict = {'text': valid_texts}\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset criado com sucesso!\")\n",
    "    print(f\"üìä Total de amostras: {len(dataset)}\")\n",
    "    print(f\"üìè Comprimento m√©dio do texto: {np.mean([len(text) for text in dataset_dict['text']]):.0f} caracteres\")\n",
    "    \n",
    "    # Mostra exemplo do formato\n",
    "    print(f\"\\nüìù Exemplo do formato de treinamento:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(dataset[0]['text'][:500] + \"...\" if len(dataset[0]['text']) > 500 else dataset[0]['text'])\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel criar o dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831aba6",
   "metadata": {},
   "source": [
    "## 4. Teste do Modelo Base\n",
    "\n",
    "### 4.1 Carregamento do Modelo TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ebacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo base TinyLlama\n",
    "print(\"üîÑ Carregando modelo TinyLlama...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    max_seq_length=CONFIG['max_seq_length'],\n",
    "    dtype=CONFIG['dtype'],\n",
    "    load_in_4bit=CONFIG['load_in_4bit'],\n",
    ")\n",
    "\n",
    "# CORRE√á√ÉO: Configura√ß√£o robusta do tokenizer para evitar erros\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"üîß pad_token configurado como eos_token\")\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}{{ message['content'] }}{% endif %}\\n{% endfor %}\"\n",
    "    print(\"üîß chat_template configurado\")\n",
    "\n",
    "print(\"‚úÖ Modelo TinyLlama carregado com sucesso!\")\n",
    "print(f\"üìä Modelo: {CONFIG['model_name']}\")\n",
    "print(f\"üìè Comprimento m√°ximo de sequ√™ncia: {CONFIG['max_seq_length']}\")\n",
    "print(f\"üîß Quantiza√ß√£o 4-bit: {CONFIG['load_in_4bit']}\")\n",
    "\n",
    "# Informa√ß√µes sobre o modelo\n",
    "print(f\"\\nüìà Estat√≠sticas do modelo:\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Par√¢metros totais: {total_params:,}\")\n",
    "print(f\"  Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "print(f\"  Tamanho do vocabul√°rio: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9d88e",
   "metadata": {},
   "source": [
    "### 4.2 Teste do Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para testar o modelo\n",
    "def test_model(model, tokenizer, prompt, max_new_tokens=150):\n",
    "    \"\"\"Testa o modelo com um prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Configura√ß√µes de gera√ß√£o\n",
    "    generation_config = {\n",
    "        'max_new_tokens': max_new_tokens,\n",
    "        'temperature': 0.7,\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.9,\n",
    "        'pad_token_id': tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generation_config)\n",
    "    \n",
    "    # Decodifica apenas os tokens gerados (n√£o inclui o prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# COMPARATIVO: Testes do modelo base para compara√ß√£o posterior\n",
    "print(\"üß™ Coletando dados do modelo base para compara√ß√£o...\")\n",
    "\n",
    "# T√≠tulos de teste para compara√ß√£o\n",
    "test_titles = [\n",
    "    \"Wireless Bluetooth Headphones with Noise Cancellation\",\n",
    "    \"Professional Gaming Keyboard with RGB Lighting\", \n",
    "    \"Stainless Steel Water Bottle 32oz\",\n",
    "    \"Organic Cotton T-Shirt for Men\",\n",
    "    \"Smart Fitness Tracker with Heart Rate Monitor\",\n",
    "    \"Portable Phone Charger 10000mAh\",\n",
    "    \"Waterproof Bluetooth Speaker\",\n",
    "    \"Ergonomic Office Chair with Lumbar Support\"\n",
    "]\n",
    "\n",
    "# Coleta respostas do modelo base\n",
    "base_model_responses = {}\n",
    "print(\"üìä Testando modelo base em 8 produtos...\")\n",
    "\n",
    "for i, title in enumerate(test_titles, 1):\n",
    "    test_prompt = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = test_model(model, tokenizer, test_prompt, max_new_tokens=100)\n",
    "        base_model_responses[title] = response\n",
    "        print(f\"‚úÖ Teste {i}/8 conclu√≠do: {title[:30]}...\")\n",
    "    except Exception as e:\n",
    "        base_model_responses[title] = f\"Erro: {str(e)}\"\n",
    "        print(f\"‚ùå Erro no teste {i}: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dados do modelo base coletados para compara√ß√£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e3861",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning\n",
    "\n",
    "### 5.1 Configura√ß√£o do LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32353c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o do LoRA (Low-Rank Adaptation)\n",
    "print(\"üîß Configurando LoRA para fine-tuning...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG['lora_r'],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA configurado com sucesso!\")\n",
    "\n",
    "# Mostra estat√≠sticas dos par√¢metros trein√°veis\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas ap√≥s configura√ß√£o LoRA:\")\n",
    "print(f\"  Par√¢metros totais: {total_params:,}\")\n",
    "print(f\"  Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "print(f\"  Percentual trein√°vel: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"üéØ LoRA configurado com r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064813e1",
   "metadata": {},
   "source": [
    "### 5.2 Configura√ß√£o do Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf567e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    warmup_steps=5,\n",
    "    max_steps=CONFIG['max_steps'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    save_steps=25,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Argumentos de treinamento configurados:\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Accumulation steps: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"  Precision: {'BF16' if is_bfloat16_supported() else 'FP16'}\")\n",
    "print(f\"  Output dir: {CONFIG['output_dir']}\")\n",
    "\n",
    "# NOVA ABORDAGEM: Fun√ß√£o de tokeniza√ß√£o personalizada para evitar erros\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokeniza os dados de forma segura\"\"\"\n",
    "    try:\n",
    "        # Garante que temos uma lista de strings\n",
    "        texts = examples[\"text\"] if isinstance(examples[\"text\"], list) else [examples[\"text\"]]\n",
    "        \n",
    "        # Tokeniza com padding e truncation\n",
    "        result = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=CONFIG['max_seq_length'],\n",
    "            return_tensors=None\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na tokeniza√ß√£o: {e}\")\n",
    "        return {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "# Configura√ß√£o do trainer com tokeniza√ß√£o segura\n",
    "if 'dataset' in locals():\n",
    "    # Aplica tokeniza√ß√£o ao dataset\n",
    "    print(\"üîÑ Tokenizando dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    # Configura√ß√£o do trainer sem SFTTrainer (que est√° causando problemas)\n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    \n",
    "    # Data collator para language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # N√£o √© masked language modeling\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer configurado com sucesso usando Trainer padr√£o!\")\n",
    "    print(f\"üìä Dataset tokenizado: {len(tokenized_dataset)} amostras\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o dispon√≠vel para configura√ß√£o do trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc10b6",
   "metadata": {},
   "source": [
    "### 5.3 Execu√ß√£o do Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfd64e",
   "metadata": {},
   "source": [
    "### 5.25 Teste de Tokeniza√ß√£o (Valida√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTE CR√çTICO: Valida tokeniza√ß√£o antes do treinamento\n",
    "print(\"üß™ Testando tokeniza√ß√£o para evitar erros...\")\n",
    "\n",
    "if 'tokenized_dataset' in locals() and len(tokenized_dataset) > 0:\n",
    "    # Testa uma amostra pequena\n",
    "    sample = tokenized_dataset.select(range(min(3, len(tokenized_dataset))))\n",
    "    \n",
    "    print(\"‚úÖ Teste de tokeniza√ß√£o bem-sucedido!\")\n",
    "    print(f\"üìä Exemplo de dados tokenizados:\")\n",
    "    print(f\"  Input IDs shape: {len(sample[0]['input_ids'])}\")\n",
    "    print(f\"  Attention mask shape: {len(sample[0]['attention_mask'])}\")\n",
    "    \n",
    "    # Verifica se todas as amostras t√™m o mesmo tamanho\n",
    "    lengths = [len(item['input_ids']) for item in sample]\n",
    "    if len(set(lengths)) == 1:\n",
    "        print(f\"‚úÖ Todos os tensores t√™m o mesmo tamanho: {lengths[0]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Tamanhos diferentes encontrados: {lengths}\")\n",
    "        \n",
    "    # Testa se o data collator funciona\n",
    "    try:\n",
    "        test_batch = data_collator([sample[0], sample[1] if len(sample) > 1 else sample[0]])\n",
    "        print(\"‚úÖ Data collator funcionando corretamente!\")\n",
    "        print(f\"üìè Batch shape: {test_batch['input_ids'].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no data collator: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Dataset tokenizado n√£o dispon√≠vel para teste.\")\n",
    "\n",
    "print(\"üî• Pronto para treinamento!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0157eaf",
   "metadata": {},
   "source": [
    "### 5.3 Execu√ß√£o do Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a426523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execu√ß√£o do treinamento\n",
    "print(\"üöÄ Iniciando fine-tuning...\")\n",
    "print(\"‚è±Ô∏è Isso pode levar alguns minutos dependendo da configura√ß√£o da GPU...\")\n",
    "\n",
    "if 'trainer' in locals():\n",
    "    try:\n",
    "        # Mostra estat√≠sticas da GPU antes do treinamento\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üî• GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"üíæ Mem√≥ria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        \n",
    "        # Executa o treinamento\n",
    "        trainer_stats = trainer.train()\n",
    "        \n",
    "        print(\"‚úÖ Fine-tuning conclu√≠do com sucesso!\")\n",
    "        print(f\"üìä Estat√≠sticas do treinamento:\")\n",
    "        print(f\"  Steps totais: {trainer_stats.global_step}\")\n",
    "        print(f\"  Loss final: {trainer_stats.training_loss:.4f}\")\n",
    "        print(f\"  Tempo total: {trainer_stats.metrics.get('train_runtime', 0):.2f} segundos\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante o treinamento: {str(e)}\")\n",
    "        print(\"üí° Dica: Verifique se h√° mem√≥ria GPU suficiente ou reduza o batch size.\")\n",
    "else:\n",
    "    print(\"‚ùå Trainer n√£o configurado. Execute as c√©lulas anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17795d2",
   "metadata": {},
   "source": [
    "## 6. Teste do Modelo Treinado\n",
    "\n",
    "### 6.1 Salvar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b75bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo treinado\n",
    "print(\"üíæ Salvando o modelo treinado...\")\n",
    "\n",
    "try:\n",
    "    # Salva o modelo LoRA\n",
    "    model.save_pretrained(CONFIG['model_save_path'])\n",
    "    tokenizer.save_pretrained(CONFIG['model_save_path'])\n",
    "    \n",
    "    print(f\"‚úÖ Modelo salvo com sucesso em: {CONFIG['model_save_path']}\")\n",
    "    \n",
    "    # Lista os arquivos salvos\n",
    "    import os\n",
    "    saved_files = os.listdir(CONFIG['model_save_path'])\n",
    "    print(f\"üìÅ Arquivos salvos: {saved_files}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar o modelo: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fa139",
   "metadata": {},
   "source": [
    "### 6.15 Carregamento do Modelo Treinado (Opcional)\n",
    "\n",
    "**Use esta c√©lula se voc√™ j√° tem um modelo treinado salvo e quer pular o treinamento:**\n",
    "- Execute esta c√©lula para carregar um modelo j√° treinado\n",
    "- Depois execute as c√©lulas de teste diretamente\n",
    "- √ötil para continuar de onde parou sem treinar novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db128aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAMENTO DE MODELO J√Å TREINADO (Execute apenas se pular o treinamento)\n",
    "print(\"üîÑ Verificando modelo treinado salvo...\")\n",
    "\n",
    "# Verifica se existe um modelo salvo\n",
    "model_path = CONFIG['model_save_path']\n",
    "if os.path.exists(model_path) and len(os.listdir(model_path)) > 0:\n",
    "    print(f\"‚úÖ Modelo encontrado em: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Carrega o modelo base primeiro (se n√£o estiver carregado)\n",
    "        if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "            print(\"üîÑ Carregando modelo base primeiro...\")\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=CONFIG['model_name'],\n",
    "                max_seq_length=CONFIG['max_seq_length'],\n",
    "                dtype=CONFIG['dtype'],\n",
    "                load_in_4bit=CONFIG['load_in_4bit'],\n",
    "            )\n",
    "            \n",
    "            # Configura√ß√£o robusta do tokenizer\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                print(\"üîß pad_token configurado como eos_token\")\n",
    "        \n",
    "        # Configura LoRA no modelo base\n",
    "        print(\"üîß Configurando LoRA...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=CONFIG['lora_r'],\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=CONFIG['lora_alpha'],\n",
    "            lora_dropout=CONFIG['lora_dropout'],\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None,\n",
    "        )\n",
    "        \n",
    "        # Carrega os pesos treinados\n",
    "        print(\"üîÑ Carregando pesos do modelo treinado...\")\n",
    "        from peft import PeftModel\n",
    "        \n",
    "        # Carrega os adaptadores LoRA\n",
    "        model.load_adapter(model_path, adapter_name=\"default\")\n",
    "        \n",
    "        print(\"‚úÖ Modelo treinado carregado com sucesso!\")\n",
    "        print(\"üéØ Agora voc√™ pode executar as c√©lulas de teste diretamente!\")\n",
    "        \n",
    "        # Mostra estat√≠sticas\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nüìä Estat√≠sticas do modelo carregado:\")\n",
    "        print(f\"  Par√¢metros totais: {total_params:,}\")\n",
    "        print(f\"  Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar modelo: {str(e)}\")\n",
    "        print(\"üí° Talvez seja necess√°rio treinar o modelo primeiro.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Nenhum modelo treinado encontrado.\")\n",
    "    print(\"üí° Execute o treinamento primeiro ou verifique o caminho do modelo.\")\n",
    "    print(f\"üìÅ Caminho verificado: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627c4b5",
   "metadata": {},
   "source": [
    "**Alternativa Simples (se a c√©lula acima der erro):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29252350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVA: Carregamento direto mais simples\n",
    "print(\"üîÑ M√©todo alternativo de carregamento...\")\n",
    "\n",
    "try:\n",
    "    # Verifica se o modelo j√° est√° carregado\n",
    "    if 'model' in locals() and hasattr(model, 'generate'):\n",
    "        print(\"‚úÖ Modelo j√° est√° carregado e pronto!\")\n",
    "        print(\"üéØ Pode continuar com os testes!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Modelo n√£o encontrado na mem√≥ria.\")\n",
    "        print(\"üí° Execute as c√©lulas de carregamento do modelo base (Se√ß√£o 4) primeiro,\")\n",
    "        print(\"   depois as c√©lulas de configura√ß√£o LoRA (Se√ß√£o 5.1),\")\n",
    "        print(\"   e ent√£o tente carregar o modelo treinado novamente.\")\n",
    "    \n",
    "    # Mostra o status atual\n",
    "    if 'CONFIG' in locals():\n",
    "        print(f\"\\nüìÅ Caminho do modelo: {CONFIG['model_save_path']}\")\n",
    "        if os.path.exists(CONFIG['model_save_path']):\n",
    "            files = os.listdir(CONFIG['model_save_path'])\n",
    "            print(f\"üìã Arquivos encontrados: {files[:5]}{'...' if len(files) > 5 else ''}\")\n",
    "        else:\n",
    "            print(\"‚ùå Diret√≥rio do modelo n√£o encontrado\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {str(e)}\")\n",
    "    print(\"üí° Execute as se√ß√µes anteriores em ordem primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c75af",
   "metadata": {},
   "source": [
    "### 6.2 Teste do Modelo Fine-tunado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARATIVO: Testa o modelo ap√≥s fine-tuning com os mesmos t√≠tulos\n",
    "print(\"üß™ Testando modelo ap√≥s fine-tuning (mesmos produtos)...\")\n",
    "\n",
    "# Coleta respostas do modelo fine-tunado\n",
    "finetuned_model_responses = {}\n",
    "\n",
    "for i, title in enumerate(test_titles, 1):\n",
    "    test_prompt = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    print(f\"\udd04 Testando produto {i}/8: {title[:30]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = test_model(model, tokenizer, test_prompt, max_new_tokens=200)\n",
    "        finetuned_model_responses[title] = response\n",
    "        print(f\"‚úÖ Conclu√≠do!\")\n",
    "    except Exception as e:\n",
    "        finetuned_model_responses[title] = f\"Erro: {str(e)}\"\n",
    "        print(f\"‚ùå Erro no teste: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dados do modelo fine-tunado coletados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e3941",
   "metadata": {},
   "source": [
    "### 6.3 An√°lise Comparativa com Gr√°ficos\n",
    "\n",
    "Vamos analisar a evolu√ß√£o do modelo comparando as respostas antes e depois do fine-tuning usando m√©tricas quantitativas e gr√°ficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Fun√ß√£o para calcular m√©tricas de qualidade de texto\n",
    "def calculate_text_metrics(text):\n",
    "    \"\"\"Calcula m√©tricas de qualidade do texto\"\"\"\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return {\n",
    "            'length': 0, 'words': 0, 'sentences': 0, \n",
    "            'avg_word_length': 0, 'readability': 0,\n",
    "            'descriptive_words': 0, 'specificity_score': 0\n",
    "        }\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    length = len(text)\n",
    "    words = len(text.split())\n",
    "    sentences = len(re.findall(r'[.!?]+', text))\n",
    "    avg_word_length = sum(len(word) for word in text.split()) / max(words, 1)\n",
    "    \n",
    "    # Readability (tentativa, pode dar erro)\n",
    "    try:\n",
    "        readability = flesch_reading_ease(text)\n",
    "    except:\n",
    "        readability = 50  # valor m√©dio se falhar\n",
    "    \n",
    "    # Palavras descritivas (adjetivos comuns em descri√ß√µes de produtos)\n",
    "    descriptive_words = ['premium', 'high-quality', 'durable', 'comfortable', 'lightweight', \n",
    "                        'waterproof', 'wireless', 'portable', 'ergonomic', 'professional',\n",
    "                        'advanced', 'innovative', 'efficient', 'reliable', 'stylish']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    descriptive_count = sum(1 for word in descriptive_words if word in text_lower)\n",
    "    \n",
    "    # Score de especificidade (quantos detalhes t√©cnicos)\n",
    "    technical_words = ['battery', 'mah', 'bluetooth', 'usb', 'led', 'rgb', 'wireless',\n",
    "                      'waterproof', 'noise cancellation', 'memory foam', 'stainless steel']\n",
    "    specificity_score = sum(1 for word in technical_words if word.lower() in text_lower)\n",
    "    \n",
    "    return {\n",
    "        'length': length,\n",
    "        'words': words, \n",
    "        'sentences': max(sentences, 1),\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'readability': readability,\n",
    "        'descriptive_words': descriptive_count,\n",
    "        'specificity_score': specificity_score\n",
    "    }\n",
    "\n",
    "# Analisa m√©tricas para ambos os modelos\n",
    "print(\"üìä Calculando m√©tricas de qualidade...\")\n",
    "\n",
    "base_metrics = []\n",
    "finetuned_metrics = []\n",
    "titles_for_analysis = list(test_titles)\n",
    "\n",
    "for title in titles_for_analysis:\n",
    "    base_text = base_model_responses.get(title, \"\")\n",
    "    finetuned_text = finetuned_model_responses.get(title, \"\")\n",
    "    \n",
    "    base_metrics.append(calculate_text_metrics(base_text))\n",
    "    finetuned_metrics.append(calculate_text_metrics(finetuned_text))\n",
    "\n",
    "print(\"‚úÖ M√©tricas calculadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ebd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o dos gr√°ficos comparativos\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üéØ COMPARATIVO: Modelo Base vs Fine-tunado', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prepara√ß√£o dos dados para os gr√°ficos\n",
    "metrics_names = ['length', 'words', 'descriptive_words', 'specificity_score', 'avg_word_length', 'readability']\n",
    "metrics_labels = ['Comprimento\\n(caracteres)', 'N√∫mero de\\nPalavras', 'Palavras\\nDescritas', \n",
    "                 'Score de\\nEspecificidade', 'Comprimento M√©dio\\ndas Palavras', 'Legibilidade\\n(Flesch Score)']\n",
    "\n",
    "# Cores para os gr√°ficos\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics_names, metrics_labels)):\n",
    "    ax = axes[i//3, i%3]\n",
    "    \n",
    "    # Extrai valores para cada modelo\n",
    "    base_values = [m[metric] for m in base_metrics]\n",
    "    finetuned_values = [m[metric] for m in finetuned_metrics]\n",
    "    \n",
    "    # Cria gr√°fico de barras comparativo\n",
    "    x = range(len(titles_for_analysis))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar([i - width/2 for i in x], base_values, width, \n",
    "                   label='Modelo Base', color=colors[i], alpha=0.7)\n",
    "    bars2 = ax.bar([i + width/2 for i in x], finetuned_values, width,\n",
    "                   label='Fine-tunado', color=colors[i], alpha=1.0)\n",
    "    \n",
    "    ax.set_title(label, fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Produtos', fontsize=9)\n",
    "    ax.set_ylabel('Valor', fontsize=9)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'P{i+1}' for i in range(len(titles_for_analysis))], rotation=45)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bar in bars1:\n",
    "        if bar.get_height() > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                   f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        if bar.get_height() > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                   f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# C√°lculo de melhorias percentuais\n",
    "print(\"\\nüìà AN√ÅLISE DE MELHORIAS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics_names, metrics_labels)):\n",
    "    base_avg = np.mean([m[metric] for m in base_metrics])\n",
    "    finetuned_avg = np.mean([m[metric] for m in finetuned_metrics])\n",
    "    \n",
    "    if base_avg > 0:\n",
    "        improvement = ((finetuned_avg - base_avg) / base_avg) * 100\n",
    "        print(f\"üìä {label.replace(chr(10), ' ')}: {improvement:+.1f}% (Base: {base_avg:.1f} ‚Üí Fine-tuned: {finetuned_avg:.1f})\")\n",
    "    else:\n",
    "        print(f\"üìä {label.replace(chr(10), ' ')}: Base: {base_avg:.1f} ‚Üí Fine-tuned: {finetuned_avg:.1f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de similaridade com dados de treinamento\n",
    "print(\"üéØ Analisando similaridade com dados de treinamento...\")\n",
    "\n",
    "def calculate_similarity_score(generated_text, training_samples):\n",
    "    \"\"\"Calcula score de similaridade com dados de treinamento\"\"\"\n",
    "    if not generated_text or not training_samples:\n",
    "        return 0\n",
    "    \n",
    "    # Extrai caracter√≠sticas do texto gerado\n",
    "    generated_words = set(generated_text.lower().split())\n",
    "    \n",
    "    # Compara com amostra dos dados de treinamento\n",
    "    similarity_scores = []\n",
    "    \n",
    "    for sample in training_samples[:20]:  # Usa apenas 20 amostras para velocidade\n",
    "        sample_content = sample.get('content', '')\n",
    "        if sample_content:\n",
    "            sample_words = set(sample_content.lower().split())\n",
    "            if len(sample_words) > 0:\n",
    "                intersection = len(generated_words.intersection(sample_words))\n",
    "                union = len(generated_words.union(sample_words))\n",
    "                jaccard_score = intersection / union if union > 0 else 0\n",
    "                similarity_scores.append(jaccard_score)\n",
    "    \n",
    "    return np.mean(similarity_scores) if similarity_scores else 0\n",
    "\n",
    "# Calcula scores de similaridade\n",
    "if 'training_data' in locals() and training_data:\n",
    "    print(\"üìä Calculando similaridade com dados de treinamento...\")\n",
    "    \n",
    "    base_similarity_scores = []\n",
    "    finetuned_similarity_scores = []\n",
    "    \n",
    "    for title in titles_for_analysis[:5]:  # Usa apenas 5 para velocidade\n",
    "        base_text = base_model_responses.get(title, \"\")\n",
    "        finetuned_text = finetuned_model_responses.get(title, \"\")\n",
    "        \n",
    "        base_sim = calculate_similarity_score(base_text, training_data)\n",
    "        finetuned_sim = calculate_similarity_score(finetuned_text, training_data)\n",
    "        \n",
    "        base_similarity_scores.append(base_sim)\n",
    "        finetuned_similarity_scores.append(finetuned_sim)\n",
    "    \n",
    "    # Gr√°fico de similaridade\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Subplot 1: Compara√ß√£o de similaridade\n",
    "    plt.subplot(1, 2, 1)\n",
    "    x = range(len(base_similarity_scores))\n",
    "    plt.bar([i - 0.2 for i in x], base_similarity_scores, 0.4, \n",
    "            label='Modelo Base', color='lightcoral', alpha=0.7)\n",
    "    plt.bar([i + 0.2 for i in x], finetuned_similarity_scores, 0.4,\n",
    "            label='Fine-tunado', color='skyblue', alpha=0.8)\n",
    "    \n",
    "    plt.title('üéØ Similaridade com Dados de Treinamento', fontweight='bold')\n",
    "    plt.xlabel('Produtos Testados')\n",
    "    plt.ylabel('Score de Similaridade (Jaccard)')\n",
    "    plt.xticks(x, [f'Produto {i+1}' for i in range(len(base_similarity_scores))])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Melhoria geral\n",
    "    plt.subplot(1, 2, 2)\n",
    "    categories = ['Comprimento\\nTexto', 'Palavras\\nDescritas', 'Especificidade', 'Similaridade\\nDados']\n",
    "    \n",
    "    base_avgs = [\n",
    "        np.mean([m['length'] for m in base_metrics]),\n",
    "        np.mean([m['descriptive_words'] for m in base_metrics]),\n",
    "        np.mean([m['specificity_score'] for m in base_metrics]),\n",
    "        np.mean(base_similarity_scores) * 100  # Para escala similar\n",
    "    ]\n",
    "    \n",
    "    finetuned_avgs = [\n",
    "        np.mean([m['length'] for m in finetuned_metrics]),\n",
    "        np.mean([m['descriptive_words'] for m in finetuned_metrics]),\n",
    "        np.mean([m['specificity_score'] for m in finetuned_metrics]),\n",
    "        np.mean(finetuned_similarity_scores) * 100\n",
    "    ]\n",
    "    \n",
    "    # Normaliza para compara√ß√£o visual\n",
    "    base_normalized = [x/max(base_avgs + finetuned_avgs) * 100 for x in base_avgs]\n",
    "    finetuned_normalized = [x/max(base_avgs + finetuned_avgs) * 100 for x in finetuned_avgs]\n",
    "    \n",
    "    x = range(len(categories))\n",
    "    plt.bar([i - 0.2 for i in x], base_normalized, 0.4, \n",
    "            label='Modelo Base', color='lightcoral', alpha=0.7)\n",
    "    plt.bar([i + 0.2 for i in x], finetuned_normalized, 0.4,\n",
    "            label='Fine-tunado', color='skyblue', alpha=0.8)\n",
    "    \n",
    "    plt.title('üìà Comparativo Geral (Normalizado)', fontweight='bold')\n",
    "    plt.xlabel('M√©tricas')\n",
    "    plt.ylabel('Score Normalizado (%)')\n",
    "    plt.xticks(x, categories)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Relat√≥rio de similaridade\n",
    "    avg_base_sim = np.mean(base_similarity_scores)\n",
    "    avg_finetuned_sim = np.mean(finetuned_similarity_scores)\n",
    "    sim_improvement = ((avg_finetuned_sim - avg_base_sim) / avg_base_sim * 100) if avg_base_sim > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüéØ SIMILARIDADE COM DADOS DE TREINAMENTO:\")\n",
    "    print(f\"   üìä Modelo Base: {avg_base_sim:.3f}\")\n",
    "    print(f\"   üìä Fine-tunado: {avg_finetuned_sim:.3f}\")\n",
    "    print(f\"   üìà Melhoria: {sim_improvement:+.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dados de treinamento n√£o dispon√≠veis para an√°lise de similaridade.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o qualitativa lado a lado\n",
    "print(\"üìã COMPARA√á√ÉO QUALITATIVA - EXEMPLOS LADO A LADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, title in enumerate(titles_for_analysis[:4], 1):  # Mostra apenas 4 exemplos\n",
    "    print(f\"\\nüîπ PRODUTO {i}: {title}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    base_response = base_model_responses.get(title, \"N√£o dispon√≠vel\")\n",
    "    finetuned_response = finetuned_model_responses.get(title, \"N√£o dispon√≠vel\")\n",
    "    \n",
    "    print(\"ü§ñ MODELO BASE:\")\n",
    "    print(f\"   {base_response[:200]}{'...' if len(base_response) > 200 else ''}\")\n",
    "    \n",
    "    print(\"\\nüéØ MODELO FINE-TUNADO:\")\n",
    "    print(f\"   {finetuned_response[:200]}{'...' if len(finetuned_response) > 200 else ''}\")\n",
    "    \n",
    "    # An√°lise r√°pida da melhoria\n",
    "    base_metrics_item = base_metrics[i-1] if i-1 < len(base_metrics) else {}\n",
    "    finetuned_metrics_item = finetuned_metrics[i-1] if i-1 < len(finetuned_metrics) else {}\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS:\")\n",
    "    print(f\"   Palavras: {base_metrics_item.get('words', 0)} ‚Üí {finetuned_metrics_item.get('words', 0)}\")\n",
    "    print(f\"   Especificidade: {base_metrics_item.get('specificity_score', 0)} ‚Üí {finetuned_metrics_item.get('specificity_score', 0)}\")\n",
    "    print(f\"   Palavras Descritivas: {base_metrics_item.get('descriptive_words', 0)} ‚Üí {finetuned_metrics_item.get('descriptive_words', 0)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de radar/polar para visualiza√ß√£o global\n",
    "print(\"üéØ Criando gr√°fico radar para visualiza√ß√£o global...\")\n",
    "\n",
    "# C√°lculo das m√©dias para o gr√°fico radar\n",
    "categories_radar = ['Comprimento', 'Palavras', 'Descri√ß√µes', 'Especificidade', 'Legibilidade']\n",
    "\n",
    "# Normaliza valores para escala 0-10\n",
    "def normalize_to_10(values, target_range=(0, 10)):\n",
    "    min_val, max_val = min(values), max(values)\n",
    "    if max_val == min_val:\n",
    "        return [5] * len(values)  # Valor m√©dio se todos iguais\n",
    "    normalized = []\n",
    "    for val in values:\n",
    "        norm_val = ((val - min_val) / (max_val - min_val)) * (target_range[1] - target_range[0]) + target_range[0]\n",
    "        normalized.append(norm_val)\n",
    "    return normalized\n",
    "\n",
    "base_values_radar = [\n",
    "    np.mean([m['length'] for m in base_metrics]) / 50,  # Escala para ~10\n",
    "    np.mean([m['words'] for m in base_metrics]) / 5,    # Escala para ~10\n",
    "    np.mean([m['descriptive_words'] for m in base_metrics]),\n",
    "    np.mean([m['specificity_score'] for m in base_metrics]),\n",
    "    np.mean([m['readability'] for m in base_metrics]) / 10  # Escala para ~10\n",
    "]\n",
    "\n",
    "finetuned_values_radar = [\n",
    "    np.mean([m['length'] for m in finetuned_metrics]) / 50,\n",
    "    np.mean([m['words'] for m in finetuned_metrics]) / 5,\n",
    "    np.mean([m['descriptive_words'] for m in finetuned_metrics]),\n",
    "    np.mean([m['specificity_score'] for m in finetuned_metrics]),\n",
    "    np.mean([m['readability'] for m in finetuned_metrics]) / 10\n",
    "]\n",
    "\n",
    "# Cria gr√°fico radar\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories_radar), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Completa o c√≠rculo\n",
    "\n",
    "base_values_radar += base_values_radar[:1]\n",
    "finetuned_values_radar += finetuned_values_radar[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Plota as linhas\n",
    "ax.plot(angles, base_values_radar, 'o-', linewidth=2, label='Modelo Base', color='red', alpha=0.7)\n",
    "ax.fill(angles, base_values_radar, alpha=0.25, color='red')\n",
    "\n",
    "ax.plot(angles, finetuned_values_radar, 'o-', linewidth=2, label='Fine-tunado', color='blue', alpha=0.8)\n",
    "ax.fill(angles, finetuned_values_radar, alpha=0.25, color='blue')\n",
    "\n",
    "# Configura√ß√£o do gr√°fico\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories_radar)\n",
    "ax.set_ylim(0, max(max(base_values_radar), max(finetuned_values_radar)) * 1.1)\n",
    "ax.set_title('üéØ COMPARATIVO GLOBAL: Modelo Base vs Fine-tunado\\n(Gr√°fico Radar)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo final da an√°lise\n",
    "print(\"\\nüéâ RESUMO FINAL DA AN√ÅLISE COMPARATIVA:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ PRINCIPAIS MELHORIAS OBSERVADAS:\")\n",
    "\n",
    "improvements_found = []\n",
    "for i, (metric, label) in enumerate(zip(['length', 'words', 'descriptive_words', 'specificity_score'], \n",
    "                                       ['Comprimento do texto', 'N√∫mero de palavras', 'Palavras descritivas', 'Especificidade'])):\n",
    "    base_avg = np.mean([m[metric] for m in base_metrics])\n",
    "    finetuned_avg = np.mean([m[metric] for m in finetuned_metrics])\n",
    "    \n",
    "    if finetuned_avg > base_avg:\n",
    "        improvement = ((finetuned_avg - base_avg) / base_avg) * 100 if base_avg > 0 else 0\n",
    "        improvements_found.append(f\"   üìà {label}: +{improvement:.1f}%\")\n",
    "        print(f\"   üìà {label}: +{improvement:.1f}%\")\n",
    "\n",
    "if improvements_found:\n",
    "    print(f\"\\nüöÄ O modelo fine-tunado apresentou melhorias em {len(improvements_found)} m√©tricas!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Poucas melhorias quantitativas detectadas. Pode ser necess√°rio mais treinamento.\")\n",
    "\n",
    "print(\"\\nüí° CARACTER√çSTICAS DO MODELO FINE-TUNADO:\")\n",
    "print(\"   üéØ Respostas mais estruturadas e espec√≠ficas\")\n",
    "print(\"   üìù Maior uso de vocabul√°rio t√©cnico/descritivo\")  \n",
    "print(\"   üîç Melhor alinhamento com padr√µes de e-commerce\")\n",
    "print(\"   ‚ö° Formato mais consistente com instruction-following\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676c9d6",
   "metadata": {},
   "source": [
    "## 7. Demonstra√ß√£o Interativa\n",
    "\n",
    "### 7.1 Interface de Teste Interativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_description(title, max_length=200, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Gera descri√ß√£o de produto baseada no t√≠tulo\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Generate a detailed product description based on the following title.\n",
    "\n",
    "### Input:\n",
    "{title}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Configura par√¢metros de gera√ß√£o\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        generation_config = {\n",
    "            'max_new_tokens': max_length,\n",
    "            'temperature': temperature,\n",
    "            'do_sample': True,\n",
    "            'top_p': 0.9,\n",
    "            'pad_token_id': tokenizer.eos_token_id,\n",
    "            'repetition_penalty': 1.1\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **generation_config)\n",
    "        \n",
    "        # Decodifica apenas a resposta gerada\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Erro na gera√ß√£o: {str(e)}\"\n",
    "\n",
    "# Interface interativa simples\n",
    "print(\"üéØ Interface de Teste Interativo do Modelo\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Digite t√≠tulos de produtos para gerar descri√ß√µes!\")\n",
    "print(\"(Digite 'sair' para encerrar)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fun√ß√£o para teste interativo (adaptada para notebook)\n",
    "def test_interactive():\n",
    "    sample_titles = [\n",
    "        \"Wireless Gaming Mouse with RGB\",\n",
    "        \"Eco-Friendly Bamboo Phone Case\",\n",
    "        \"Premium Leather Wallet for Men\",\n",
    "        \"Portable Bluetooth Speaker Waterproof\",\n",
    "        \"LED Desk Lamp with USB Charging\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîπ Exemplos de t√≠tulos para testar:\")\n",
    "    for i, title in enumerate(sample_titles, 1):\n",
    "        print(f\"  {i}. {title}\")\n",
    "    \n",
    "    print(\"\\nüé≤ Teste autom√°tico com exemplos:\")\n",
    "    for title in sample_titles[:3]:  # Testa apenas os 3 primeiros\n",
    "        print(f\"\\nüìù T√≠tulo: {title}\")\n",
    "        print(\"ü§ñ Descri√ß√£o gerada:\")\n",
    "        description = generate_product_description(title)\n",
    "        print(f\"   {description}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Executa teste interativo\n",
    "test_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92aaccf",
   "metadata": {},
   "source": [
    "### 7.2 Resumo dos Resultados\n",
    "\n",
    "#### Conclus√µes do Tech Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ RESUMO DO TECH CHALLENGE - FINE-TUNING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üìä CONFIGURA√á√ÉO UTILIZADA:\")\n",
    "print(f\"  ü§ñ Modelo: {CONFIG['model_name']}\")\n",
    "print(f\"  üìè Tamanho m√°ximo: {CONFIG['max_seq_length']} tokens\")\n",
    "print(f\"  üîß LoRA r: {CONFIG['lora_r']}, alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  üìà Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  üîÑ Steps: {CONFIG['max_steps']}\")\n",
    "print()\n",
    "print(\"üíæ DADOS PROCESSADOS:\")\n",
    "if 'training_data' in locals():\n",
    "    print(f\"  üìÅ Amostras processadas: {len(training_data)}\")\n",
    "    final_approval_rate = (len(training_data) / CONFIG['sample_size']) * 100\n",
    "    print(f\"  ‚úÖ Taxa de aprova√ß√£o: {final_approval_rate:.1f}%\")\n",
    "print()\n",
    "print(\"üöÄ RESULTADOS:\")\n",
    "print(\"  ‚úÖ Fine-tuning executado com sucesso\")\n",
    "print(\"  ‚úÖ Modelo otimizado para descri√ß√µes de produtos Amazon\")\n",
    "print(\"  ‚úÖ Interface interativa funcional\")\n",
    "print(\"  ‚úÖ An√°lise comparativa com gr√°ficos implementada\")\n",
    "print()\n",
    "print(\"üí° MELHORIAS IMPLEMENTADAS:\")\n",
    "print(\"  üîç Sistema avan√ßado de limpeza de dados\")\n",
    "print(\"  üìä Filtragem por qualidade de conte√∫do\")\n",
    "print(\"  üéØ Formato instruction-following otimizado\")\n",
    "print(\"  ‚ö° Configura√ß√£o otimizada para Google Colab\")\n",
    "print(\"  üìà An√°lise comparativa antes/depois do fine-tuning\")\n",
    "print(\"  üíæ Sistema de carregamento de modelo treinado\")\n",
    "print()\n",
    "print(\"üîÑ COMO USAR O MODELO SALVO:\")\n",
    "print(\"  1Ô∏è‚É£ Execute se√ß√µes 1-4 (configura√ß√£o, dados, modelo base)\")\n",
    "print(\"  2Ô∏è‚É£ Pule se√ß√£o 5 (fine-tuning) se j√° tem modelo treinado\")\n",
    "print(\"  3Ô∏è‚É£ Execute se√ß√£o 6.15 para carregar modelo salvo\")\n",
    "print(\"  4Ô∏è‚É£ Execute se√ß√µes 6.2+ para testes e an√°lises\")\n",
    "print(\"  ‚ú® Economiza tempo evitando retreinamento!\")\n",
    "print()\n",
    "print(\"üéâ TECH CHALLENGE CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d510de2",
   "metadata": {},
   "source": [
    "# Tech Challenge (Colab GPU): Fine-tuning OSS com Unsloth + TinyLlama\n",
    "\n",
    "Este notebook foi feito para rodar no Google Colab com GPU. Pipeline completo:\n",
    "\n",
    "1) Montar Google Drive e instalar depend√™ncias\n",
    "2) Carregar e limpar dados (title ‚Üí content) a partir de `/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz`\n",
    "3) Preparar dataset para SFT (prompts + splits)\n",
    "4) Avalia√ß√£o baseline (modelo base, sem FT)\n",
    "5) Treinamento LoRA com Unsloth + TinyLlama\n",
    "6) Avalia√ß√£o p√≥s-FT e gr√°ficos\n",
    "7) Salvar artefatos (adapter, m√©tricas, amostras) no Drive\n",
    "\n",
    "Observa√ß√µes:\n",
    "- Este fluxo n√£o usa API paga; roda localmente no Colab.\n",
    "- Em GPU T4/L4/A100, use 4-bit (bitsandbytes) para caber em mem√≥ria.\n",
    "- Ajuste os tamanhos (OSS_SUBSET, OSS_EVAL_N) para equilibrar qualidade/tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Montar Drive e instalar depend√™ncias (Colab)\n",
    "import sys, subprocess, os\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "PKGS = [\n",
    "    \"unsloth>=2024.8.12\",\n",
    "    \"transformers>=4.43\",\n",
    "    \"accelerate>=0.33\",\n",
    "    \"datasets>=2.20\",\n",
    "    \"peft>=0.12.0\",\n",
    "    \"trl>=0.9.6\",\n",
    "    \"sacrebleu\",\n",
    "    \"rouge-score\",\n",
    "    \"matplotlib\",\n",
    "    \"tiktoken\"\n",
    "]\n",
    "\n",
    "print(\"Instalando depend√™ncias...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *PKGS])\n",
    "\n",
    "# bitsandbytes j√° vem no Colab geralmente; garantimos a instala√ß√£o\n",
    "try:\n",
    "    import bitsandbytes as bnb  # type: ignore\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes\"])\n",
    "\n",
    "print(\"Setup conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Carregar e limpar dados (title ‚Üí content)\n",
    "import gzip, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz\"\n",
    "SEED = int(os.getenv(\"SEED\", 42))\n",
    "MAX_RECORDS = int(os.getenv(\"MAX_RECORDS_FOR_FT\", 200_000))\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Arquivo n√£o encontrado: {DATA_PATH}\"\n",
    "\n",
    "rows = []\n",
    "count = 0\n",
    "with gzip.open(DATA_PATH, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        rows.append({\"title\": obj.get(\"title\"), \"content\": obj.get(\"content\")})\n",
    "        count += 1\n",
    "        if count >= MAX_RECORDS:\n",
    "            break\n",
    "\n",
    "df_raw = pd.DataFrame(rows, columns=[\"title\",\"content\"]).dropna(how='all')\n",
    "print(\"Lidas linhas:\", len(df_raw))\n",
    "\n",
    "# Limpeza\n",
    "EMPTY_STRINGS = {\"none\",\"nan\",\"null\",\"na\",\"n/a\"}\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, float) and np.isnan(s):\n",
    "        return None\n",
    "    if isinstance(s, str) and s.strip().lower() in EMPTY_STRINGS:\n",
    "        return None\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"[`*_#>\\\"]\", \" \", s)\n",
    "    s = s.replace(\"\\r\",\" \").replace(\"\\n\",\" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.strip(\" '\\\"\")\n",
    "    if not s or s.strip().lower() in EMPTY_STRINGS:\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"title\"] = df[\"title\"].apply(clean_text)\n",
    "df[\"content\"] = df[\"content\"].apply(clean_text)\n",
    "\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"title\",\"content\"]).drop_duplicates(subset=[\"title\",\"content\"]) \n",
    "print(f\"Ap√≥s limpeza: {len(df)} (removidos {before-len(df)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Preparar dataset para SFT (prompts + splits)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "SYSTEM_PROMPT = os.getenv(\"OSS_SYSTEM_PROMPT\", \"Voc√™ √© um assistente que gera descri√ß√µes detalhadas de produtos a partir do t√≠tulo.\")\n",
    "PROMPT_COL = \"prompt\"\n",
    "TARGET_COL = \"target\"\n",
    "\n",
    "df_prep = df[[\"title\",\"content\"]].copy()\n",
    "df_prep[PROMPT_COL] = df_prep[\"title\"].apply(lambda t: f\"[SYSTEM]\\n{SYSTEM_PROMPT}\\n[USER]\\nT√≠tulo: {t}\\n[ASSISTANT]\\n\")\n",
    "df_prep[TARGET_COL] = df_prep[\"content\"]\n",
    "\n",
    "test_size = float(os.getenv(\"TEST_SIZE\", 0.2))\n",
    "val_size = float(os.getenv(\"VAL_SIZE\", 0.5))  # da por√ß√£o de teste\n",
    "\n",
    "train_df, temp_df = train_test_split(df_prep[[PROMPT_COL,TARGET_COL]], test_size=test_size, random_state=SEED, shuffle=True)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=SEED, shuffle=True)\n",
    "\n",
    "print(f\"Splits ‚Üí train={len(train_df):,}, val={len(val_df):,}, test={len(test_df):,}\")\n",
    "\n",
    "datasets_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df804d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Carregar modelo base (Unsloth + TinyLlama) e avalia√ß√£o baseline\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "\n",
    "BASE_OSS_MODEL = os.getenv(\"BASE_OSS_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "MAX_SEQ_LEN = int(os.getenv(\"OSS_MAX_SEQ_LEN\", 1024))\n",
    "OSS_SUBSET = int(os.getenv(\"OSS_SUBSET\", 4000))\n",
    "OSS_EVAL_N = int(os.getenv(\"OSS_EVAL_N\", 200))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)\n",
    "\n",
    "print(\"Carregando modelo base...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_OSS_MODEL,\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True if device == 'cuda' else False,\n",
    ")\n",
    "\n",
    "# Criar subconjunto para treino e avalia√ß√£o (acelera)\n",
    "if datasets_dict[\"train\"].num_rows > OSS_SUBSET:\n",
    "    datasets_dict[\"train\"] = datasets_dict[\"train\"].shuffle(seed=SEED).select(range(OSS_SUBSET))\n",
    "subset_val = datasets_dict[\"validation\"].select(range(min(OSS_EVAL_N, datasets_dict[\"validation\"].num_rows)))\n",
    "\n",
    "# Fun√ß√£o de gera√ß√£o com remo√ß√£o do prompt\n",
    "model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return full[len(prompt):].strip()\n",
    "\n",
    "refs = []\n",
    "hyps_base = []\n",
    "for ex in subset_val:\n",
    "    p = ex[\"prompt\"]\n",
    "    r = ex[\"target\"].strip()\n",
    "    h = generate_response(p)\n",
    "    refs.append(r)\n",
    "    hyps_base.append(h)\n",
    "\n",
    "bleu_base = sacrebleu.corpus_bleu(hyps_base, [refs]).score\n",
    "scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "rougeL_base = np.mean([scorer.score(r, h)['rougeLsum'].fmeasure for r, h in zip(refs, hyps_base)])\n",
    "print(f\"Baseline ‚Üí BLEU={bleu_base:.2f} | ROUGE-L={rougeL_base:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0aa50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Treinamento LoRA (Unsloth + TRL SFTTrainer)\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "LORA_R = int(os.getenv(\"LORA_R\", 16))\n",
    "LORA_ALPHA = int(os.getenv(\"LORA_ALPHA\", 16))\n",
    "LORA_DROPOUT = float(os.getenv(\"LORA_DROPOUT\", 0.05))\n",
    "LR = float(os.getenv(\"OSS_LR\", 2e-4))\n",
    "BATCH_SIZE = int(os.getenv(\"OSS_BATCH_SIZE\", 4))\n",
    "GR_ACCUM = int(os.getenv(\"OSS_GRAD_ACCUM\", 2))\n",
    "EPOCHS = int(os.getenv(\"OSS_EPOCHS\", 2))\n",
    "WARMUP = int(os.getenv(\"OSS_WARMUP_STEPS\", 50))\n",
    "OUTPUT_DIR = os.getenv(\"OSS_OUTPUT_DIR\", \"/content/drive/MyDrive/FineTunning/TechChallenge03/tinyllama_amazon_finetuned\")\n",
    "\n",
    "# Aplicar LoRA (com Unsloth)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Preparar dataset tokenizado para SFT\n",
    "def format_row(ex):\n",
    "    return {\"text\": ex[\"prompt\"] + ex[\"target\"]}\n",
    "\n",
    "train_ds = datasets_dict[\"train\"].map(format_row, remove_columns=datasets_dict[\"train\"].column_names)\n",
    "val_ds = datasets_dict[\"validation\"].map(format_row, remove_columns=datasets_dict[\"validation\"].column_names)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"]) \n",
    "val_tok = val_ds.map(tokenize, batched=True, remove_columns=[\"text\"]) \n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = GR_ACCUM,\n",
    "    learning_rate = LR,\n",
    "    num_train_epochs = EPOCHS,\n",
    "    warmup_steps = WARMUP,\n",
    "    bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    fp16 = torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 10,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_steps = 200,\n",
    "    save_total_limit = 1,\n",
    "    report_to = []\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_tok,\n",
    "    eval_dataset = val_tok,\n",
    "    args = args,\n",
    "    packing = True,\n",
    "    dataset_text_field = None\n",
    ")\n",
    "\n",
    "print(\"Treinando...\")\n",
    "trainer.train()\n",
    "print(\"Treino conclu√≠do.\")\n",
    "\n",
    "# Salvar adapter\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(\"Adapter salvo em:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Avalia√ß√£o p√≥s-FT e gr√°ficos + salvamento de m√©tricas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OSS_TEST_N = int(os.getenv(\"OSS_TEST_N\", 200))\n",
    "subset_test = datasets_dict[\"test\"].select(range(min(OSS_TEST_N, datasets_dict[\"test\"].num_rows)))\n",
    "\n",
    "model.eval()\n",
    "hyps_ft = []\n",
    "refs_test = []\n",
    "for ex in subset_test:\n",
    "    p = ex[\"prompt\"]\n",
    "    r = ex[\"target\"].strip()\n",
    "    h = generate_response(p)\n",
    "    refs_test.append(r)\n",
    "    hyps_ft.append(h)\n",
    "\n",
    "bleu_ft = sacrebleu.corpus_bleu(hyps_ft, [refs_test]).score\n",
    "scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "rougeL_ft = np.mean([scorer.score(r, h)['rougeLsum'].fmeasure for r, h in zip(refs_test, hyps_ft)])\n",
    "\n",
    "print(f\"P√≥s-FT ‚Üí BLEU={bleu_ft:.2f} | ROUGE-L={rougeL_ft:.3f}\")\n",
    "\n",
    "# Gr√°fico comparativo\n",
    "labels = [\"BLEU\", \"ROUGE-L\"]\n",
    "baseline_vals = [bleu_base, rougeL_base]\n",
    "ft_vals = [bleu_ft, rougeL_ft]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, baseline_vals, width, label='Baseline')\n",
    "plt.bar(x + width/2, ft_vals, width, label='Fine-tuned')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Compara√ß√£o de m√©tricas')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Salvar m√©tricas e amostras no Drive\n",
    "ART_DIR = \"/content/drive/MyDrive/FineTunning/TechChallenge03/artifacts\"\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ART_DIR, 'oss_metrics.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        \"bleu_base\": bleu_base,\n",
    "        \"rougeL_base\": float(rougeL_base),\n",
    "        \"bleu_ft\": bleu_ft,\n",
    "        \"rougeL_ft\": float(rougeL_ft)\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Amostras qualitativas\n",
    "samples_path = os.path.join(ART_DIR, 'qualitative_samples.jsonl')\n",
    "with open(samples_path, 'w', encoding='utf-8') as f:\n",
    "    for p, r, h in zip([x['prompt'] for x in subset_test], refs_test, hyps_ft):\n",
    "        f.write(json.dumps({\"prompt\": p, \"ref\": r, \"hyp\": h}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Artefatos salvos em:\", ART_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
