{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9a15d7",
   "metadata": {},
   "source": [
    "# Tech Challenge - Fine-tuning para Produtos Amazon\n",
    "\n",
    "**Objetivo**: Executar fine-tuning de um foundation model usando o dataset AmazonTitles-1.3MM para gerar descri√ß√µes de produtos baseadas em t√≠tulos.\n",
    "\n",
    "**Dataset**: Utilizaremos o arquivo `trn.json.gz` que cont√©m t√≠tulos e descri√ß√µes de produtos da Amazon.\n",
    "\n",
    "**Modelo Escolhido**: Llama 3-8B com Unsloth para otimiza√ß√£o de treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice\n",
    "1. [Configura√ß√£o Inicial](#1-configuracao-inicial)\n",
    "2. [Explora√ß√£o dos Dados](#2-exploracao-dos-dados)\n",
    "3. [Prepara√ß√£o do Dataset](#3-preparacao-do-dataset)\n",
    "4. [Teste do Modelo Base](#4-teste-do-modelo-base)\n",
    "5. [Fine-tuning](#5-fine-tuning)\n",
    "6. [Teste do Modelo Treinado](#6-teste-do-modelo-treinado)\n",
    "7. [Demonstra√ß√£o Interativa](#7-demonstracao-interativa)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde9b9b",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o Inicial\n",
    "\n",
    "### 1.1 Montagem do Google Drive\n",
    "Primeiro, vamos montar o Google Drive para acessar e salvar nossos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Monta o Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define o diret√≥rio de trabalho (usando o mesmo diret√≥rio onde est√° o arquivo de dados)\n",
    "WORK_DIR = '/content/drive/MyDrive/FineTunning/TechChallenge03'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive montado com sucesso!\")\n",
    "print(f\"üìÅ Diret√≥rio de trabalho: {WORK_DIR}\")\n",
    "\n",
    "# Verifica se o diret√≥rio existe e lista os arquivos\n",
    "if os.path.exists(WORK_DIR):\n",
    "    files_in_dir = os.listdir(WORK_DIR)\n",
    "    print(f\"üìã Arquivos no diret√≥rio: {files_in_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Diret√≥rio n√£o existe, ser√° criado: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6adf7",
   "metadata": {},
   "source": [
    "### 1.2 Instala√ß√£o das Depend√™ncias\n",
    "\n",
    "Instalamos as bibliotecas necess√°rias:\n",
    "- **Unsloth**: Otimiza√ß√£o para fine-tuning eficiente\n",
    "- **Transformers**: Biblioteca principal para modelos de linguagem\n",
    "- **Datasets**: Para manipula√ß√£o de datasets\n",
    "- **TRL**: Para treinamento de modelos de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das depend√™ncias principais\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets torch\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964444a4",
   "metadata": {},
   "source": [
    "### 1.3 Importa√ß√£o das Bibliotecas e Configura√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(f\"üî• CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "print(f\"üíæ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N√£o dispon√≠vel'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e692fdc",
   "metadata": {},
   "source": [
    "### 1.4 Configura√ß√µes do Modelo e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes principais\n",
    "CONFIG = {\n",
    "    # Configura√ß√µes do modelo\n",
    "    'model_name': \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    'max_seq_length': 2048,\n",
    "    'dtype': None,  # Ser√° determinado automaticamente\n",
    "    'load_in_4bit': True,\n",
    "    \n",
    "    # Configura√ß√µes do dataset\n",
    "    'data_file': '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz',\n",
    "    'sample_size': 10000,  # N√∫mero de amostras para treinamento (pode ajustar)\n",
    "    'test_size': 100,  # N√∫mero de amostras para teste\n",
    "    \n",
    "    # Configura√ß√µes do fine-tuning\n",
    "    'lora_r': 16,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0,\n",
    "    'max_steps': 100,  # Ajuste conforme necess√°rio\n",
    "    'learning_rate': 2e-4,\n",
    "    'batch_size': 2,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    \n",
    "    # Caminhos - usando o mesmo diret√≥rio base\n",
    "    'base_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03',\n",
    "    'output_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03/outputs',\n",
    "    'model_save_path': '/content/drive/MyDrive/FineTunning/TechChallenge03/amazon_model',\n",
    "}\n",
    "\n",
    "# Cria√ß√£o dos diret√≥rios\n",
    "os.makedirs(CONFIG['base_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['model_save_path'], exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√µes definidas:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db812510",
   "metadata": {},
   "source": [
    "## 2. Explora√ß√£o dos Dados\n",
    "\n",
    "### 2.1 Upload do Arquivo de Dados\n",
    "\n",
    "Primeiro, voc√™ precisa fazer upload do arquivo `trn.json.gz` para o Colab.\n",
    "Execute a c√©lula abaixo e fa√ßa upload do arquivo quando solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26bfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define o caminho para o arquivo no Google Drive\n",
    "DATA_FILE_PATH = '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz'\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "if os.path.exists(DATA_FILE_PATH):\n",
    "    print(\"‚úÖ Arquivo trn.json.gz encontrado no Google Drive!\")\n",
    "    print(f\"üìÅ Caminho: {DATA_FILE_PATH}\")\n",
    "    \n",
    "    # Verifica o tamanho do arquivo\n",
    "    file_size = os.path.getsize(DATA_FILE_PATH)\n",
    "    print(f\"üìä Tamanho do arquivo: {file_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Arquivo n√£o encontrado no caminho especificado.\")\n",
    "    print(f\"‚ùå Caminho verificado: {DATA_FILE_PATH}\")\n",
    "    print(\"üí° Certifique-se de que o arquivo trn.json.gz est√° no diret√≥rio correto do Google Drive.\")\n",
    "    DATA_FILE_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dc203",
   "metadata": {},
   "source": [
    "### 2.2 Carregamento dos Dados\n",
    "\n",
    "Vamos carregar o dataset completo (ou conforme configurado) e analisar sua estrutura para entender melhor os dados com que estamos trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_amazon_data(file_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    Carrega os dados do arquivo JSON comprimido, extraindo apenas title e content\n",
    "    \n",
    "    Args:\n",
    "        file_path: Caminho para o arquivo trn.json.gz\n",
    "        sample_size: N√∫mero de amostras a carregar (None para carregar tudo)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dicion√°rios com apenas os campos title e content\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    print(f\"üìñ Carregando dados de {file_path}...\")\n",
    "    print(\"üéØ Extraindo apenas os campos 'title' e 'content'\")\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if sample_size and i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    json_obj = json.loads(line.strip())\n",
    "                    \n",
    "                    # Extrai apenas title e content, desconsiderando outros campos\n",
    "                    if 'title' in json_obj and 'content' in json_obj:\n",
    "                        clean_item = {\n",
    "                            'title': json_obj['title'].strip(),\n",
    "                            'content': json_obj['content'].strip()\n",
    "                        }\n",
    "                        \n",
    "                        # S√≥ adiciona se ambos os campos n√£o est√£o vazios\n",
    "                        if clean_item['title'] and clean_item['content']:\n",
    "                            data.append(clean_item)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "                # Progress update\n",
    "                if (i + 1) % 1000 == 0:\n",
    "                    print(f\"  Processadas {i + 1} linhas, v√°lidas: {len(data)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Dados carregados com sucesso!\")\n",
    "    print(f\"üìä Total de amostras v√°lidas: {len(data)}\")\n",
    "    print(f\"üéØ Campos por amostra: title, content\")\n",
    "    return data\n",
    "\n",
    "# Carrega o dataset conforme configura√ß√£o (completo ou amostra)\n",
    "if DATA_FILE_PATH:\n",
    "    print(f\"üîÑ Carregando {CONFIG['sample_size']} amostras conforme configura√ß√£o...\")\n",
    "    raw_data = load_amazon_data(DATA_FILE_PATH, sample_size=CONFIG['sample_size'])\n",
    "else:\n",
    "    print(\"‚ùå Arquivo de dados n√£o dispon√≠vel. Execute a c√©lula de verifica√ß√£o primeiro.\")\n",
    "    raw_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da estrutura dos dados brutos (antes da limpeza)\n",
    "if raw_data:\n",
    "    print(\"üîç AN√ÅLISE DOS DADOS BRUTOS (ANTES DA LIMPEZA)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Exemplo de uma amostra\n",
    "    print(\"üìù Exemplo de uma amostra (apenas title e content):\")\n",
    "    sample_item = raw_data[0]\n",
    "    for key, value in sample_item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    print(f\"üìä ESTAT√çSTICAS GERAIS DOS DADOS BRUTOS:\")\n",
    "    print(f\"  Total de amostras carregadas: {len(raw_data)}\")\n",
    "    print(f\"  Campos utilizados: title, content\")\n",
    "    print(f\"  Outros campos: desconsiderados conforme solicitado\")\n",
    "    \n",
    "    # An√°lise de qualidade inicial\n",
    "    print(f\"\\nüîç AN√ÅLISE DE QUALIDADE INICIAL:\")\n",
    "    \n",
    "    # Verifica tamanhos dos textos\n",
    "    title_lengths = [len(item['title']) for item in raw_data]\n",
    "    content_lengths = [len(item['content']) for item in raw_data]\n",
    "    \n",
    "    print(f\"  T√≠tulos muito curtos (<3 chars): {sum(1 for x in title_lengths if x < 3)}\")\n",
    "    print(f\"  T√≠tulos muito longos (>200 chars): {sum(1 for x in title_lengths if x > 200)}\")\n",
    "    print(f\"  Conte√∫do muito curto (<5 chars): {sum(1 for x in content_lengths if x < 5)}\")\n",
    "    print(f\"  Conte√∫do muito longo (>1000 chars): {sum(1 for x in content_lengths if x > 1000)}\")\n",
    "    \n",
    "    # Verifica duplicatas\n",
    "    unique_titles = len(set(item['title'].lower() for item in raw_data))\n",
    "    duplicates = len(raw_data) - unique_titles\n",
    "    print(f\"  T√≠tulos duplicados: {duplicates}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Dados precisam de limpeza antes do treinamento!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado foi carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90519116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada dos dados brutos\n",
    "if raw_data:\n",
    "    print(\"üìè AN√ÅLISE DETALHADA DOS DADOS BRUTOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calcula estat√≠sticas de comprimento\n",
    "    title_lengths = [len(item['title']) for item in raw_data]\n",
    "    content_lengths = [len(item['content']) for item in raw_data]\n",
    "    \n",
    "    title_words = [len(item['title'].split()) for item in raw_data]\n",
    "    content_words = [len(item['content'].split()) for item in raw_data]\n",
    "    \n",
    "    print(\"üìù Comprimento em caracteres:\")\n",
    "    print(f\"  T√≠tulos - M√≠n: {min(title_lengths)}, M√°x: {max(title_lengths)}, M√©dia: {np.mean(title_lengths):.1f}\")\n",
    "    print(f\"  Conte√∫do - M√≠n: {min(content_lengths)}, M√°x: {max(content_lengths)}, M√©dia: {np.mean(content_lengths):.1f}\")\n",
    "    \n",
    "    print(\"\\nüî§ Comprimento em palavras:\")\n",
    "    print(f\"  T√≠tulos - M√≠n: {min(title_words)}, M√°x: {max(title_words)}, M√©dia: {np.mean(title_words):.1f}\")\n",
    "    print(f\"  Conte√∫do - M√≠n: {min(content_words)}, M√°x: {max(content_words)}, M√©dia: {np.mean(content_words):.1f}\")\n",
    "    \n",
    "    # Exemplos de diferentes tamanhos\n",
    "    print(\"\\nüìã EXEMPLOS DE PRODUTOS (DADOS BRUTOS):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # T√≠tulo mais curto\n",
    "    shortest_idx = title_lengths.index(min(title_lengths))\n",
    "    print(f\"üî∏ T√≠tulo mais curto ({len(raw_data[shortest_idx]['title'])} chars):\")\n",
    "    print(f\"  T√≠tulo: {raw_data[shortest_idx]['title']}\")\n",
    "    print(f\"  Conte√∫do: {raw_data[shortest_idx]['content']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    \n",
    "    # T√≠tulo mais longo\n",
    "    longest_idx = title_lengths.index(max(title_lengths))\n",
    "    print(f\"üî∏ T√≠tulo mais longo ({len(raw_data[longest_idx]['title'])} chars):\")\n",
    "    print(f\"  T√≠tulo: {raw_data[longest_idx]['title']}\")\n",
    "    print(f\"  Conte√∫do: {raw_data[longest_idx]['content'][:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    \n",
    "    # Exemplo aleat√≥rio\n",
    "    import random\n",
    "    random_idx = random.randint(0, len(raw_data)-1)\n",
    "    print(f\"üî∏ Exemplo aleat√≥rio:\")\n",
    "    print(f\"  T√≠tulo: {raw_data[random_idx]['title']}\")\n",
    "    print(f\"  Conte√∫do: {raw_data[random_idx]['content']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para an√°lise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252fb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes dos dados brutos\n",
    "if raw_data:\n",
    "    print(\"üìä CRIANDO VISUALIZA√á√ïES DOS DADOS BRUTOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Recalcula as estat√≠sticas para as visualiza√ß√µes\n",
    "    title_lengths = [len(item['title']) for item in raw_data]\n",
    "    content_lengths = [len(item['content']) for item in raw_data]\n",
    "    title_words = [len(item['title'].split()) for item in raw_data]\n",
    "    content_words = [len(item['content'].split()) for item in raw_data]\n",
    "    \n",
    "    # Configura√ß√£o do matplotlib\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('An√°lise dos Dados Brutos - Amazon Products', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 1: Distribui√ß√£o do comprimento dos t√≠tulos\n",
    "    axes[0,0].hist(title_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,0].set_title('Distribui√ß√£o - Comprimento dos T√≠tulos (caracteres)')\n",
    "    axes[0,0].set_xlabel('N√∫mero de caracteres')\n",
    "    axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].axvline(x=3, color='red', linestyle='--', alpha=0.7, label='M√≠n (3)')\n",
    "    axes[0,0].axvline(x=200, color='red', linestyle='--', alpha=0.7, label='M√°x (200)')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o do comprimento do conte√∫do\n",
    "    axes[0,1].hist(content_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_title('Distribui√ß√£o - Comprimento do Conte√∫do (caracteres)')\n",
    "    axes[0,1].set_xlabel('N√∫mero de caracteres')\n",
    "    axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].axvline(x=5, color='red', linestyle='--', alpha=0.7, label='M√≠n (5)')\n",
    "    axes[0,1].axvline(x=1000, color='red', linestyle='--', alpha=0.7, label='M√°x (1000)')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Gr√°fico 3: Distribui√ß√£o de palavras nos t√≠tulos\n",
    "    axes[1,0].hist(title_words, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1,0].set_title('Distribui√ß√£o - Palavras nos T√≠tulos')\n",
    "    axes[1,0].set_xlabel('N√∫mero de palavras')\n",
    "    axes[1,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 4: Rela√ß√£o entre t√≠tulo e conte√∫do\n",
    "    axes[1,1].scatter(title_lengths, content_lengths, alpha=0.5, color='purple', s=10)\n",
    "    axes[1,1].set_title('Rela√ß√£o: T√≠tulo vs Conte√∫do (caracteres)')\n",
    "    axes[1,1].set_xlabel('Comprimento do t√≠tulo')\n",
    "    axes[1,1].set_ylabel('Comprimento do conte√∫do')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualiza√ß√µes dos dados brutos criadas!\")\n",
    "    print(\"‚ö†Ô∏è Linhas vermelhas mostram limites para limpeza\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para visualiza√ß√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e5cf5",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o do Dataset\n",
    "\n",
    "### 3.1 Formata√ß√£o dos Dados para Fine-tuning\n",
    "\n",
    "Agora vamos formatar os dados no padr√£o esperado pelo modelo. Criaremos prompts estruturados onde:\n",
    "- **Input**: T√≠tulo do produto\n",
    "- **Output**: Descri√ß√£o do produto\n",
    "\n",
    "O formato seguir√° o padr√£o de chat do Llama 3, usando tags especiais para delimitar o in√≠cio e fim das respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da13b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(title, content):\n",
    "    \"\"\"\n",
    "    Formata um exemplo de treinamento no padr√£o do Llama 3\n",
    "    \n",
    "    Args:\n",
    "        title: T√≠tulo do produto\n",
    "        content: Descri√ß√£o do produto\n",
    "    \n",
    "    Returns:\n",
    "        String formatada para treinamento\n",
    "    \"\"\"\n",
    "    \n",
    "    # Template de prompt para o modelo\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Voc√™ √© um assistente especializado em produtos da Amazon. Sua tarefa √© gerar descri√ß√µes detalhadas e precisas de produtos baseadas apenas no t√≠tulo fornecido. As descri√ß√µes devem ser informativas, concisas e atrativas para potenciais compradores.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Gere uma descri√ß√£o para o seguinte produto: {title}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{content}<|eot_id|><|end_of_text|>\"\"\"\n",
    "    \n",
    "    return prompt_template.format(title=title, content=content)\n",
    "\n",
    "def prepare_training_data(data, train_size=None):\n",
    "    \"\"\"\n",
    "    Prepara os dados para treinamento formatando cada exemplo\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de dicion√°rios com title e content\n",
    "        train_size: N√∫mero m√°ximo de exemplos para treinamento\n",
    "    \n",
    "    Returns:\n",
    "        Lista de strings formatadas para treinamento\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Preparando dados para treinamento...\")\n",
    "    \n",
    "    if train_size:\n",
    "        data = data[:train_size]\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        formatted_prompt = format_prompt(item['title'], item['content'])\n",
    "        formatted_data.append(formatted_prompt)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"  Formatados {i + 1} exemplos...\")\n",
    "    \n",
    "    print(f\"‚úÖ Prepara√ß√£o conclu√≠da! Total: {len(formatted_data)} exemplos formatados\")\n",
    "    return formatted_data\n",
    "\n",
    "# Testa a formata√ß√£o com um exemplo\n",
    "if sample_data:\n",
    "    print(\"üß™ TESTE DE FORMATA√á√ÉO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pega um exemplo para demonstrar a formata√ß√£o\n",
    "    test_example = sample_data[0]\n",
    "    formatted_example = format_prompt(test_example['title'], test_example['content'])\n",
    "    \n",
    "    print(\"üìù Exemplo original:\")\n",
    "    print(f\"  T√≠tulo: {test_example['title']}\")\n",
    "    print(f\"  Conte√∫do: {test_example['content']}\")\n",
    "    \n",
    "    print(\"\\nüéØ Exemplo formatado para treinamento:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(formatted_example)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para teste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23a8bf",
   "metadata": {},
   "source": [
    "### 3.3 Divis√£o em Conjuntos de Treino e Teste\n",
    "\n",
    "Agora que temos os dados limpos, vamos dividi-los em conjuntos de treinamento e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358165c",
   "metadata": {},
   "source": [
    "### 3.2 Limpeza e Pr√©-processamento dos Dados\n",
    "\n",
    "Antes de prosseguir com o treinamento, √© essencial limpar os dados removendo duplicatas, valores nulos, textos muito curtos ou muito longos, e outros problemas de qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amazon_data(data, min_title_length=3, max_title_length=200, \n",
    "                      min_content_length=5, max_content_length=1000):\n",
    "    \"\"\"\n",
    "    Limpa e pr√©-processa os dados do Amazon dataset\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de dicion√°rios com title e content\n",
    "        min_title_length: Comprimento m√≠nimo do t√≠tulo\n",
    "        max_title_length: Comprimento m√°ximo do t√≠tulo\n",
    "        min_content_length: Comprimento m√≠nimo do conte√∫do\n",
    "        max_content_length: Comprimento m√°ximo do conte√∫do\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dados limpos e estat√≠sticas da limpeza\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üßπ INICIANDO LIMPEZA DOS DADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Estat√≠sticas iniciais\n",
    "    initial_count = len(data)\n",
    "    print(f\"üìä Dados iniciais: {initial_count} amostras\")\n",
    "    \n",
    "    # 1. Remove valores nulos ou vazios\n",
    "    print(\"\\nüîç 1. Removendo valores nulos ou vazios...\")\n",
    "    data = [item for item in data if item.get('title') and item.get('content')]\n",
    "    after_null_removal = len(data)\n",
    "    removed_null = initial_count - after_null_removal\n",
    "    print(f\"   Removidas: {removed_null} amostras\")\n",
    "    print(f\"   Restantes: {after_null_removal} amostras\")\n",
    "    \n",
    "    # 2. Remove whitespace extra e normaliza\n",
    "    print(\"\\n‚úÇÔ∏è 2. Normalizando espa√ßos em branco...\")\n",
    "    for item in data:\n",
    "        item['title'] = ' '.join(item['title'].split())  # Remove espa√ßos extras\n",
    "        item['content'] = ' '.join(item['content'].split())\n",
    "    \n",
    "    # 3. Remove duplicatas baseadas no t√≠tulo\n",
    "    print(\"\\nüîÑ 3. Removendo duplicatas...\")\n",
    "    seen_titles = set()\n",
    "    unique_data = []\n",
    "    duplicates_removed = 0\n",
    "    \n",
    "    for item in data:\n",
    "        title_lower = item['title'].lower()\n",
    "        if title_lower not in seen_titles:\n",
    "            seen_titles.add(title_lower)\n",
    "            unique_data.append(item)\n",
    "        else:\n",
    "            duplicates_removed += 1\n",
    "    \n",
    "    data = unique_data\n",
    "    print(f\"   Duplicatas removidas: {duplicates_removed}\")\n",
    "    print(f\"   Restantes: {len(data)} amostras\")\n",
    "    \n",
    "    # 4. Filtra por comprimento do t√≠tulo\n",
    "    print(f\"\\nüìè 4. Filtrando t√≠tulos (min: {min_title_length}, max: {max_title_length} chars)...\")\n",
    "    before_title_filter = len(data)\n",
    "    data = [item for item in data if min_title_length <= len(item['title']) <= max_title_length]\n",
    "    removed_title = before_title_filter - len(data)\n",
    "    print(f\"   Removidas: {removed_title} amostras\")\n",
    "    print(f\"   Restantes: {len(data)} amostras\")\n",
    "    \n",
    "    # 5. Filtra por comprimento do conte√∫do\n",
    "    print(f\"\\nüìÑ 5. Filtrando conte√∫do (min: {min_content_length}, max: {max_content_length} chars)...\")\n",
    "    before_content_filter = len(data)\n",
    "    data = [item for item in data if min_content_length <= len(item['content']) <= max_content_length]\n",
    "    removed_content = before_content_filter - len(data)\n",
    "    print(f\"   Removidas: {removed_content} amostras\")\n",
    "    print(f\"   Restantes: {len(data)} amostras\")\n",
    "    \n",
    "    # 6. Remove caracteres especiais problem√°ticos\n",
    "    print(\"\\nüîß 6. Limpando caracteres especiais...\")\n",
    "    import re\n",
    "    \n",
    "    for item in data:\\n        # Remove caracteres de controle e caracteres n√£o imprim√≠veis\n",
    "        item['title'] = re.sub(r'[\\\\x00-\\\\x1f\\\\x7f-\\\\x9f]', '', item['title'])\n",
    "        item['content'] = re.sub(r'[\\\\x00-\\\\x1f\\\\x7f-\\\\x9f]', '', item['content'])\n",
    "        \n",
    "        # Remove m√∫ltiplas quebras de linha\n",
    "        item['content'] = re.sub(r'\\\\n+', ' ', item['content'])\n",
    "        \n",
    "        # Remove espa√ßos extras novamente\n",
    "        item['title'] = ' '.join(item['title'].split())\n",
    "        item['content'] = ' '.join(item['content'].split())\n",
    "    \n",
    "    # 7. Verifica√ß√£o final de qualidade\n",
    "    print(\"\\n‚úÖ 7. Verifica√ß√£o final de qualidade...\")\n",
    "    final_data = []\n",
    "    removed_final = 0\n",
    "    \n",
    "    for item in data:\n",
    "        # Verifica se ainda tem conte√∫do v√°lido\n",
    "        if (item['title'].strip() and item['content'].strip() and \n",
    "            len(item['title'].strip()) >= min_title_length and\n",
    "            len(item['content'].strip()) >= min_content_length):\n",
    "            final_data.append(item)\n",
    "        else:\n",
    "            removed_final += 1\n",
    "    \n",
    "    data = final_data\n",
    "    print(f\"   Removidas na verifica√ß√£o final: {removed_final}\")\n",
    "    print(f\"   Total final: {len(data)} amostras\")\n",
    "    \n",
    "    # Estat√≠sticas de limpeza\n",
    "    total_removed = initial_count - len(data)\n",
    "    retention_rate = (len(data) / initial_count) * 100\n",
    "    \n",
    "    print(f\"\\nüìà RESUMO DA LIMPEZA:\")\n",
    "    print(f\"   Dados iniciais: {initial_count}\")\n",
    "    print(f\"   Dados finais: {len(data)}\")\n",
    "    print(f\"   Total removido: {total_removed} ({(total_removed/initial_count)*100:.1f}%)\")\n",
    "    print(f\"   Taxa de reten√ß√£o: {retention_rate:.1f}%\")\n",
    "    \n",
    "    # Estat√≠sticas dos dados limpos\n",
    "    if data:\n",
    "        title_lengths = [len(item['title']) for item in data]\n",
    "        content_lengths = [len(item['content']) for item in data]\n",
    "        \n",
    "        print(f\"\\nüìä ESTAT√çSTICAS DOS DADOS LIMPOS:\")\n",
    "        print(f\"   T√≠tulos - M√≠n: {min(title_lengths)}, M√°x: {max(title_lengths)}, M√©dia: {np.mean(title_lengths):.1f}\")\n",
    "        print(f\"   Conte√∫do - M√≠n: {min(content_lengths)}, M√°x: {max(content_lengths)}, M√©dia: {np.mean(content_lengths):.1f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Aplica a limpeza nos dados carregados\n",
    "if raw_data:\n",
    "    print(\"üîÑ Aplicando limpeza nos dados carregados...\")\n",
    "    cleaned_data = clean_amazon_data(raw_data.copy())\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para limpeza.\")\n",
    "    cleaned_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b571ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o visual: Antes vs Depois da limpeza\n",
    "if raw_data and cleaned_data:\n",
    "    print(\"üìä COMPARA√á√ÉO VISUAL: ANTES vs DEPOIS DA LIMPEZA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Cria visualiza√ß√µes comparativas\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Compara√ß√£o: Dados Originais vs Dados Limpos', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Dados originais\n",
    "    orig_title_lengths = [len(item['title']) for item in raw_data]\n",
    "    orig_content_lengths = [len(item['content']) for item in raw_data]\n",
    "    \n",
    "    # Dados limpos\n",
    "    clean_title_lengths = [len(item['title']) for item in cleaned_data]\n",
    "    clean_content_lengths = [len(item['content']) for item in cleaned_data]\n",
    "    \n",
    "    # Gr√°fico 1: Contagem de amostras\n",
    "    categories = ['Dados Originais', 'Dados Limpos']\n",
    "    counts = [len(raw_data), len(cleaned_data)]\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    axes[0,0].bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_title('N√∫mero Total de Amostras')\n",
    "    axes[0,0].set_ylabel('Quantidade')\n",
    "    for i, v in enumerate(counts):\n",
    "        axes[0,0].text(i, v + max(counts)*0.01, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o t√≠tulos - Originais\n",
    "    axes[0,1].hist(orig_title_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_title('T√≠tulos - Dados Originais')\n",
    "    axes[0,1].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 3: Distribui√ß√£o t√≠tulos - Limpos\n",
    "    axes[0,2].hist(clean_title_lengths, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,2].set_title('T√≠tulos - Dados Limpos')\n",
    "    axes[0,2].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[0,2].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 4: Distribui√ß√£o conte√∫do - Originais\n",
    "    axes[1,0].hist(orig_content_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1,0].set_title('Conte√∫do - Dados Originais')\n",
    "    axes[1,0].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[1,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 5: Distribui√ß√£o conte√∫do - Limpos\n",
    "    axes[1,1].hist(clean_content_lengths, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1,1].set_title('Conte√∫do - Dados Limpos')\n",
    "    axes[1,1].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[1,1].set_ylabel('Frequ√™ncia')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 6: Boxplot comparativo\n",
    "    data_to_plot = [orig_title_lengths, clean_title_lengths, orig_content_lengths, clean_content_lengths]\n",
    "    labels = ['T√≠tulos\\\\nOriginais', 'T√≠tulos\\\\nLimpos', 'Conte√∫do\\\\nOriginais', 'Conte√∫do\\\\nLimpos']\n",
    "    colors_box = ['lightcoral', 'lightgreen', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    bp = axes[1,2].boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[1,2].set_title('Compara√ß√£o de Distribui√ß√µes')\n",
    "    axes[1,2].set_ylabel('Comprimento (caracteres)')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas comparativas em tabela\n",
    "    print(\"\\\\nüìã ESTAT√çSTICAS COMPARATIVAS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'M√©trica':<25} {'Originais':<15} {'Limpos':<15} {'Varia√ß√£o':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Total de amostras\n",
    "    original_count = len(raw_data)\n",
    "    clean_count = len(cleaned_data)\n",
    "    variation = ((clean_count - original_count) / original_count) * 100\n",
    "    print(f\"{'Total de amostras':<25} {original_count:<15} {clean_count:<15} {variation:+.1f}%\")\n",
    "    \n",
    "    # Estat√≠sticas de t√≠tulos\n",
    "    orig_title_mean = np.mean(orig_title_lengths)\n",
    "    clean_title_mean = np.mean(clean_title_lengths)\n",
    "    title_variation = ((clean_title_mean - orig_title_mean) / orig_title_mean) * 100\n",
    "    print(f\"{'T√≠tulo m√©dio (chars)':<25} {orig_title_mean:<15.1f} {clean_title_mean:<15.1f} {title_variation:+.1f}%\")\n",
    "    \n",
    "    # Estat√≠sticas de conte√∫do\n",
    "    orig_content_mean = np.mean(orig_content_lengths)\n",
    "    clean_content_mean = np.mean(clean_content_lengths)\n",
    "    content_variation = ((clean_content_mean - orig_content_mean) / orig_content_mean) * 100\n",
    "    print(f\"{'Conte√∫do m√©dio (chars)':<25} {orig_content_mean:<15.1f} {clean_content_mean:<15.1f} {content_variation:+.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados para compara√ß√£o n√£o dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos de dados problem√°ticos que foram removidos\n",
    "if raw_data and cleaned_data:\n",
    "    print(\"üîç EXEMPLOS DE DADOS PROBLEM√ÅTICOS REMOVIDOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Identifica dados que foram removidos\n",
    "    original_titles = {item['title'] for item in raw_data}\n",
    "    clean_titles = {item['title'] for item in cleaned_data}\n",
    "    removed_titles = original_titles - clean_titles\n",
    "    \n",
    "    if removed_titles:\n",
    "        print(f\"üìä Total de t√≠tulos √∫nicos removidos: {len(removed_titles)}\")\n",
    "        \n",
    "        # Encontra exemplos espec√≠ficos dos dados removidos\n",
    "        removed_examples = []\n",
    "        for item in raw_data:\n",
    "            if item['title'] in removed_titles:\n",
    "                removed_examples.append(item)\n",
    "                if len(removed_examples) >= 5:  # Mostra at√© 5 exemplos\n",
    "                    break\n",
    "        \n",
    "        print(\"\\\\n‚ùå EXEMPLOS DE DADOS REMOVIDOS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, example in enumerate(removed_examples, 1):\n",
    "            print(f\"\\\\n{i}. EXEMPLO PROBLEM√ÅTICO:\")\n",
    "            print(f\"   T√≠tulo: '{example['title']}'\")\n",
    "            print(f\"   Conte√∫do: '{example['content']}'\")\n",
    "            \n",
    "            # Identifica o problema\n",
    "            problems = []\n",
    "            if len(example['title']) < 3:\n",
    "                problems.append(f\"t√≠tulo muito curto ({len(example['title'])} chars)\")\n",
    "            if len(example['title']) > 200:\n",
    "                problems.append(f\"t√≠tulo muito longo ({len(example['title'])} chars)\")\n",
    "            if len(example['content']) < 5:\n",
    "                problems.append(f\"conte√∫do muito curto ({len(example['content'])} chars)\")\n",
    "            if len(example['content']) > 1000:\n",
    "                problems.append(f\"conte√∫do muito longo ({len(example['content'])} chars)\")\n",
    "            if not example['title'].strip():\n",
    "                problems.append(\"t√≠tulo vazio\")\n",
    "            if not example['content'].strip():\n",
    "                problems.append(\"conte√∫do vazio\")\n",
    "            \n",
    "            print(f\"   ‚ö†Ô∏è Problema(s): {', '.join(problems) if problems else 'duplicata'}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚úÖ Nenhum dado foi removido - todos estavam dentro dos crit√©rios!\")\n",
    "    \n",
    "    # Mostra exemplos de dados que PERMANECERAM ap√≥s limpeza\n",
    "    print(\"\\\\n\\\\n‚úÖ EXEMPLOS DE DADOS LIMPOS (QUE PERMANECERAM):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(min(3, len(cleaned_data))):\n",
    "        example = cleaned_data[i]\n",
    "        print(f\"\\\\n{i+1}. EXEMPLO LIMPO:\")\n",
    "        print(f\"   T√≠tulo: '{example['title']}'\")\n",
    "        print(f\"   Conte√∫do: '{example['content']}'\")\n",
    "        print(f\"   üìè T√≠tulo: {len(example['title'])} chars, Conte√∫do: {len(example['content'])} chars\")\n",
    "        print(f\"   ‚úÖ Status: Dados v√°lidos e dentro dos par√¢metros\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dados para an√°lise de exemplos n√£o dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o dos dados limpos em treino e teste\n",
    "if cleaned_data:\n",
    "    print(\"\udd04 DIVIDINDO DADOS LIMPOS EM TREINO E TESTE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"üìä Total de dados limpos dispon√≠veis: {len(cleaned_data)}\")\n",
    "    \n",
    "    # Configura√ß√£o da divis√£o\n",
    "    test_size = min(CONFIG['test_size'], len(cleaned_data) // 10)  # M√°ximo 10% para teste\n",
    "    train_size = len(cleaned_data) - test_size\n",
    "    \n",
    "    # Embaralha os dados antes de dividir\n",
    "    import random\n",
    "    random.shuffle(cleaned_data)\n",
    "    \n",
    "    # Divis√£o dos dados\n",
    "    train_data = cleaned_data[:train_size]\n",
    "    test_data = cleaned_data[train_size:train_size + test_size]\n",
    "    \n",
    "    print(f\"  üìö Dados de treinamento: {len(train_data)}\")\n",
    "    print(f\"  üß™ Dados de teste: {len(test_data)}\")\n",
    "    print(f\"  üìä Propor√ß√£o treino/teste: {len(train_data)/len(test_data):.1f}\")\n",
    "    \n",
    "    # Estat√≠sticas dos conjuntos\n",
    "    train_title_lengths = [len(item['title']) for item in train_data]\n",
    "    train_content_lengths = [len(item['content']) for item in train_data]\n",
    "    \n",
    "    test_title_lengths = [len(item['title']) for item in test_data]\n",
    "    test_content_lengths = [len(item['content']) for item in test_data]\n",
    "    \n",
    "    print(f\"\\nüìè ESTAT√çSTICAS DO CONJUNTO DE TREINAMENTO:\")\n",
    "    print(f\"  T√≠tulos - M√©dia: {np.mean(train_title_lengths):.1f} caracteres\")\n",
    "    print(f\"  Conte√∫do - M√©dia: {np.mean(train_content_lengths):.1f} caracteres\")\n",
    "    \n",
    "    print(f\"\\nüìè ESTAT√çSTICAS DO CONJUNTO DE TESTE:\")\n",
    "    print(f\"  T√≠tulos - M√©dia: {np.mean(test_title_lengths):.1f} caracteres\")\n",
    "    print(f\"  Conte√∫do - M√©dia: {np.mean(test_content_lengths):.1f} caracteres\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Divis√£o conclu√≠da com sucesso!\")\n",
    "    print(f\"üéØ Dados prontos para formata√ß√£o e treinamento\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado limpo dispon√≠vel para divis√£o.\")\n",
    "    train_data = []\n",
    "    test_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ab3f0",
   "metadata": {},
   "source": [
    "### 3.4 Cria√ß√£o do Dataset no Formato Hugging Face\n",
    "\n",
    "Vamos criar datasets no formato esperado pela biblioteca Hugging Face para facilitar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara os dados formatados para treinamento\n",
    "if 'train_data' in locals() and train_data:\n",
    "    print(\"üîß CRIANDO DATASETS FORMATADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Formata os dados de treinamento\n",
    "    formatted_train_data = prepare_training_data(train_data)\n",
    "    \n",
    "    # Cria o dataset de treinamento no formato Hugging Face\n",
    "    train_dataset_dict = {\n",
    "        'text': formatted_train_data\n",
    "    }\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(train_dataset_dict)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset de treinamento criado:\")\n",
    "    print(f\"   N√∫mero de exemplos: {len(train_dataset)}\")\n",
    "    print(f\"   Colunas: {train_dataset.column_names}\")\n",
    "    \n",
    "    # Salva algumas amostras dos dados de teste para avalia√ß√£o posterior\n",
    "    test_samples = test_data[:10]  # Primeiras 10 amostras para teste\n",
    "    \n",
    "    print(f\"\\nüìã Amostras de teste separadas: {len(test_samples)}\")\n",
    "    \n",
    "    # Mostra estat√≠sticas do dataset final\n",
    "    text_lengths = [len(text) for text in formatted_train_data]\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DO DATASET FORMATADO:\")\n",
    "    print(f\"   Comprimento m√©dio do texto: {np.mean(text_lengths):.0f} caracteres\")\n",
    "    print(f\"   Comprimento m√≠nimo: {min(text_lengths)} caracteres\")\n",
    "    print(f\"   Comprimento m√°ximo: {max(text_lengths)} caracteres\")\n",
    "    \n",
    "    # Verifica se os textos n√£o s√£o muito longos para o modelo\n",
    "    max_length = CONFIG['max_seq_length']\n",
    "    long_texts = [t for t in text_lengths if t > max_length * 4]  # Aproximadamente 4 chars por token\n",
    "    \n",
    "    if long_texts:\n",
    "        print(f\"   ‚ö†Ô∏è Textos muito longos: {len(long_texts)} ({len(long_texts)/len(text_lengths)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Todos os textos est√£o dentro do limite esperado\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados de treinamento n√£o dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra exemplos do dataset formatado\n",
    "if 'train_dataset' in locals() and train_dataset:\n",
    "    print(\"üëÄ VISUALIZA√á√ÉO DOS DADOS FORMATADOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Mostra 2 exemplos completos\n",
    "    for i in range(min(2, len(train_dataset))):\n",
    "        print(f\"\\nüìù EXEMPLO {i+1}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Pega o texto formatado\n",
    "        formatted_text = train_dataset[i]['text']\n",
    "        \n",
    "        # Extrai partes espec√≠ficas para visualiza√ß√£o\n",
    "        lines = formatted_text.split('\\n')\n",
    "        \n",
    "        # Encontra o t√≠tulo (depois de \"Gere uma descri√ß√£o para o seguinte produto:\")\n",
    "        title_line = None\n",
    "        content_start = None\n",
    "        \n",
    "        for j, line in enumerate(lines):\n",
    "            if \"Gere uma descri√ß√£o para o seguinte produto:\" in line:\n",
    "                if j + 1 < len(lines):\n",
    "                    title_line = lines[j + 1].strip()\n",
    "            elif \"<|start_header_id|>assistant<|end_header_id|>\" in line:\n",
    "                content_start = j + 1\n",
    "                break\n",
    "        \n",
    "        if title_line:\n",
    "            print(f\"üè∑Ô∏è  T√≠tulo: {title_line}\")\n",
    "        \n",
    "        if content_start and content_start < len(lines):\n",
    "            # Pega o conte√∫do (at√© encontrar <|eot_id|>)\n",
    "            content_lines = []\n",
    "            for k in range(content_start, len(lines)):\n",
    "                if \"<|eot_id|>\" in lines[k]:\n",
    "                    break\n",
    "                if lines[k].strip():\n",
    "                    content_lines.append(lines[k].strip())\n",
    "            \n",
    "            content = \" \".join(content_lines)\n",
    "            if content:\n",
    "                print(f\"üìÑ Descri√ß√£o: {content}\")\n",
    "        \n",
    "        print(f\"üìè Comprimento total: {len(formatted_text)} caracteres\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset pronto para treinamento!\")\n",
    "    print(f\"üìä Resumo final: {len(train_dataset)} exemplos formatados\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset formatado n√£o dispon√≠vel.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
