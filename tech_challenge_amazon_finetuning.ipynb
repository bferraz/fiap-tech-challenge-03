{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9a15d7",
   "metadata": {},
   "source": [
    "# Tech Challenge - Fine-tuning para Produtos Amazon\n",
    "\n",
    "**Objetivo**: Executar fine-tuning de um foundation model usando o dataset AmazonTitles-1.3MM para gerar descri√ß√µes de produtos baseadas em t√≠tulos.\n",
    "\n",
    "**Dataset**: Utilizaremos o arquivo `trn.json.gz` que cont√©m t√≠tulos e descri√ß√µes de produtos da Amazon.\n",
    "\n",
    "**Modelo Escolhido**: Llama 3-8B com Unsloth para otimiza√ß√£o de treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice\n",
    "1. [Configura√ß√£o Inicial](#1-configuracao-inicial)\n",
    "2. [Explora√ß√£o dos Dados](#2-exploracao-dos-dados)\n",
    "3. [Prepara√ß√£o do Dataset](#3-preparacao-do-dataset)\n",
    "4. [Teste do Modelo Base](#4-teste-do-modelo-base)\n",
    "5. [Fine-tuning](#5-fine-tuning)\n",
    "6. [Teste do Modelo Treinado](#6-teste-do-modelo-treinado)\n",
    "7. [Demonstra√ß√£o Interativa](#7-demonstracao-interativa)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde9b9b",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o Inicial\n",
    "\n",
    "### 1.1 Montagem do Google Drive\n",
    "Primeiro, vamos montar o Google Drive para acessar e salvar nossos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Monta o Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define o diret√≥rio de trabalho (usando o mesmo diret√≥rio onde est√° o arquivo de dados)\n",
    "WORK_DIR = '/content/drive/MyDrive/FineTunning/TechChallenge03'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive montado com sucesso!\")\n",
    "print(f\"üìÅ Diret√≥rio de trabalho: {WORK_DIR}\")\n",
    "\n",
    "# Verifica se o diret√≥rio existe e lista os arquivos\n",
    "if os.path.exists(WORK_DIR):\n",
    "    files_in_dir = os.listdir(WORK_DIR)\n",
    "    print(f\"üìã Arquivos no diret√≥rio: {files_in_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Diret√≥rio n√£o existe, ser√° criado: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6adf7",
   "metadata": {},
   "source": [
    "### 1.2 Instala√ß√£o das Depend√™ncias\n",
    "\n",
    "Instalamos as bibliotecas necess√°rias:\n",
    "- **Unsloth**: Otimiza√ß√£o para fine-tuning eficiente\n",
    "- **Transformers**: Biblioteca principal para modelos de linguagem\n",
    "- **Datasets**: Para manipula√ß√£o de datasets\n",
    "- **TRL**: Para treinamento de modelos de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das depend√™ncias principais\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets torch\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias foram instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964444a4",
   "metadata": {},
   "source": [
    "### 1.3 Importa√ß√£o das Bibliotecas e Configura√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(f\"üî• CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "print(f\"üíæ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N√£o dispon√≠vel'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e692fdc",
   "metadata": {},
   "source": [
    "### 1.4 Configura√ß√µes do Modelo e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes principais\n",
    "CONFIG = {\n",
    "    # Configura√ß√µes do modelo\n",
    "    'model_name': \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    'max_seq_length': 2048,\n",
    "    'dtype': None,  # Ser√° determinado automaticamente\n",
    "    'load_in_4bit': True,\n",
    "    \n",
    "    # Configura√ß√µes do dataset\n",
    "    'data_file': '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz',\n",
    "    'sample_size': 1000000,  # N√∫mero de amostras para treinamento (pode ajustar)\n",
    "    'test_size': 100000,  # N√∫mero de amostras para teste\n",
    "    \n",
    "    # Configura√ß√µes do fine-tuning\n",
    "    'lora_r': 16,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0,\n",
    "    'max_steps': 100,  # Ajuste conforme necess√°rio\n",
    "    'learning_rate': 2e-4,\n",
    "    'batch_size': 2,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    \n",
    "    # Caminhos - usando o mesmo diret√≥rio base\n",
    "    'base_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03',\n",
    "    'output_dir': '/content/drive/MyDrive/FineTunning/TechChallenge03/outputs',\n",
    "    'model_save_path': '/content/drive/MyDrive/FineTunning/TechChallenge03/amazon_model',\n",
    "}\n",
    "\n",
    "# Cria√ß√£o dos diret√≥rios\n",
    "os.makedirs(CONFIG['base_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['model_save_path'], exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√µes definidas:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db812510",
   "metadata": {},
   "source": [
    "## 2. Explora√ß√£o dos Dados\n",
    "\n",
    "### 2.1 Upload do Arquivo de Dados\n",
    "\n",
    "Primeiro, voc√™ precisa fazer upload do arquivo `trn.json.gz` para o Colab.\n",
    "Execute a c√©lula abaixo e fa√ßa upload do arquivo quando solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26bfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define o caminho para o arquivo no Google Drive\n",
    "DATA_FILE_PATH = '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz'\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "if os.path.exists(DATA_FILE_PATH):\n",
    "    print(\"‚úÖ Arquivo trn.json.gz encontrado no Google Drive!\")\n",
    "    print(f\"üìÅ Caminho: {DATA_FILE_PATH}\")\n",
    "    \n",
    "    # Verifica o tamanho do arquivo\n",
    "    file_size = os.path.getsize(DATA_FILE_PATH)\n",
    "    print(f\"üìä Tamanho do arquivo: {file_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Arquivo n√£o encontrado no caminho especificado.\")\n",
    "    print(f\"‚ùå Caminho verificado: {DATA_FILE_PATH}\")\n",
    "    print(\"üí° Certifique-se de que o arquivo trn.json.gz est√° no diret√≥rio correto do Google Drive.\")\n",
    "    DATA_FILE_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dc203",
   "metadata": {},
   "source": [
    "### 2.2 Carregamento dos Dados\n",
    "\n",
    "Vamos carregar o dataset completo (ou conforme configurado) e analisar sua estrutura para entender melhor os dados com que estamos trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_amazon_data(file_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    Carrega os dados do arquivo JSON comprimido, extraindo apenas title e content\n",
    "    \n",
    "    Args:\n",
    "        file_path: Caminho para o arquivo trn.json.gz\n",
    "        sample_size: N√∫mero de amostras a carregar (None para carregar tudo)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dicion√°rios com apenas os campos title e content\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    print(f\"üìñ Carregando dados de {file_path}...\")\n",
    "    print(\"üéØ Extraindo apenas os campos 'title' e 'content'\")\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if sample_size and i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    json_obj = json.loads(line.strip())\n",
    "                    \n",
    "                    # Extrai apenas title e content, desconsiderando outros campos\n",
    "                    if 'title' in json_obj and 'content' in json_obj:\n",
    "                        clean_item = {\n",
    "                            'title': json_obj['title'].strip(),\n",
    "                            'content': json_obj['content'].strip()\n",
    "                        }\n",
    "                        \n",
    "                        # S√≥ adiciona se ambos os campos n√£o est√£o vazios\n",
    "                        if clean_item['title'] and clean_item['content']:\n",
    "                            data.append(clean_item)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "                # Progress update\n",
    "                if (i + 1) % 1000 == 0:\n",
    "                    print(f\"  Processadas {i + 1} linhas, v√°lidas: {len(data)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Dados carregados com sucesso!\")\n",
    "    print(f\"üìä Total de amostras v√°lidas: {len(data)}\")\n",
    "    print(f\"üéØ Campos por amostra: title, content\")\n",
    "    return data\n",
    "\n",
    "# Carrega o dataset conforme configura√ß√£o (completo ou amostra)\n",
    "if DATA_FILE_PATH:\n",
    "    print(f\"üîÑ Carregando {CONFIG['sample_size']} amostras conforme configura√ß√£o...\")\n",
    "    raw_data = load_amazon_data(DATA_FILE_PATH, sample_size=CONFIG['sample_size'])\n",
    "else:\n",
    "    print(\"‚ùå Arquivo de dados n√£o dispon√≠vel. Execute a c√©lula de verifica√ß√£o primeiro.\")\n",
    "    raw_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da estrutura dos dados brutos (antes da limpeza)\n",
    "if raw_data:\n",
    "    print(\"üîç AN√ÅLISE DOS DADOS BRUTOS (ANTES DA LIMPEZA)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Exemplo de uma amostra\n",
    "    print(\"üìù Exemplo de uma amostra (apenas title e content):\")\n",
    "    sample_item = raw_data[0]\n",
    "    for key, value in sample_item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    print(f\"üìä ESTAT√çSTICAS GERAIS DOS DADOS BRUTOS:\")\n",
    "    print(f\"  Total de amostras carregadas: {len(raw_data)}\")\n",
    "    print(f\"  Campos utilizados: title, content\")\n",
    "    print(f\"  Outros campos: desconsiderados conforme solicitado\")\n",
    "    \n",
    "    # An√°lise de qualidade inicial\n",
    "    print(f\"\\nüîç AN√ÅLISE DE QUALIDADE INICIAL:\")\n",
    "    \n",
    "    # Verifica tamanhos dos textos\n",
    "    title_lengths = [len(item['title']) for item in raw_data]\n",
    "    content_lengths = [len(item['content']) for item in raw_data]\n",
    "    \n",
    "    print(f\"  T√≠tulos muito curtos (<3 chars): {sum(1 for x in title_lengths if x < 3)}\")\n",
    "    print(f\"  T√≠tulos muito longos (>200 chars): {sum(1 for x in title_lengths if x > 500)}\")\n",
    "    print(f\"  Conte√∫do muito curto (<5 chars): {sum(1 for x in content_lengths if x < 5)}\")\n",
    "    print(f\"  Conte√∫do muito longo (>1000 chars): {sum(1 for x in content_lengths if x > 100000)}\")\n",
    "    \n",
    "    # Verifica duplicatas\n",
    "    unique_titles = len(set(item['title'].lower() for item in raw_data))\n",
    "    duplicates = len(raw_data) - unique_titles\n",
    "    print(f\"  T√≠tulos duplicados: {duplicates}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Dados precisam de limpeza antes do treinamento!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado foi carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90519116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada dos dados brutos\n",
    "if raw_data:\n",
    "    print(\"üìè AN√ÅLISE DETALHADA DOS DADOS BRUTOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calcula estat√≠sticas de comprimento\n",
    "    title_lengths = [len(item['title']) for item in raw_data]\n",
    "    content_lengths = [len(item['content']) for item in raw_data]\n",
    "    \n",
    "    title_words = [len(item['title'].split()) for item in raw_data]\n",
    "    content_words = [len(item['content'].split()) for item in raw_data]\n",
    "    \n",
    "    print(\"üìù Comprimento em caracteres:\")\n",
    "    print(f\"  T√≠tulos - M√≠n: {min(title_lengths)}, M√°x: {max(title_lengths)}, M√©dia: {np.mean(title_lengths):.1f}\")\n",
    "    print(f\"  Conte√∫do - M√≠n: {min(content_lengths)}, M√°x: {max(content_lengths)}, M√©dia: {np.mean(content_lengths):.1f}\")\n",
    "    \n",
    "    print(\"\\nüî§ Comprimento em palavras:\")\n",
    "    print(f\"  T√≠tulos - M√≠n: {min(title_words)}, M√°x: {max(title_words)}, M√©dia: {np.mean(title_words):.1f}\")\n",
    "    print(f\"  Conte√∫do - M√≠n: {min(content_words)}, M√°x: {max(content_words)}, M√©dia: {np.mean(content_words):.1f}\")\n",
    "    \n",
    "    # Exemplos de diferentes tamanhos\n",
    "    print(\"\\nüìã EXEMPLOS DE PRODUTOS (DADOS BRUTOS):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # T√≠tulo mais curto\n",
    "    shortest_idx = title_lengths.index(min(title_lengths))\n",
    "    print(f\"üî∏ T√≠tulo mais curto ({len(raw_data[shortest_idx]['title'])} chars):\")\n",
    "    print(f\"  T√≠tulo: {raw_data[shortest_idx]['title']}\")\n",
    "    print(f\"  Conte√∫do: {raw_data[shortest_idx]['content']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    \n",
    "    # T√≠tulo mais longo\n",
    "    longest_idx = title_lengths.index(max(title_lengths))\n",
    "    print(f\"üî∏ T√≠tulo mais longo ({len(raw_data[longest_idx]['title'])} chars):\")\n",
    "    print(f\"  T√≠tulo: {raw_data[longest_idx]['title']}\")\n",
    "    print(f\"  Conte√∫do: {raw_data[longest_idx]['content'][:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    \n",
    "    # Exemplo aleat√≥rio\n",
    "    import random\n",
    "    random_idx = random.randint(0, len(raw_data)-1)\n",
    "    print(f\"üî∏ Exemplo aleat√≥rio:\")\n",
    "    print(f\"  T√≠tulo: {raw_data[random_idx]['title']}\")\n",
    "    print(f\"  Conte√∫do: {raw_data[random_idx]['content']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para an√°lise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252fb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes dos dados brutos\n",
    "if raw_data:\n",
    "    print(\"üìä CRIANDO VISUALIZA√á√ïES DOS DADOS BRUTOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Recalcula as estat√≠sticas para as visualiza√ß√µes\n",
    "    title_lengths = [len(item['title']) for item in raw_data]\n",
    "    content_lengths = [len(item['content']) for item in raw_data]\n",
    "    title_words = [len(item['title'].split()) for item in raw_data]\n",
    "    content_words = [len(item['content'].split()) for item in raw_data]\n",
    "    \n",
    "    # Configura√ß√£o do matplotlib\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('An√°lise dos Dados Brutos - Amazon Products', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 1: Distribui√ß√£o do comprimento dos t√≠tulos\n",
    "    axes[0,0].hist(title_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,0].set_title('Distribui√ß√£o - Comprimento dos T√≠tulos (caracteres)')\n",
    "    axes[0,0].set_xlabel('N√∫mero de caracteres')\n",
    "    axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].axvline(x=3, color='red', linestyle='--', alpha=0.7, label='M√≠n (3)')\n",
    "    axes[0,0].axvline(x=200, color='red', linestyle='--', alpha=0.7, label='M√°x (200)')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o do comprimento do conte√∫do\n",
    "    axes[0,1].hist(content_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_title('Distribui√ß√£o - Comprimento do Conte√∫do (caracteres)')\n",
    "    axes[0,1].set_xlabel('N√∫mero de caracteres')\n",
    "    axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].axvline(x=5, color='red', linestyle='--', alpha=0.7, label='M√≠n (5)')\n",
    "    axes[0,1].axvline(x=1000, color='red', linestyle='--', alpha=0.7, label='M√°x (1000)')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Gr√°fico 3: Distribui√ß√£o de palavras nos t√≠tulos\n",
    "    axes[1,0].hist(title_words, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1,0].set_title('Distribui√ß√£o - Palavras nos T√≠tulos')\n",
    "    axes[1,0].set_xlabel('N√∫mero de palavras')\n",
    "    axes[1,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 4: Rela√ß√£o entre t√≠tulo e conte√∫do\n",
    "    axes[1,1].scatter(title_lengths, content_lengths, alpha=0.5, color='purple', s=10)\n",
    "    axes[1,1].set_title('Rela√ß√£o: T√≠tulo vs Conte√∫do (caracteres)')\n",
    "    axes[1,1].set_xlabel('Comprimento do t√≠tulo')\n",
    "    axes[1,1].set_ylabel('Comprimento do conte√∫do')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualiza√ß√µes dos dados brutos criadas!\")\n",
    "    print(\"‚ö†Ô∏è Linhas vermelhas mostram limites para limpeza\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para visualiza√ß√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e5cf5",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o do Dataset\n",
    "\n",
    "### 3.1 Formata√ß√£o dos Dados para Fine-tuning\n",
    "\n",
    "Agora vamos formatar os dados no padr√£o esperado pelo modelo. Criaremos prompts estruturados onde:\n",
    "- **Input**: T√≠tulo do produto\n",
    "- **Output**: Descri√ß√£o do produto\n",
    "\n",
    "O formato seguir√° o padr√£o de chat do Llama 3, usando tags especiais para delimitar o in√≠cio e fim das respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da13b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(title, content):\n",
    "    \"\"\"\n",
    "    Formata um exemplo de treinamento no padr√£o do Llama 3\n",
    "    \n",
    "    Args:\n",
    "        title: T√≠tulo do produto\n",
    "        content: Descri√ß√£o do produto\n",
    "    \n",
    "    Returns:\n",
    "        String formatada para treinamento\n",
    "    \"\"\"\n",
    "    \n",
    "    # Template de prompt para o modelo\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Voc√™ √© um assistente especializado em produtos da Amazon. Sua tarefa √© gerar descri√ß√µes detalhadas e precisas de produtos baseadas apenas no t√≠tulo fornecido. As descri√ß√µes devem ser informativas, concisas e atrativas para potenciais compradores.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Gere uma descri√ß√£o para o seguinte produto: {title}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{content}<|eot_id|><|end_of_text|>\"\"\"\n",
    "    \n",
    "    return prompt_template.format(title=title, content=content)\n",
    "\n",
    "def prepare_training_data(data, train_size=None):\n",
    "    \"\"\"\n",
    "    Prepara os dados para treinamento formatando cada exemplo\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de dicion√°rios com title e content\n",
    "        train_size: N√∫mero m√°ximo de exemplos para treinamento\n",
    "    \n",
    "    Returns:\n",
    "        Lista de strings formatadas para treinamento\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Preparando dados para treinamento...\")\n",
    "    \n",
    "    if train_size:\n",
    "        data = data[:train_size]\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        formatted_prompt = format_prompt(item['title'], item['content'])\n",
    "        formatted_data.append(formatted_prompt)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"  Formatados {i + 1} exemplos...\")\n",
    "    \n",
    "    print(f\"‚úÖ Prepara√ß√£o conclu√≠da! Total: {len(formatted_data)} exemplos formatados\")\n",
    "    return formatted_data\n",
    "\n",
    "# Testa a formata√ß√£o com um exemplo dos dados brutos\n",
    "if raw_data:\n",
    "    print(\"üß™ TESTE DE FORMATA√á√ÉO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pega um exemplo para demonstrar a formata√ß√£o\n",
    "    test_example = raw_data[0]\n",
    "    formatted_example = format_prompt(test_example['title'], test_example['content'])\n",
    "    \n",
    "    print(\"üìù Exemplo original:\")\n",
    "    print(f\"  T√≠tulo: {test_example['title']}\")\n",
    "    print(f\"  Conte√∫do: {test_example['content']}\")\n",
    "    \n",
    "    print(\"\\nüéØ Exemplo formatado para treinamento:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(formatted_example)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para teste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23a8bf",
   "metadata": {},
   "source": [
    "### 3.3 Divis√£o em Conjuntos de Treino e Teste\n",
    "\n",
    "Agora que temos os dados limpos, vamos dividi-los em conjuntos de treinamento e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358165c",
   "metadata": {},
   "source": [
    "### 3.2 Limpeza e Pr√©-processamento dos Dados\n",
    "\n",
    "Antes de prosseguir com o treinamento, √© essencial limpar os dados removendo duplicatas, valores nulos, textos muito curtos ou muito longos, e outros problemas de qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amazon_data(data, min_title_length=3, max_title_length=2000, \n",
    "                      min_content_length=5, max_content_length=10000):\n",
    "    \"\"\"\n",
    "    Limpa e pr√©-processa os dados do Amazon dataset\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de dicion√°rios com title e content\n",
    "        min_title_length: Comprimento m√≠nimo do t√≠tulo\n",
    "        max_title_length: Comprimento m√°ximo do t√≠tulo\n",
    "        min_content_length: Comprimento m√≠nimo do conte√∫do\n",
    "        max_content_length: Comprimento m√°ximo do conte√∫do\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dados limpos e estat√≠sticas da limpeza\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üßπ INICIANDO LIMPEZA DOS DADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Estat√≠sticas iniciais\n",
    "    initial_count = len(data)\n",
    "    print(f\"üìä Dados iniciais: {initial_count} amostras\")\n",
    "    \n",
    "    # 1. Remove valores nulos ou vazios\n",
    "    print(\"\\nüîç 1. Removendo valores nulos ou vazios...\")\n",
    "    data = [item for item in data if item.get('title') and item.get('content')]\n",
    "    after_null_removal = len(data)\n",
    "    removed_null = initial_count - after_null_removal\n",
    "    print(f\"   Removidas: {removed_null} amostras\")\n",
    "    print(f\"   Restantes: {after_null_removal} amostras\")\n",
    "    \n",
    "    # 2. Remove whitespace extra e normaliza\n",
    "    print(\"\\n‚úÇÔ∏è 2. Normalizando espa√ßos em branco...\")\n",
    "    for item in data:\n",
    "        item['title'] = ' '.join(item['title'].split())  # Remove espa√ßos extras\n",
    "        item['content'] = ' '.join(item['content'].split())\n",
    "    \n",
    "    # 3. Remove duplicatas baseadas no t√≠tulo\n",
    "    print(\"\\nüîÑ 3. Removendo duplicatas...\")\n",
    "    seen_titles = set()\n",
    "    unique_data = []\n",
    "    duplicates_removed = 0\n",
    "    \n",
    "    for item in data:\n",
    "        title_lower = item['title'].lower()\n",
    "        if title_lower not in seen_titles:\n",
    "            seen_titles.add(title_lower)\n",
    "            unique_data.append(item)\n",
    "        else:\n",
    "            duplicates_removed += 1\n",
    "    \n",
    "    data = unique_data\n",
    "    print(f\"   Duplicatas removidas: {duplicates_removed}\")\n",
    "    print(f\"   Restantes: {len(data)} amostras\")\n",
    "    \n",
    "    # 4. Filtra por comprimento do t√≠tulo\n",
    "    print(f\"\\nüìè 4. Filtrando t√≠tulos (min: {min_title_length}, max: {max_title_length} chars)...\")\n",
    "    before_title_filter = len(data)\n",
    "    data = [item for item in data if min_title_length <= len(item['title']) <= max_title_length]\n",
    "    removed_title = before_title_filter - len(data)\n",
    "    print(f\"   Removidas: {removed_title} amostras\")\n",
    "    print(f\"   Restantes: {len(data)} amostras\")\n",
    "    \n",
    "    # 5. Filtra por comprimento do conte√∫do\n",
    "    print(f\"\\nüìÑ 5. Filtrando conte√∫do (min: {min_content_length}, max: {max_content_length} chars)...\")\n",
    "    before_content_filter = len(data)\n",
    "    data = [item for item in data if min_content_length <= len(item['content']) <= max_content_length]\n",
    "    removed_content = before_content_filter - len(data)\n",
    "    print(f\"   Removidas: {removed_content} amostras\")\n",
    "    print(f\"   Restantes: {len(data)} amostras\")\n",
    "    \n",
    "    # 7. Verifica√ß√£o final de qualidade\n",
    "    print(\"\\n‚úÖ 7. Verifica√ß√£o final de qualidade...\")\n",
    "    final_data = []\n",
    "    removed_final = 0\n",
    "    \n",
    "    for item in data:\n",
    "        # Verifica se ainda tem conte√∫do v√°lido\n",
    "        if (item['title'].strip() and item['content'].strip() and \n",
    "            len(item['title'].strip()) >= min_title_length and\n",
    "            len(item['content'].strip()) >= min_content_length):\n",
    "            final_data.append(item)\n",
    "        else:\n",
    "            removed_final += 1\n",
    "    \n",
    "    data = final_data\n",
    "    print(f\"   Removidas na verifica√ß√£o final: {removed_final}\")\n",
    "    print(f\"   Total final: {len(data)} amostras\")\n",
    "    \n",
    "    # Estat√≠sticas de limpeza\n",
    "    total_removed = initial_count - len(data)\n",
    "    retention_rate = (len(data) / initial_count) * 100\n",
    "    \n",
    "    print(f\"\\nüìà RESUMO DA LIMPEZA:\")\n",
    "    print(f\"   Dados iniciais: {initial_count}\")\n",
    "    print(f\"   Dados finais: {len(data)}\")\n",
    "    print(f\"   Total removido: {total_removed} ({(total_removed/initial_count)*100:.1f}%)\")\n",
    "    print(f\"   Taxa de reten√ß√£o: {retention_rate:.1f}%\")\n",
    "    \n",
    "    # Estat√≠sticas dos dados limpos\n",
    "    if data:\n",
    "        title_lengths = [len(item['title']) for item in data]\n",
    "        content_lengths = [len(item['content']) for item in data]\n",
    "        \n",
    "        print(f\"\\nüìä ESTAT√çSTICAS DOS DADOS LIMPOS:\")\n",
    "        print(f\"   T√≠tulos - M√≠n: {min(title_lengths)}, M√°x: {max(title_lengths)}, M√©dia: {np.mean(title_lengths):.1f}\")\n",
    "        print(f\"   Conte√∫do - M√≠n: {min(content_lengths)}, M√°x: {max(content_lengths)}, M√©dia: {np.mean(content_lengths):.1f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Aplica a limpeza nos dados carregados\n",
    "if raw_data:\n",
    "    print(\"üîÑ Aplicando limpeza nos dados carregados...\")\n",
    "    cleaned_data = clean_amazon_data(raw_data.copy())\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para limpeza.\")\n",
    "    cleaned_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b571ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o visual: Antes vs Depois da limpeza\n",
    "if raw_data and cleaned_data:\n",
    "    print(\"üìä COMPARA√á√ÉO VISUAL: ANTES vs DEPOIS DA LIMPEZA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Cria visualiza√ß√µes comparativas\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Compara√ß√£o: Dados Originais vs Dados Limpos', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Dados originais\n",
    "    orig_title_lengths = [len(item['title']) for item in raw_data]\n",
    "    orig_content_lengths = [len(item['content']) for item in raw_data]\n",
    "    \n",
    "    # Dados limpos\n",
    "    clean_title_lengths = [len(item['title']) for item in cleaned_data]\n",
    "    clean_content_lengths = [len(item['content']) for item in cleaned_data]\n",
    "    \n",
    "    # Gr√°fico 1: Contagem de amostras\n",
    "    categories = ['Dados Originais', 'Dados Limpos']\n",
    "    counts = [len(raw_data), len(cleaned_data)]\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    axes[0,0].bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_title('N√∫mero Total de Amostras')\n",
    "    axes[0,0].set_ylabel('Quantidade')\n",
    "    for i, v in enumerate(counts):\n",
    "        axes[0,0].text(i, v + max(counts)*0.01, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o t√≠tulos - Originais\n",
    "    axes[0,1].hist(orig_title_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_title('T√≠tulos - Dados Originais')\n",
    "    axes[0,1].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 3: Distribui√ß√£o t√≠tulos - Limpos\n",
    "    axes[0,2].hist(clean_title_lengths, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,2].set_title('T√≠tulos - Dados Limpos')\n",
    "    axes[0,2].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[0,2].set_ylabel('Frequ√™ncia')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 4: Distribui√ß√£o conte√∫do - Originais\n",
    "    axes[1,0].hist(orig_content_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1,0].set_title('Conte√∫do - Dados Originais')\n",
    "    axes[1,0].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[1,0].set_ylabel('Frequ√™ncia')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 5: Distribui√ß√£o conte√∫do - Limpos\n",
    "    axes[1,1].hist(clean_content_lengths, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1,1].set_title('Conte√∫do - Dados Limpos')\n",
    "    axes[1,1].set_xlabel('Comprimento (caracteres)')\n",
    "    axes[1,1].set_ylabel('Frequ√™ncia')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 6: Boxplot comparativo\n",
    "    data_to_plot = [orig_title_lengths, clean_title_lengths, orig_content_lengths, clean_content_lengths]\n",
    "    labels = ['T√≠tulos\\\\nOriginais', 'T√≠tulos\\\\nLimpos', 'Conte√∫do\\\\nOriginais', 'Conte√∫do\\\\nLimpos']\n",
    "    colors_box = ['lightcoral', 'lightgreen', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    bp = axes[1,2].boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[1,2].set_title('Compara√ß√£o de Distribui√ß√µes')\n",
    "    axes[1,2].set_ylabel('Comprimento (caracteres)')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas comparativas em tabela\n",
    "    print(\"\\\\nüìã ESTAT√çSTICAS COMPARATIVAS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'M√©trica':<25} {'Originais':<15} {'Limpos':<15} {'Varia√ß√£o':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Total de amostras\n",
    "    original_count = len(raw_data)\n",
    "    clean_count = len(cleaned_data)\n",
    "    variation = ((clean_count - original_count) / original_count) * 100\n",
    "    print(f\"{'Total de amostras':<25} {original_count:<15} {clean_count:<15} {variation:+.1f}%\")\n",
    "    \n",
    "    # Estat√≠sticas de t√≠tulos\n",
    "    orig_title_mean = np.mean(orig_title_lengths)\n",
    "    clean_title_mean = np.mean(clean_title_lengths)\n",
    "    title_variation = ((clean_title_mean - orig_title_mean) / orig_title_mean) * 100\n",
    "    print(f\"{'T√≠tulo m√©dio (chars)':<25} {orig_title_mean:<15.1f} {clean_title_mean:<15.1f} {title_variation:+.1f}%\")\n",
    "    \n",
    "    # Estat√≠sticas de conte√∫do\n",
    "    orig_content_mean = np.mean(orig_content_lengths)\n",
    "    clean_content_mean = np.mean(clean_content_lengths)\n",
    "    content_variation = ((clean_content_mean - orig_content_mean) / orig_content_mean) * 100\n",
    "    print(f\"{'Conte√∫do m√©dio (chars)':<25} {orig_content_mean:<15.1f} {clean_content_mean:<15.1f} {content_variation:+.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados para compara√ß√£o n√£o dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c445af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos de dados problem√°ticos que foram removidos\n",
    "if raw_data and cleaned_data:\n",
    "    print(\"üîç EXEMPLOS DE DADOS PROBLEM√ÅTICOS REMOVIDOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Identifica dados que foram removidos\n",
    "    original_titles = {item['title'] for item in raw_data}\n",
    "    clean_titles = {item['title'] for item in cleaned_data}\n",
    "    removed_titles = original_titles - clean_titles\n",
    "    \n",
    "    if removed_titles:\n",
    "        print(f\"üìä Total de t√≠tulos √∫nicos removidos: {len(removed_titles)}\")\n",
    "        \n",
    "        # Encontra exemplos espec√≠ficos dos dados removidos\n",
    "        removed_examples = []\n",
    "        for item in raw_data:\n",
    "            if item['title'] in removed_titles:\n",
    "                removed_examples.append(item)\n",
    "                if len(removed_examples) >= 5:  # Mostra at√© 5 exemplos\n",
    "                    break\n",
    "        \n",
    "        print(\"\\\\n‚ùå EXEMPLOS DE DADOS REMOVIDOS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, example in enumerate(removed_examples, 1):\n",
    "            print(f\"\\\\n{i}. EXEMPLO PROBLEM√ÅTICO:\")\n",
    "            print(f\"   T√≠tulo: '{example['title']}'\")\n",
    "            print(f\"   Conte√∫do: '{example['content']}'\")\n",
    "            \n",
    "            # Identifica o problema\n",
    "            problems = []\n",
    "            if len(example['title']) < 3:\n",
    "                problems.append(f\"t√≠tulo muito curto ({len(example['title'])} chars)\")\n",
    "            if len(example['title']) > 200:\n",
    "                problems.append(f\"t√≠tulo muito longo ({len(example['title'])} chars)\")\n",
    "            if len(example['content']) < 5:\n",
    "                problems.append(f\"conte√∫do muito curto ({len(example['content'])} chars)\")\n",
    "            if len(example['content']) > 1000:\n",
    "                problems.append(f\"conte√∫do muito longo ({len(example['content'])} chars)\")\n",
    "            if not example['title'].strip():\n",
    "                problems.append(\"t√≠tulo vazio\")\n",
    "            if not example['content'].strip():\n",
    "                problems.append(\"conte√∫do vazio\")\n",
    "            \n",
    "            print(f\"   ‚ö†Ô∏è Problema(s): {', '.join(problems) if problems else 'duplicata'}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚úÖ Nenhum dado foi removido - todos estavam dentro dos crit√©rios!\")\n",
    "    \n",
    "    # Mostra exemplos de dados que PERMANECERAM ap√≥s limpeza\n",
    "    print(\"\\\\n\\\\n‚úÖ EXEMPLOS DE DADOS LIMPOS (QUE PERMANECERAM):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(min(3, len(cleaned_data))):\n",
    "        example = cleaned_data[i]\n",
    "        print(f\"\\\\n{i+1}. EXEMPLO LIMPO:\")\n",
    "        print(f\"   T√≠tulo: '{example['title']}'\")\n",
    "        print(f\"   Conte√∫do: '{example['content']}'\")\n",
    "        print(f\"   üìè T√≠tulo: {len(example['title'])} chars, Conte√∫do: {len(example['content'])} chars\")\n",
    "        print(f\"   ‚úÖ Status: Dados v√°lidos e dentro dos par√¢metros\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dados para an√°lise de exemplos n√£o dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o dos dados limpos em treino e teste (OTIMIZADO)\n",
    "if cleaned_data:\n",
    "    print(\"‚ö° DIVIDINDO DADOS LIMPOS EM TREINO E TESTE (OTIMIZADO)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"üìä Total de dados limpos dispon√≠veis: {len(cleaned_data):,}\")\n",
    "    \n",
    "    # Configura√ß√£o da divis√£o\n",
    "    test_size = min(CONFIG['test_size'], len(cleaned_data) // 10)  # M√°ximo 10% para teste\n",
    "    train_size = len(cleaned_data) - test_size\n",
    "    \n",
    "    print(f\"üéØ Configura√ß√£o da divis√£o:\")\n",
    "    print(f\"   Treino: {train_size:,} amostras\")\n",
    "    print(f\"   Teste: {test_size:,} amostras\")\n",
    "    \n",
    "    # OTIMIZA√á√ÉO: Usa random.sample ao inv√©s de shuffle (muito mais r√°pido!)\n",
    "    import random\n",
    "    print(f\"‚ö° Selecionando amostras aleat√≥rias (sem shuffle)...\")\n",
    "    \n",
    "    # Gera √≠ndices aleat√≥rios para teste\n",
    "    test_indices = set(random.sample(range(len(cleaned_data)), test_size))\n",
    "    \n",
    "    # Separa os dados sem embaralhar a lista inteira\n",
    "    test_data = [cleaned_data[i] for i in test_indices]\n",
    "    train_data = [cleaned_data[i] for i in range(len(cleaned_data)) if i not in test_indices]\n",
    "    \n",
    "    print(f\"  üìö Dados de treinamento: {len(train_data):,}\")\n",
    "    print(f\"  üß™ Dados de teste: {len(test_data):,}\")\n",
    "    print(f\"  üìä Propor√ß√£o treino/teste: {len(train_data)/len(test_data):.1f}\")\n",
    "    \n",
    "    # Estat√≠sticas dos conjuntos\n",
    "    train_title_lengths = [len(item['title']) for item in train_data]\n",
    "    train_content_lengths = [len(item['content']) for item in train_data]\n",
    "    \n",
    "    test_title_lengths = [len(item['title']) for item in test_data]\n",
    "    test_content_lengths = [len(item['content']) for item in test_data]\n",
    "    \n",
    "    print(f\"\\nüìè ESTAT√çSTICAS DO CONJUNTO DE TREINAMENTO:\")\n",
    "    print(f\"  T√≠tulos - M√©dia: {np.mean(train_title_lengths):.1f} caracteres\")\n",
    "    print(f\"  Conte√∫do - M√©dia: {np.mean(train_content_lengths):.1f} caracteres\")\n",
    "    \n",
    "    print(f\"\\nüìè ESTAT√çSTICAS DO CONJUNTO DE TESTE:\")\n",
    "    print(f\"  T√≠tulos - M√©dia: {np.mean(test_title_lengths):.1f} caracteres\")\n",
    "    print(f\"  Conte√∫do - M√©dia: {np.mean(test_content_lengths):.1f} caracteres\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Divis√£o conclu√≠da com sucesso! (Muito mais r√°pido!)\")\n",
    "    print(f\"üéØ Dados prontos para formata√ß√£o e treinamento\")\n",
    "    print(f\"‚ö° Tempo economizado: ~90% menos tempo que shuffle()\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado limpo dispon√≠vel para divis√£o.\")\n",
    "    train_data = []\n",
    "    test_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ab3f0",
   "metadata": {},
   "source": [
    "### 3.4 Cria√ß√£o do Dataset no Formato Hugging Face\n",
    "\n",
    "Vamos criar datasets no formato esperado pela biblioteca Hugging Face para facilitar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara os dados formatados para treinamento\n",
    "if 'train_data' in locals() and train_data:\n",
    "    print(\"üîß CRIANDO DATASETS FORMATADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Formata os dados de treinamento\n",
    "    formatted_train_data = prepare_training_data(train_data)\n",
    "    \n",
    "    # Cria o dataset de treinamento no formato Hugging Face\n",
    "    train_dataset_dict = {\n",
    "        'text': formatted_train_data\n",
    "    }\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(train_dataset_dict)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset de treinamento criado:\")\n",
    "    print(f\"   N√∫mero de exemplos: {len(train_dataset)}\")\n",
    "    print(f\"   Colunas: {train_dataset.column_names}\")\n",
    "    \n",
    "    # Salva algumas amostras dos dados de teste para avalia√ß√£o posterior\n",
    "    test_samples = test_data[:10]  # Primeiras 10 amostras para teste\n",
    "    \n",
    "    print(f\"\\nüìã Amostras de teste separadas: {len(test_samples)}\")\n",
    "    \n",
    "    # Mostra estat√≠sticas do dataset final\n",
    "    text_lengths = [len(text) for text in formatted_train_data]\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DO DATASET FORMATADO:\")\n",
    "    print(f\"   Comprimento m√©dio do texto: {np.mean(text_lengths):.0f} caracteres\")\n",
    "    print(f\"   Comprimento m√≠nimo: {min(text_lengths)} caracteres\")\n",
    "    print(f\"   Comprimento m√°ximo: {max(text_lengths)} caracteres\")\n",
    "    \n",
    "    # Verifica se os textos n√£o s√£o muito longos para o modelo\n",
    "    max_length = CONFIG['max_seq_length']\n",
    "    long_texts = [t for t in text_lengths if t > max_length * 4]  # Aproximadamente 4 chars por token\n",
    "    \n",
    "    if long_texts:\n",
    "        print(f\"   ‚ö†Ô∏è Textos muito longos: {len(long_texts)} ({len(long_texts)/len(text_lengths)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Todos os textos est√£o dentro do limite esperado\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados de treinamento n√£o dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra exemplos do dataset formatado\n",
    "if 'train_dataset' in locals() and train_dataset:\n",
    "    print(\"üëÄ VISUALIZA√á√ÉO DOS DADOS FORMATADOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Mostra 2 exemplos completos\n",
    "    for i in range(min(2, len(train_dataset))):\n",
    "        print(f\"\\nüìù EXEMPLO {i+1}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Pega o texto formatado\n",
    "        formatted_text = train_dataset[i]['text']\n",
    "        \n",
    "        # Extrai partes espec√≠ficas para visualiza√ß√£o\n",
    "        lines = formatted_text.split('\\n')\n",
    "        \n",
    "        # Encontra o t√≠tulo (depois de \"Gere uma descri√ß√£o para o seguinte produto:\")\n",
    "        title_line = None\n",
    "        content_start = None\n",
    "        \n",
    "        for j, line in enumerate(lines):\n",
    "            if \"Gere uma descri√ß√£o para o seguinte produto:\" in line:\n",
    "                if j + 1 < len(lines):\n",
    "                    title_line = lines[j + 1].strip()\n",
    "            elif \"<|start_header_id|>assistant<|end_header_id|>\" in line:\n",
    "                content_start = j + 1\n",
    "                break\n",
    "        \n",
    "        if title_line:\n",
    "            print(f\"üè∑Ô∏è  T√≠tulo: {title_line}\")\n",
    "        \n",
    "        if content_start and content_start < len(lines):\n",
    "            # Pega o conte√∫do (at√© encontrar <|eot_id|>)\n",
    "            content_lines = []\n",
    "            for k in range(content_start, len(lines)):\n",
    "                if \"<|eot_id|>\" in lines[k]:\n",
    "                    break\n",
    "                if lines[k].strip():\n",
    "                    content_lines.append(lines[k].strip())\n",
    "            \n",
    "            content = \" \".join(content_lines)\n",
    "            if content:\n",
    "                print(f\"üìÑ Descri√ß√£o: {content}\")\n",
    "        \n",
    "        print(f\"üìè Comprimento total: {len(formatted_text)} caracteres\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset pronto para treinamento!\")\n",
    "    print(f\"üìä Resumo final: {len(train_dataset)} exemplos formatados\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset formatado n√£o dispon√≠vel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af266912",
   "metadata": {},
   "source": [
    "## 4. Teste do Modelo Base\n",
    "\n",
    "### 4.1 Carregamento do Modelo Base\n",
    "\n",
    "Agora vamos carregar o modelo Llama 3-8B base (antes do fine-tuning) para testar sua performance inicial na tarefa de gera√ß√£o de descri√ß√µes de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do modelo base para teste inicial\n",
    "print(\"ü§ñ CARREGANDO MODELO BASE LLAMA 3-8B\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configura√ß√µes do modelo\n",
    "model_name = CONFIG['model_name']\n",
    "max_seq_length = CONFIG['max_seq_length']\n",
    "\n",
    "print(f\"üì¶ Modelo: {model_name}\")\n",
    "print(f\"üìè Comprimento m√°ximo de sequ√™ncia: {max_seq_length}\")\n",
    "print(f\"üîß Carregando em 4-bit: {CONFIG['load_in_4bit']}\")\n",
    "\n",
    "# Carrega o modelo e tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=CONFIG['dtype'],\n",
    "    load_in_4bit=CONFIG['load_in_4bit'],\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo carregado com sucesso!\")\n",
    "print(f\"üß† Par√¢metros do modelo: {model.num_parameters():,}\")\n",
    "\n",
    "# CORRE√á√ÉO: Configura√ß√£o inicial do tokenizer\n",
    "print(f\"\\nüîß CONFIGURANDO TOKENIZER:\")\n",
    "print(f\"   Tokenizer original: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Verifica e configura pad_token se necess√°rio\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\"   ‚úÖ Pad token configurado: {tokenizer.pad_token}\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Pad token j√° existe: {tokenizer.pad_token}\")\n",
    "\n",
    "# Configura√ß√µes adicionais do tokenizer\n",
    "tokenizer.padding_side = \"right\"  # Importante para modelos causais\n",
    "print(f\"   ‚úÖ Padding side: {tokenizer.padding_side}\")\n",
    "\n",
    "print(f\"\\nüìã TOKENS ESPECIAIS:\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"   BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"   PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"   UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "\n",
    "# Configura√ß√£o para gera√ß√£o de texto\n",
    "FastLanguageModel.for_inference(model)  # Habilita modo de infer√™ncia\n",
    "\n",
    "print(f\"\\nüöÄ Modelo e tokenizer prontos para uso!\")\n",
    "print(f\"‚úÖ Configura√ß√µes otimizadas para evitar erros de tokeniza√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e1448",
   "metadata": {},
   "source": [
    "### 4.2 Fun√ß√£o de Teste para Gera√ß√£o de Descri√ß√µes\n",
    "\n",
    "Vamos criar uma fun√ß√£o para testar o modelo base gerando descri√ß√µes a partir de t√≠tulos de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_generation(model, tokenizer, title, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Testa a gera√ß√£o de descri√ß√£o para um t√≠tulo usando o modelo\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo carregado\n",
    "        tokenizer: Tokenizer\n",
    "        title: T√≠tulo do produto\n",
    "        max_new_tokens: M√°ximo de tokens a gerar\n",
    "        temperature: Controla criatividade (0.1 = conservador, 1.0 = criativo)\n",
    "        top_p: Controla diversidade do vocabul√°rio\n",
    "    \n",
    "    Returns:\n",
    "        Descri√ß√£o gerada pelo modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cria o prompt para o modelo\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Voc√™ √© um assistente especializado em produtos da Amazon. Sua tarefa √© gerar descri√ß√µes detalhadas e precisas de produtos baseadas apenas no t√≠tulo fornecido. As descri√ß√µes devem ser informativas, concisas e atrativas para potenciais compradores.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Gere uma descri√ß√£o para o seguinte produto: {title}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokeniza o prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Gera a resposta\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decodifica apenas a parte gerada (remove o prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Limpa a resposta removendo tokens de fim\n",
    "    generated_text = generated_text.split('<|eot_id|>')[0].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de teste criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e89faf",
   "metadata": {},
   "source": [
    "### 4.3 Teste do Modelo Base com Amostras Reais\n",
    "\n",
    "Vamos testar o modelo base com algumas amostras do nosso dataset para avaliar sua performance inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo base com amostras reais do dataset\n",
    "if 'test_samples' in locals() and test_samples and 'model' in locals() and 'tokenizer' in locals():\n",
    "    print(\"üß™ TESTANDO MODELO BASE COM AMOSTRAS REAIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Testa com algumas amostras\n",
    "    num_tests = min(5, len(test_samples))\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        print(f\"\\nüî¨ TESTE {i+1}/{num_tests}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        sample = test_samples[i]\n",
    "        title = sample['title']\n",
    "        real_content = sample['content']\n",
    "        \n",
    "        print(f\"üè∑Ô∏è  T√≠tulo: {title}\")\n",
    "        print(f\"üìÑ Descri√ß√£o Real: {real_content}\")\n",
    "        \n",
    "        # Gera descri√ß√£o com o modelo base\n",
    "        print(f\"\\nü§ñ Gerando com modelo base...\")\n",
    "        try:\n",
    "            generated_content = test_model_generation(\n",
    "                model, tokenizer, title, \n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(f\"üîÆ Descri√ß√£o Gerada: {generated_content}\")\n",
    "            \n",
    "            # An√°lise b√°sica de qualidade\n",
    "            real_length = len(real_content)\n",
    "            gen_length = len(generated_content)\n",
    "            length_ratio = gen_length / real_length if real_length > 0 else 0\n",
    "            \n",
    "            print(f\"\\nüìä An√°lise:\")\n",
    "            print(f\"   Comprimento real: {real_length} caracteres\")\n",
    "            print(f\"   Comprimento gerado: {gen_length} caracteres\")\n",
    "            print(f\"   Raz√£o de comprimento: {length_ratio:.2f}\")\n",
    "            \n",
    "            # Verifica palavras-chave do t√≠tulo na descri√ß√£o gerada\n",
    "            title_words = set(title.lower().split())\n",
    "            gen_words = set(generated_content.lower().split())\n",
    "            common_words = title_words.intersection(gen_words)\n",
    "            relevance_score = len(common_words) / len(title_words) if title_words else 0\n",
    "            \n",
    "            print(f\"   Relev√¢ncia (palavras em comum): {relevance_score:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na gera√ß√£o: {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Teste do modelo base conclu√≠do!\")\n",
    "    print(f\"üí° Observe a qualidade das descri√ß√µes geradas antes do fine-tuning\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados de teste ou modelo n√£o dispon√≠veis para teste.\")\n",
    "    print(\"Execute as c√©lulas anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639eb9b",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning\n",
    "\n",
    "### 5.1 Configura√ß√£o do LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Agora vamos configurar o modelo para fine-tuning usando LoRA, que √© uma t√©cnica eficiente que permite treinar apenas uma pequena fra√ß√£o dos par√¢metros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o do modelo para fine-tuning com LoRA\n",
    "print(\"üîß CONFIGURANDO MODELO PARA FINE-TUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configura o modelo para treinamento\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG['lora_r'],  # Rank da matriz LoRA\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",  # Supports any, but only tested for \"none\"\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long contexts\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank Stabilized LoRA\n",
    "    loftq_config=None,  # LoftQ\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo configurado para LoRA fine-tuning!\")\n",
    "\n",
    "# Informa√ß√µes sobre par√¢metros trein√°veis\n",
    "total_params = model.num_parameters()\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\nüìä ESTAT√çSTICAS DO MODELO:\")\n",
    "print(f\"   Total de par√¢metros: {total_params:,}\")\n",
    "print(f\"   Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "print(f\"   Percentual trein√°vel: {percentage:.4f}%\")\n",
    "print(f\"   Configura√ß√£o LoRA - r: {CONFIG['lora_r']}, alpha: {CONFIG['lora_alpha']}\")\n",
    "\n",
    "print(f\"\\nüéØ LoRA permite treinar apenas {percentage:.4f}% dos par√¢metros!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61127fbb",
   "metadata": {},
   "source": [
    "### 5.2 Configura√ß√£o do Treinamento\n",
    "\n",
    "Vamos configurar os par√¢metros de treinamento e inicializar o trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos argumentos de treinamento\n",
    "print(\"‚öôÔ∏è CONFIGURANDO PAR√ÇMETROS DE TREINAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    warmup_steps=5,\n",
    "    max_steps=CONFIG['max_steps'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    save_steps=50,  # Salva checkpoint a cada 50 steps\n",
    "    save_total_limit=2,  # Mant√©m apenas os 2 √∫ltimos checkpoints\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Argumentos de treinamento configurados!\")\n",
    "print(f\"\\nüìã CONFIGURA√á√ïES DE TREINAMENTO:\")\n",
    "print(f\"   Batch size por device: {CONFIG['batch_size']}\")\n",
    "print(f\"   Gradient accumulation steps: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"   Batch size efetivo: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"   Precision: {'BF16' if is_bfloat16_supported() else 'FP16'}\")\n",
    "print(f\"   Output directory: {CONFIG['output_dir']}\")\n",
    "\n",
    "# C√°lculo estimado do tempo de treinamento\n",
    "effective_batch_size = CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']\n",
    "samples_per_step = effective_batch_size\n",
    "total_samples = CONFIG['max_steps'] * samples_per_step\n",
    "estimated_epochs = total_samples / len(train_dataset) if 'train_dataset' in locals() else 0\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è ESTIMATIVAS:\")\n",
    "print(f\"   Amostras por step: {samples_per_step}\")\n",
    "print(f\"   Total de amostras processadas: {total_samples:,}\")\n",
    "if 'train_dataset' in locals():\n",
    "    print(f\"   √âpocas estimadas: {estimated_epochs:.2f}\")\n",
    "    print(f\"   Dataset size: {len(train_dataset):,}\")\n",
    "else:\n",
    "    print(f\"   Dataset n√£o carregado ainda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc94825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida√ß√£o dos dados antes do treinamento\n",
    "print(\"üîç VALIDANDO DADOS ANTES DO TREINAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'train_dataset' in locals() and train_dataset:\n",
    "    print(f\"‚úÖ Dataset encontrado: {len(train_dataset)} exemplos\")\n",
    "    \n",
    "    # Verifica alguns exemplos\n",
    "    sample_texts = train_dataset['text'][:3]\n",
    "    \n",
    "    print(f\"\\nüß™ VALIDA√á√ÉO DE AMOSTRAS:\")\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        print(f\"\\n   Exemplo {i+1}:\")\n",
    "        print(f\"   Tipo: {type(text)}\")\n",
    "        print(f\"   Comprimento: {len(text)} caracteres\")\n",
    "        \n",
    "        # Verifica se √© string v√°lida\n",
    "        if isinstance(text, str):\n",
    "            print(f\"   ‚úÖ Formato correto (string)\")\n",
    "            \n",
    "            # Mostra in√≠cio e fim do texto\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"   Preview: {preview}\")\n",
    "            \n",
    "            # Testa tokeniza√ß√£o\n",
    "            try:\n",
    "                test_tokens = tokenizer(text, truncation=True, padding=False, max_length=CONFIG['max_seq_length'])\n",
    "                print(f\"   ‚úÖ Tokeniza√ß√£o bem-sucedida: {len(test_tokens['input_ids'])} tokens\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erro na tokeniza√ß√£o: {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Formato incorreto: esperado str, encontrado {type(text)}\")\n",
    "    \n",
    "    # Verifica distribui√ß√£o de comprimentos\n",
    "    text_lengths = [len(text) for text in sample_texts]\n",
    "    print(f\"\\nüìä DISTRIBUI√á√ÉO DE COMPRIMENTOS (amostra):\")\n",
    "    print(f\"   M√≠nimo: {min(text_lengths)} caracteres\")\n",
    "    print(f\"   M√°ximo: {max(text_lengths)} caracteres\")\n",
    "    print(f\"   M√©dia: {np.mean(text_lengths):.0f} caracteres\")\n",
    "    \n",
    "    # Verifica se h√° textos muito longos\n",
    "    max_chars = CONFIG['max_seq_length'] * 4  # ~4 chars por token\n",
    "    too_long = [len(text) for text in train_dataset['text'] if len(text) > max_chars]\n",
    "    \n",
    "    if too_long:\n",
    "        print(f\"   ‚ö†Ô∏è {len(too_long)} textos podem ser muito longos (>{max_chars} chars)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Todos os textos est√£o dentro do limite esperado\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Valida√ß√£o conclu√≠da - Dados prontos para treinamento!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o encontrado. Execute as c√©lulas anteriores primeiro.\")\n",
    "    print(\"‚ùå Necess√°rio: train_dataset deve estar definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste r√°pido de tokeniza√ß√£o antes do treinamento\n",
    "print(\"üß™ TESTE DE TOKENIZA√á√ÉO ANTES DO TREINAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'train_dataset' in locals() and train_dataset and 'tokenizer' in locals():\n",
    "    # Pega uma amostra pequena para teste\n",
    "    test_samples = train_dataset['text'][:5]\n",
    "    \n",
    "    print(f\"üîç Testando tokeniza√ß√£o com {len(test_samples)} amostras...\")\n",
    "    \n",
    "    all_good = True\n",
    "    \n",
    "    for i, text in enumerate(test_samples):\n",
    "        try:\n",
    "            # Testa tokeniza√ß√£o com truncation e padding\n",
    "            tokens = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=CONFIG['max_seq_length'],\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            input_ids = tokens['input_ids']\n",
    "            attention_mask = tokens['attention_mask']\n",
    "            \n",
    "            print(f\"   ‚úÖ Amostra {i+1}: {input_ids.shape[1]} tokens, shape: {input_ids.shape}\")\n",
    "            \n",
    "            # Verifica se tem padding\n",
    "            if tokenizer.pad_token_id in input_ids[0]:\n",
    "                pad_count = (input_ids[0] == tokenizer.pad_token_id).sum().item()\n",
    "                print(f\"      Padding: {pad_count} tokens\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro na amostra {i+1}: {e}\")\n",
    "            all_good = False\n",
    "            \n",
    "    if all_good:\n",
    "        print(f\"\\n‚úÖ TODOS OS TESTES PASSARAM!\")\n",
    "        print(f\"üéØ Tokeniza√ß√£o funcionando corretamente\")\n",
    "        print(f\"üöÄ Pronto para iniciar o treinamento!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå ALGUNS TESTES FALHARAM!\")\n",
    "        print(f\"‚ö†Ô∏è Verifique os erros acima antes de continuar\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Vari√°veis necess√°rias n√£o encontradas:\")\n",
    "    if 'train_dataset' not in locals():\n",
    "        print(\"   - train_dataset n√£o existe\")\n",
    "    if 'tokenizer' not in locals():\n",
    "        print(\"   - tokenizer n√£o existe\")\n",
    "    print(\"üîß Execute as c√©lulas anteriores primeiro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa√ß√£o do trainer (CORRIGIDO para erro de tokeniza√ß√£o)\n",
    "print(\"üèÉ‚Äç‚ôÇÔ∏è INICIALIZANDO TRAINER (COM CORRE√á√ïES)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# CORRE√á√ÉO: Configura padding token se n√£o existir\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(\"üîß Configurado pad_token = eos_token\")\n",
    "\n",
    "print(f\"üîç Verifica√ß√µes do tokenizer:\")\n",
    "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"   Pad token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token}\")\n",
    "print(f\"   EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Cria o trainer com configura√ß√µes corrigidas\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset if 'train_dataset' in locals() else None,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG['max_seq_length'],\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Desabilitado para evitar problemas de comprimento\n",
    "    args=training_args,\n",
    "    # CORRE√á√ÉO: Adiciona configura√ß√µes para lidar com sequ√™ncias de tamanhos diferentes\n",
    "    data_collator=None,  # Usa o padr√£o do SFTTrainer\n",
    "    formatting_func=None,  # Usa o dataset_text_field\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Trainer inicializado com sucesso! (Corrigido)\")\n",
    "print(f\"üìä Dataset de treinamento: {len(train_dataset) if 'train_dataset' in locals() else 0} exemplos\")\n",
    "print(f\"üîß Pad token configurado para evitar erros de tokeniza√ß√£o\")\n",
    "print(f\"üéØ Pronto para iniciar o fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621bcce7",
   "metadata": {},
   "source": [
    "### 5.3 Execu√ß√£o do Fine-tuning\n",
    "\n",
    "‚ö†Ô∏è **ATEN√á√ÉO**: O fine-tuning pode levar algum tempo para ser executado, especialmente dependendo do hardware dispon√≠vel. Monitore o progresso atrav√©s dos logs de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fe239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execu√ß√£o do fine-tuning\n",
    "import time\n",
    "\n",
    "print(\"üöÄ INICIANDO FINE-TUNING DO MODELO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚è∞ In√≠cio: {time.strftime('%H:%M:%S')}\")\n",
    "print(f\"üéØ Total de steps: {CONFIG['max_steps']}\")\n",
    "print(f\"üìä Dataset size: {len(train_dataset) if 'train_dataset' in locals() else 0}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Inicia o treinamento\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    print(f\"\\n‚úÖ FINE-TUNING CONCLU√çDO!\")\n",
    "    print(f\"‚è∞ Fim: {time.strftime('%H:%M:%S')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Mostra estat√≠sticas do treinamento\n",
    "    print(f\"üìà ESTAT√çSTICAS DO TREINAMENTO:\")\n",
    "    print(f\"   Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"   Training time: {trainer_stats.train_runtime:.2f} segundos\")\n",
    "    print(f\"   Samples per second: {trainer_stats.train_samples_per_second:.2f}\")\n",
    "    print(f\"   Steps per second: {trainer_stats.train_steps_per_second:.4f}\")\n",
    "    \n",
    "    training_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO DURANTE O TREINAMENTO:\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    print(f\"‚è∞ Hora do erro: {time.strftime('%H:%M:%S')}\")\n",
    "    training_success = False\n",
    "\n",
    "print(\"\\nüíæ Salvando logs de treinamento...\")\n",
    "if 'trainer_stats' in locals():\n",
    "    print(f\"‚úÖ Treinamento {'bem-sucedido' if training_success else 'com erros'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f4cb5",
   "metadata": {},
   "source": [
    "### 5.4 Salvamento do Modelo Fine-tuned\n",
    "\n",
    "Ap√≥s o treinamento, vamos salvar o modelo para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamento do modelo treinado\n",
    "print(\"üíæ SALVANDO MODELO FINE-TUNED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Salva o modelo no Google Drive\n",
    "    model_save_path = CONFIG['model_save_path']\n",
    "    \n",
    "    print(f\"üìÅ Salvando modelo em: {model_save_path}\")\n",
    "    \n",
    "    # Salva modelo e tokenizer\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo salvo com sucesso!\")\n",
    "    \n",
    "    # Verifica os arquivos salvos\n",
    "    import os\n",
    "    saved_files = os.listdir(model_save_path)\n",
    "    print(f\"üìã Arquivos salvos: {saved_files}\")\n",
    "    \n",
    "    # Salva tamb√©m apenas os adaptadores LoRA (mais leve)\n",
    "    lora_save_path = f\"{model_save_path}/lora_adapters\"\n",
    "    os.makedirs(lora_save_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíø Salvando adaptadores LoRA em: {lora_save_path}\")\n",
    "    model.save_pretrained(lora_save_path, save_adapter=True, save_config=True)\n",
    "    \n",
    "    print(f\"‚úÖ Adaptadores LoRA salvos!\")\n",
    "    print(f\"üí° Use os adaptadores LoRA para carregar o modelo mais rapidamente\")\n",
    "    \n",
    "    # Informa√ß√µes sobre o salvamento\n",
    "    print(f\"\\nüìä INFORMA√á√ïES DO MODELO SALVO:\")\n",
    "    print(f\"   Modelo completo: {model_save_path}\")\n",
    "    print(f\"   Adaptadores LoRA: {lora_save_path}\")\n",
    "    print(f\"   Configura√ß√£o base: {CONFIG['model_name']}\")\n",
    "    print(f\"   LoRA r: {CONFIG['lora_r']}, alpha: {CONFIG['lora_alpha']}\")\n",
    "    \n",
    "    model_saved = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar modelo: {e}\")\n",
    "    model_saved = False\n",
    "\n",
    "print(f\"\\n{'‚úÖ Modelo salvo com sucesso!' if model_saved else '‚ùå Falha ao salvar modelo'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff278c12",
   "metadata": {},
   "source": [
    "## 6. Teste do Modelo Treinado\n",
    "\n",
    "### 6.1 Compara√ß√£o: Modelo Base vs Modelo Fine-tuned\n",
    "\n",
    "Agora vamos testar o modelo ap√≥s o fine-tuning e comparar com as respostas do modelo base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cccf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o modelo fine-tuned e compara com dados reais\n",
    "print(\"üß™ TESTANDO MODELO FINE-TUNED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configura o modelo para infer√™ncia\n",
    "FastLanguageModel.for_inference(model)  # Habilita modo de infer√™ncia\n",
    "\n",
    "if 'test_samples' in locals() and test_samples:\n",
    "    # Testa com as mesmas amostras usadas no teste do modelo base\n",
    "    num_tests = min(5, len(test_samples))\n",
    "    \n",
    "    print(f\"üî¨ Testando com {num_tests} amostras\")\n",
    "    print(\"üÜö Compara√ß√£o: Base vs Fine-tuned vs Real\")\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üß™ TESTE {i+1}/{num_tests}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        sample = test_samples[i]\n",
    "        title = sample['title']\n",
    "        real_content = sample['content']\n",
    "        \n",
    "        print(f\"üè∑Ô∏è  T√çTULO: {title}\")\n",
    "        print(f\"\\nüìÑ DESCRI√á√ÉO REAL:\")\n",
    "        print(f\"   {real_content}\")\n",
    "        \n",
    "        # Gera com modelo fine-tuned\n",
    "        print(f\"\\nü§ñ DESCRI√á√ÉO FINE-TUNED:\")\n",
    "        try:\n",
    "            finetuned_content = test_model_generation(\n",
    "                model, tokenizer, title,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            print(f\"   {finetuned_content}\")\n",
    "            \n",
    "            # An√°lise de qualidade\n",
    "            real_length = len(real_content)\n",
    "            ft_length = len(finetuned_content)\n",
    "            \n",
    "            # Verifica relev√¢ncia (palavras em comum com t√≠tulo)\n",
    "            title_words = set(title.lower().split())\n",
    "            real_words = set(real_content.lower().split())\n",
    "            ft_words = set(finetuned_content.lower().split())\n",
    "            \n",
    "            real_relevance = len(title_words.intersection(real_words)) / len(title_words) if title_words else 0\n",
    "            ft_relevance = len(title_words.intersection(ft_words)) / len(title_words) if title_words else 0\n",
    "            \n",
    "            print(f\"\\nüìä AN√ÅLISE COMPARATIVA:\")\n",
    "            print(f\"   Real: {real_length} chars, Relev√¢ncia: {real_relevance:.2f}\")\n",
    "            print(f\"   Fine-tuned: {ft_length} chars, Relev√¢ncia: {ft_relevance:.2f}\")\n",
    "            \n",
    "            # Qualidade relativa\n",
    "            length_ratio = ft_length / real_length if real_length > 0 else 0\n",
    "            relevance_improvement = ft_relevance - real_relevance\n",
    "            \n",
    "            print(f\"   Raz√£o de comprimento: {length_ratio:.2f}\")\n",
    "            print(f\"   Melhoria na relev√¢ncia: {relevance_improvement:+.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro na gera√ß√£o: {e}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Teste comparativo conclu√≠do!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Amostras de teste n√£o dispon√≠veis.\")\n",
    "    print(\"Execute as se√ß√µes anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51964cf3",
   "metadata": {},
   "source": [
    "### 6.2 Teste com Novos Produtos\n",
    "\n",
    "Vamos testar o modelo com alguns t√≠tulos de produtos que n√£o estavam no dataset de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b72f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com novos produtos (n√£o vistos durante o treinamento)\n",
    "print(\"üÜï TESTANDO COM PRODUTOS NOVOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# T√≠tulos de produtos para teste\n",
    "test_titles = [\n",
    "    \"Smartphone Samsung Galaxy S24 Ultra 256GB 5G\",\n",
    "    \"Cafeteira El√©trica Autom√°tica com Timer Program√°vel\",\n",
    "    \"T√™nis Nike Air Max 270 Masculino Running\",\n",
    "    \"Livro 'Intelig√™ncia Artificial - Uma Abordagem Moderna'\",\n",
    "    \"Fone de Ouvido Bluetooth Wireless com Cancelamento de Ru√≠do\",\n",
    "    \"Notebook Dell Inspiron 15 Intel Core i7 16GB RAM 512GB SSD\",\n",
    "    \"Panela de Press√£o El√©trica 6 Litros Inox\",\n",
    "    \"Rel√≥gio Smartwatch Apple Watch Series 9 GPS\"\n",
    "]\n",
    "\n",
    "print(f\"üß™ Testando {len(test_titles)} novos produtos\")\n",
    "\n",
    "for i, title in enumerate(test_titles, 1):\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"üîç TESTE {i}/{len(test_titles)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"üè∑Ô∏è  T√çTULO: {title}\")\n",
    "    \n",
    "    try:\n",
    "        # Gera descri√ß√£o com o modelo fine-tuned\n",
    "        generated_description = test_model_generation(\n",
    "            model, tokenizer, title,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìù DESCRI√á√ÉO GERADA:\")\n",
    "        print(f\"   {generated_description}\")\n",
    "        \n",
    "        # An√°lise b√°sica\n",
    "        description_length = len(generated_description)\n",
    "        title_words = set(title.lower().split())\n",
    "        desc_words = set(generated_description.lower().split())\n",
    "        relevance = len(title_words.intersection(desc_words)) / len(title_words) if title_words else 0\n",
    "        \n",
    "        print(f\"\\nüìä M√âTRICAS:\")\n",
    "        print(f\"   Comprimento: {description_length} caracteres\")\n",
    "        print(f\"   Palavras em comum: {len(title_words.intersection(desc_words))}/{len(title_words)}\")\n",
    "        print(f\"   Score de relev√¢ncia: {relevance:.2f}\")\n",
    "        \n",
    "        # An√°lise qualitativa simples\n",
    "        quality_indicators = []\n",
    "        if description_length > 50:\n",
    "            quality_indicators.append(\"‚úÖ Descri√ß√£o substancial\")\n",
    "        if relevance > 0.3:\n",
    "            quality_indicators.append(\"‚úÖ Boa relev√¢ncia\")\n",
    "        if any(word in generated_description.lower() for word in ['produto', 'qualidade', 'caracter√≠sticas', 'design', 'funcionalidade']):\n",
    "            quality_indicators.append(\"‚úÖ Linguagem comercial\")\n",
    "        \n",
    "        if quality_indicators:\n",
    "            print(f\"   Qualidade: {', '.join(quality_indicators)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro na gera√ß√£o: {e}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\n‚úÖ Teste com novos produtos conclu√≠do!\")\n",
    "print(f\"üí° Observe como o modelo generaliza para produtos n√£o vistos no treinamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b00159",
   "metadata": {},
   "source": [
    "## 7. Demonstra√ß√£o Interativa\n",
    "\n",
    "### 7.1 Interface Simples para Teste\n",
    "\n",
    "Criamos uma fun√ß√£o interativa onde voc√™ pode testar o modelo com qualquer t√≠tulo de produto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef161b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_description(title, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para gerar descri√ß√£o de produto de forma interativa\n",
    "    \n",
    "    Args:\n",
    "        title: T√≠tulo do produto\n",
    "        temperature: Controla criatividade (0.1-1.0)\n",
    "        max_tokens: M√°ximo de tokens a gerar\n",
    "    \n",
    "    Returns:\n",
    "        Descri√ß√£o gerada\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description = test_model_generation(\n",
    "            model, tokenizer, title,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        return f\"Erro na gera√ß√£o: {e}\"\n",
    "\n",
    "# Demonstra√ß√£o interativa\n",
    "def interactive_demo():\n",
    "    \"\"\"Fun√ß√£o de demonstra√ß√£o interativa\"\"\"\n",
    "    \n",
    "    print(\"üéØ DEMONSTRA√á√ÉO INTERATIVA DO MODELO FINE-TUNED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí° Digite t√≠tulos de produtos para gerar descri√ß√µes!\")\n",
    "    print(\"üí° Digite 'sair' para encerrar\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\nüè∑Ô∏è  Digite o t√≠tulo do produto:\")\n",
    "        user_input = input(\"T√≠tulo: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['sair', 'exit', 'quit', '']:\n",
    "            print(\"üëã Encerrando demonstra√ß√£o...\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nü§ñ Gerando descri√ß√£o para: '{user_input}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Gera a descri√ß√£o\n",
    "        description = generate_product_description(user_input)\n",
    "        \n",
    "        print(f\"üìù DESCRI√á√ÉO GERADA:\")\n",
    "        print(f\"   {description}\")\n",
    "        \n",
    "        # An√°lise r√°pida\n",
    "        desc_length = len(description)\n",
    "        title_words = set(user_input.lower().split())\n",
    "        desc_words = set(description.lower().split())\n",
    "        relevance = len(title_words.intersection(desc_words)) / len(title_words) if title_words else 0\n",
    "        \n",
    "        print(f\"\\nüìä M√âTRICAS:\")\n",
    "        print(f\"   Comprimento: {desc_length} caracteres\")\n",
    "        print(f\"   Relev√¢ncia: {relevance:.2f}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de demonstra√ß√£o interativa criada!\")\n",
    "print(\"üí° Execute interactive_demo() para come√ßar a testar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute esta c√©lula para iniciar a demonstra√ß√£o interativa\n",
    "print(\"üéÆ INICIANDO DEMONSTRA√á√ÉO INTERATIVA\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üí° Teste o modelo com t√≠tulos de produtos personalizados!\")\n",
    "print(\"üí° Exemplos de t√≠tulos para testar:\")\n",
    "print(\"   - 'Mouse Gamer RGB com DPI Ajust√°vel'\")\n",
    "print(\"   - 'Cadeira Ergon√¥mica para Escrit√≥rio'\")\n",
    "print(\"   - 'Mochila Imperme√°vel para Notebook'\")\n",
    "print(\"   - Ou qualquer outro t√≠tulo de produto!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Descomente a linha abaixo para executar a demonstra√ß√£o interativa\n",
    "# interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1087f9",
   "metadata": {},
   "source": [
    "### 7.2 Resumo Final e Conclus√µes\n",
    "\n",
    "Parab√©ns! Voc√™ completou o processo de fine-tuning do modelo Llama 3-8B para gera√ß√£o de descri√ß√µes de produtos Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final do projeto\n",
    "print(\"üéâ RESUMO FINAL DO TECH CHALLENGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üéØ OBJETIVO ALCAN√áADO:\")\n",
    "print(\"   ‚úÖ Fine-tuning do Llama 3-8B para descri√ß√µes de produtos Amazon\")\n",
    "print(\"   ‚úÖ Dataset AmazonTitles-1.3MM processado e limpo\")\n",
    "print(\"   ‚úÖ Modelo treinado com t√©cnica LoRA\")\n",
    "print(\"   ‚úÖ Testes e compara√ß√µes realizados\")\n",
    "\n",
    "print(f\"\\nüìä ESTAT√çSTICAS DO PROJETO:\")\n",
    "if 'raw_data' in locals():\n",
    "    print(f\"   üìÅ Dados originais: {len(raw_data):,} amostras\")\n",
    "if 'cleaned_data' in locals():\n",
    "    print(f\"   üßπ Dados limpos: {len(cleaned_data):,} amostras\")\n",
    "if 'train_dataset' in locals():\n",
    "    print(f\"   üìö Dataset de treinamento: {len(train_dataset):,} exemplos\")\n",
    "if 'CONFIG' in locals():\n",
    "    print(f\"   üîß Steps de treinamento: {CONFIG['max_steps']}\")\n",
    "    print(f\"   ‚öôÔ∏è LoRA r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è TECNOLOGIAS UTILIZADAS:\")\n",
    "print(\"   ü§ñ Modelo: Llama 3-8B (Unsloth 4-bit)\")\n",
    "print(\"   üìñ Biblioteca: Transformers, TRL, Unsloth\")\n",
    "print(\"   üîß T√©cnica: LoRA (Low-Rank Adaptation)\")\n",
    "print(\"   üìä Dataset: Amazon Products (trn.json.gz)\")\n",
    "print(\"   ‚òÅÔ∏è Ambiente: Google Colab\")\n",
    "\n",
    "print(f\"\\nüíæ ARQUIVOS SALVOS:\")\n",
    "if 'CONFIG' in locals():\n",
    "    print(f\"   üìÅ Modelo completo: {CONFIG['model_save_path']}\")\n",
    "    print(f\"   üíø Adaptadores LoRA: {CONFIG['model_save_path']}/lora_adapters\")\n",
    "    print(f\"   üìä Logs de treinamento: {CONFIG['output_dir']}\")\n",
    "\n",
    "print(f\"\\nüöÄ PR√ìXIMOS PASSOS SUGERIDOS:\")\n",
    "print(\"   1. Ajustar hiperpar√¢metros para melhor performance\")\n",
    "print(\"   2. Testar com datasets maiores\")\n",
    "print(\"   3. Implementar m√©tricas de avalia√ß√£o mais sofisticadas\")\n",
    "print(\"   4. Experimentar com diferentes t√©cnicas de prompt\")\n",
    "print(\"   5. Integrar o modelo em uma aplica√ß√£o web\")\n",
    "\n",
    "print(f\"\\nüìö APRENDIZADOS:\")\n",
    "print(\"   ‚úÖ Fine-tuning eficiente com LoRA\")\n",
    "print(\"   ‚úÖ Processamento de datasets grandes\")\n",
    "print(\"   ‚úÖ Formata√ß√£o de prompts para Llama 3\")\n",
    "print(\"   ‚úÖ T√©cnicas de limpeza e pr√©-processamento\")\n",
    "print(\"   ‚úÖ Avalia√ß√£o de modelos de linguagem\")\n",
    "\n",
    "print(f\"\\nüéâ PARAB√âNS! TECH CHALLENGE CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
