{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Parâmetros globais (edite aqui)\n",
    "import os\n",
    "\n",
    "# Caminhos\n",
    "DATA_PATH = os.getenv('DATA_PATH', '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz')\n",
    "ART_DIR = os.getenv('ART_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/artifacts')\n",
    "OUTPUT_DIR = os.getenv('OSS_OUTPUT_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/tinyllama_amazon_finetuned')\n",
    "MERGED_OUTPUT_DIR = os.getenv('OSS_MERGED_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/tinyllama_amazon_final')\n",
    "TRAIN_SAVE_JSONL = os.getenv('TRAIN_SAVE_JSONL', os.path.join(ART_DIR, 'train_dataset_used.jsonl'))\n",
    "TRAIN_SAVE_GZ = os.getenv('TRAIN_SAVE_GZ', os.path.join(ART_DIR, 'train_dataset_used.jsonl.gz'))\n",
    "\n",
    "# Semente\n",
    "SEED = int(os.getenv('SEED', 42))\n",
    "\n",
    "# Leitura/Limpeza\n",
    "MAX_RECORDS = int(os.getenv('MAX_RECORDS_FOR_FT', 200_000))\n",
    "TITLE_MIN_LEN = int(os.getenv('TITLE_MIN_LEN', 2))\n",
    "CONTENT_MIN_LEN = int(os.getenv('CONTENT_MIN_LEN', 5))\n",
    "\n",
    "# Dataset/Splits\n",
    "TEST_SIZE = float(os.getenv('TEST_SIZE', 0.2))\n",
    "VAL_SIZE = float(os.getenv('VAL_SIZE', 0.5))\n",
    "\n",
    "# Modelo e geração\n",
    "BASE_OSS_MODEL = os.getenv('BASE_OSS_MODEL', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "OSS_MAX_SEQ_LEN = int(os.getenv('OSS_MAX_SEQ_LEN', 1024))\n",
    "MAX_SEQ_LEN = OSS_MAX_SEQ_LEN  # alias para compatibilidade\n",
    "OSS_SUBSET = int(os.getenv('OSS_SUBSET', 4000))\n",
    "OSS_EVAL_N = int(os.getenv('OSS_EVAL_N', 200))\n",
    "OSS_TEST_N = int(os.getenv('OSS_TEST_N', 200))\n",
    "GEN_MAX_NEW_TOKENS = int(os.getenv('GEN_MAX_NEW_TOKENS', 96))\n",
    "GEN_BATCH_SIZE = int(os.getenv('GEN_BATCH_SIZE', 8))\n",
    "GEN_DO_SAMPLE = bool(int(os.getenv('GEN_DO_SAMPLE', 1)))\n",
    "\n",
    "# Treino\n",
    "OSS_LR = float(os.getenv('OSS_LR', 2e-4))\n",
    "OSS_BATCH_SIZE = int(os.getenv('OSS_BATCH_SIZE', 4))\n",
    "OSS_GRAD_ACCUM = int(os.getenv('OSS_GRAD_ACCUM', 2))\n",
    "OSS_EPOCHS = int(os.getenv('OSS_EPOCHS', 2))\n",
    "OSS_WARMUP_STEPS = int(os.getenv('OSS_WARMUP_STEPS', 50))\n",
    "\n",
    "# LoRA\n",
    "LORA_R = int(os.getenv('LORA_R', 16))\n",
    "LORA_ALPHA = int(os.getenv('LORA_ALPHA', 16))\n",
    "LORA_DROPOUT = float(os.getenv('LORA_DROPOUT', 0.05))\n",
    "\n",
    "print('Parâmetros carregados:')\n",
    "print({k:v for k,v in globals().items() if k.isupper()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67223cff",
   "metadata": {},
   "source": [
    "# Tech Challenge (Colab GPU): Fine-tuning OSS com Unsloth + TinyLlama\n",
    "\n",
    "Pipeline completo para rodar no Google Colab com GPU (sem custo de API):\n",
    "\n",
    "1) Montar Google Drive e instalar dependências\n",
    "2) Carregar e limpar dados (title → content) a partir de `/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz`\n",
    "3) Preparar dataset para SFT (prompts + splits)\n",
    "4) Avaliação baseline (modelo base, sem FT)\n",
    "5) Treinamento LoRA com Unsloth + TinyLlama\n",
    "6) Avaliação pós-FT e gráficos\n",
    "7) Salvar artefatos (adapter, métricas, amostras) no Drive\n",
    "\n",
    "Observações:\n",
    "- Use GPU (Runtime > Change runtime type > T4/L4/A100).\n",
    "- bitsandbytes (4-bit) ajuda caber na VRAM.\n",
    "- Ajuste OSS_SUBSET/OSS_EVAL_N/OSS_EPOCHS conforme tempo e GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Montar Drive e instalar dependências (Colab)\n",
    "import sys, subprocess, os\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "PKGS = [\n",
    "    'unsloth>=2024.8.12',\n",
    "    'transformers>=4.43',\n",
    "    'accelerate>=0.33',\n",
    "    'datasets>=2.20',\n",
    "    'peft>=0.12.0',\n",
    "    'trl>=0.9.6',\n",
    "    'sacrebleu',\n",
    "    'rouge-score',\n",
    "    'matplotlib',\n",
    "    'tiktoken',\n",
    "    'bert-score>=0.3.13',\n",
    "    'nltk>=3.8'\n",
    "]\n",
    "\n",
    "print('Instalando dependências...')\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', *PKGS])\n",
    "\n",
    "# Garantir bitsandbytes atualizado para 4-bit\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'bitsandbytes>=0.43.1'])\n",
    "    import importlib, bitsandbytes as bnb  # type: ignore\n",
    "    importlib.reload(bnb)\n",
    "    print('bitsandbytes version:', getattr(bnb, '__version__', 'unknown'))\n",
    "except Exception as e:\n",
    "    print('Aviso: não foi possível atualizar bitsandbytes automaticamente:', e)\n",
    "\n",
    "print('Setup concluído.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c53292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Carregar e limpar dados (title → content)\n",
    "import gzip, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f'Arquivo não encontrado: {DATA_PATH}'\n",
    "\n",
    "rows, count = [], 0\n",
    "with gzip.open(DATA_PATH, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        rows.append({'title': obj.get('title'), 'content': obj.get('content')})\n",
    "        count += 1\n",
    "        if count >= MAX_RECORDS:\n",
    "            break\n",
    "\n",
    "df_raw = pd.DataFrame(rows, columns=['title','content']).dropna(how='all')\n",
    "print('Lidas linhas:', len(df_raw))\n",
    "\n",
    "EMPTY_STRINGS = {'none','nan','null','na','n/a'}\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, float) and np.isnan(s):\n",
    "        return None\n",
    "    if isinstance(s, str) and s.strip().lower() in EMPTY_STRINGS:\n",
    "        return None\n",
    "    s = str(s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    s = re.sub(r'[`*_#>\\\"]', ' ', s)\n",
    "    s = s.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    s = re.sub(r'\\s+', ' ', s).strip(\" '\\\"\")\n",
    "    if not s or s.strip().lower() in EMPTY_STRINGS:\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "df = df_raw.copy()\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['title','content']).drop_duplicates(subset=['title','content'])\n",
    "# Filtros mínimos parametrizados\n",
    "before_len = len(df)\n",
    "df = df[df['title'].str.len() >= TITLE_MIN_LEN]\n",
    "df = df[df['content'].str.len() >= CONTENT_MIN_LEN]\n",
    "print(f'Após limpeza: {len(df)} (removidos {before-len(df)}; por tamanho {before_len-len(df)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Preparar dataset para SFT (prompts + splits)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "SYSTEM_PROMPT = os.getenv('OSS_SYSTEM_PROMPT', 'Você é um assistente que gera descrições detalhadas de produtos a partir do título.')\n",
    "PROMPT_COL = 'prompt'\n",
    "TARGET_COL = 'target'\n",
    "\n",
    "df_prep = df[['title','content']].copy()\n",
    "df_prep[PROMPT_COL] = df_prep['title'].apply(lambda t: f\"[SYSTEM]\\n{SYSTEM_PROMPT}\\n[USER]\\nTítulo: {t}\\n[ASSISTANT]\\n\")\n",
    "df_prep[TARGET_COL] = df_prep['content']\n",
    "\n",
    "test_size = float(os.getenv('TEST_SIZE', 0.2))\n",
    "val_size = float(os.getenv('VAL_SIZE', 0.5))\n",
    "\n",
    "train_df, temp_df = train_test_split(df_prep[[PROMPT_COL,TARGET_COL]], test_size=test_size, random_state=SEED, shuffle=True)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=SEED, shuffle=True)\n",
    "\n",
    "print(f\"Splits → train={len(train_df):,}, val={len(val_df):,}, test={len(test_df):,}\")\n",
    "\n",
    "datasets_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    'validation': Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    'test': Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfeca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Carregar modelo base (Unsloth + TinyLlama) e avaliação baseline (val e teste) — com geração em lote\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "MAX_SEQ_LEN = OSS_MAX_SEQ_LEN\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "print('Dispositivo:', device)\n",
    "\n",
    "print('Carregando modelo base...')\n",
    "load_in_4bit = True if device == 'cuda' else False\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = BASE_OSS_MODEL,\n",
    "        max_seq_length = MAX_SEQ_LEN,\n",
    "        dtype = None,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print('Falha ao carregar em 4-bit. Tentando sem 4-bit. Motivo:', e)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = BASE_OSS_MODEL,\n",
    "        max_seq_length = MAX_SEQ_LEN,\n",
    "        dtype = None,\n",
    "        load_in_4bit = False,\n",
    "    )\n",
    "\n",
    "# Subconjuntos para acelerar\n",
    "if datasets_dict['train'].num_rows > OSS_SUBSET:\n",
    "    datasets_dict['train'] = datasets_dict['train'].shuffle(seed=SEED).select(range(OSS_SUBSET))\n",
    "subset_val = datasets_dict['validation'].select(range(min(OSS_EVAL_N, datasets_dict['validation'].num_rows)))\n",
    "subset_test = datasets_dict['test'].select(range(min(OSS_TEST_N, datasets_dict['test'].num_rows)))\n",
    "\n",
    "model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Geração batelada e remoção de prompt precisa via input_length\n",
    "@torch.no_grad()\n",
    "def generate_batch(prompts, max_new_tokens=GEN_MAX_NEW_TOKENS, do_sample=False, temperature=0.0, top_p=0.9):\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=MAX_SEQ_LEN).to(model.device)\n",
    "    input_lengths = (inputs['input_ids'] != tokenizer.pad_token_id).sum(dim=1)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # Remover exatamente os tokens do prompt por amostra\n",
    "    result = []\n",
    "    for i, txt in enumerate(texts):\n",
    "        prompt_text = tokenizer.decode(outputs[i][:input_lengths[i]], skip_special_tokens=True)\n",
    "        result.append(txt[len(prompt_text):].strip())\n",
    "    return result\n",
    "\n",
    "# Helper para avaliação batelada (força geração determinística para métricas)\n",
    "def eval_dataset(ds, batch_size=GEN_BATCH_SIZE):\n",
    "    refs, hyps = [], []\n",
    "    n = ds.num_rows\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = ds.select(range(i, min(i+batch_size, n)))\n",
    "        prompts = [ex['prompt'] for ex in batch]\n",
    "        hyps.extend(generate_batch(prompts, do_sample=False, temperature=0.0))\n",
    "        refs.extend([ex['target'].strip() for ex in batch])\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "    rougeL = float(np.mean([scorer.score(r, h)['rougeLsum'].fmeasure for r, h in zip(refs, hyps)]))\n",
    "    return bleu, rougeL, refs, hyps\n",
    "\n",
    "# Baseline (val e test)\n",
    "bleu_base_val, rougeL_base_val, refs_val, hyps_base_val = eval_dataset(subset_val)\n",
    "print(f'Baseline (val) → BLEU={bleu_base_val:.2f} | ROUGE-L={rougeL_base_val:.3f}')\n",
    "bleu_base_test, rougeL_base_test, refs_test_base, hyps_base_test = eval_dataset(subset_test)\n",
    "print(f'Baseline (test) → BLEU={bleu_base_test:.2f} | ROUGE-L={rougeL_base_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Treinamento LoRA (Unsloth + TRL SFTTrainer)\n",
    "from transformers import TrainingArguments, __version__ as HF_VER\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import inspect\n",
    "\n",
    "LORA_R = int(os.getenv('LORA_R', 16))\n",
    "LORA_ALPHA = int(os.getenv('LORA_ALPHA', 16))\n",
    "LORA_DROPOUT = float(os.getenv('LORA_DROPOUT', 0.05))\n",
    "LR = float(os.getenv('OSS_LR', 2e-4))\n",
    "BATCH_SIZE = int(os.getenv('OSS_BATCH_SIZE', 4))\n",
    "GR_ACCUM = int(os.getenv('OSS_GRAD_ACCUM', 2))\n",
    "EPOCHS = int(os.getenv('OSS_EPOCHS', 2))\n",
    "WARMUP = int(os.getenv('OSS_WARMUP_STEPS', 50))\n",
    "OUTPUT_DIR = os.getenv('OSS_OUTPUT_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/tinyllama_amazon_finetuned')\n",
    "\n",
    "# Aplicar LoRA (com Unsloth)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Preparar dataset (texto completo) e tokenização on-the-fly\n",
    "\n",
    "def format_row(ex):\n",
    "    return {'text': ex['prompt'] + ex['target']}\n",
    "\n",
    "train_ds = datasets_dict['train'].map(format_row, remove_columns=datasets_dict['train'].column_names)\n",
    "val_ds = datasets_dict['validation'].map(format_row, remove_columns=datasets_dict['validation'].column_names)\n",
    "\n",
    "# Opcional: mascarar a perda no prompt e treinar apenas na resposta do [ASSISTANT]\n",
    "MASK_PROMPT_LOSS = True\n",
    "if MASK_PROMPT_LOSS:\n",
    "    response_template = \"[ASSISTANT]\\n\"\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        response_template=response_template,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    use_packing = False  # com collator de completion-only, prefira packing=False\n",
    "    train_data = train_ds\n",
    "    eval_data = val_ds\n",
    "else:\n",
    "    data_collator = None\n",
    "    use_packing = True\n",
    "    # Se não for mascarar, o SFTTrainer pode tokenizar internamente a partir de 'text'\n",
    "    train_data = train_ds\n",
    "    eval_data = val_ds\n",
    "\n",
    "# Compatibilidade Transformers v4/v5 via inspeção da assinatura\n",
    "args_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GR_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_steps=WARMUP,\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "sig_params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
    "if 'eval_strategy' in sig_params:\n",
    "    args_kwargs['eval_strategy'] = 'steps'\n",
    "elif 'evaluation_strategy' in sig_params:\n",
    "    args_kwargs['evaluation_strategy'] = 'steps'\n",
    "else:\n",
    "    print('Aviso: parâmetro de avaliação por estratégia não encontrado; usaremos eval_steps para controlar a frequência.')\n",
    "\n",
    "if 'save_strategy' in sig_params:\n",
    "    args_kwargs['save_strategy'] = 'steps'\n",
    "\n",
    "args = TrainingArguments(**args_kwargs)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    args=args,\n",
    "    packing=use_packing,\n",
    "    dataset_text_field='text',\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Treinando...')\n",
    "trainer.train()\n",
    "print('Treino concluído.')\n",
    "\n",
    "# Salvar adapter\n",
    "import os, sys, traceback\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print('Adapter salvo em:', OUTPUT_DIR)\n",
    "\n",
    "# Modo recomendado: recarregar base em FP16 e mesclar adapter → salva modelo completo\n",
    "merged_saved = False\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print('Mesclando recarregando base em FP16 (sem 4-bit) e aplicando adapter...')\n",
    "    base_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_OSS_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    merged_model = PeftModel.from_pretrained(base_fp16, OUTPUT_DIR)\n",
    "    merged_model = merged_model.merge_and_unload()\n",
    "    os.makedirs(MERGED_OUTPUT_DIR, exist_ok=True)\n",
    "    merged_model.save_pretrained(MERGED_OUTPUT_DIR, safe_serialization=True)\n",
    "    try:\n",
    "        tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "    except Exception:\n",
    "        AutoTokenizer.from_pretrained(BASE_OSS_MODEL).save_pretrained(MERGED_OUTPUT_DIR)\n",
    "    print('Modelo mesclado salvo em:', MERGED_OUTPUT_DIR)\n",
    "    merged_saved = True\n",
    "except Exception as e:\n",
    "    print('Falha no merge via recarga FP16:', e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Fallback: tentar merge direto no modelo atual (sem mover para CPU)\n",
    "if not merged_saved:\n",
    "    try:\n",
    "        if hasattr(model, 'merge_and_unload'):\n",
    "            print('Tentando merge no modelo atual (PEFT)...')\n",
    "            merged2 = model.merge_and_unload()\n",
    "            os.makedirs(MERGED_OUTPUT_DIR, exist_ok=True)\n",
    "            merged2.save_pretrained(MERGED_OUTPUT_DIR, safe_serialization=True)\n",
    "            tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "            print('Modelo mesclado salvo (fallback PEFT direto) em:', MERGED_OUTPUT_DIR)\n",
    "            merged_saved = True\n",
    "    except Exception as e2:\n",
    "        print('Falha PEFT merge direto:', e2)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Fallback final: salvar em diretório local do projeto\n",
    "if not merged_saved:\n",
    "    try:\n",
    "        local_dir = os.path.abspath('./tinyllama_amazon_final')\n",
    "        print('Salvando fallback local em:', local_dir)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        if 'merged_model' in locals():\n",
    "            to_save = merged_model\n",
    "        elif 'merged2' in locals():\n",
    "            to_save = merged2\n",
    "        else:\n",
    "            to_save = model\n",
    "        to_save.save_pretrained(local_dir, safe_serialization=True)\n",
    "        tokenizer.save_pretrained(local_dir)\n",
    "        print('Modelo salvo (fallback local) em:', local_dir)\n",
    "        merged_saved = True\n",
    "    except Exception as e3:\n",
    "        print('Falha ao salvar fallback local:', e3)\n",
    "        traceback.print_exc()\n",
    "\n",
    "if not merged_saved:\n",
    "    print('Aviso: não foi possível mesclar e salvar o modelo completo. Use o adapter em OUTPUT_DIR.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6299ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1) Salvar dataset usado no treino (title/content) no formato NDJSON (+GZ)\n",
    "import re, json, gzip, os\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "def extract_title(prompt: str) -> str:\n",
    "    # Extrai o conteúdo após 'Título:' até '[ASSISTANT]'\n",
    "    try:\n",
    "        start = prompt.index('Título:') + len('Título:')\n",
    "        end = prompt.index('[ASSISTANT]', start)\n",
    "        return prompt[start:end].strip()\n",
    "    except ValueError:\n",
    "        # fallback com regex mais tolerante\n",
    "        m = re.search(r'Título:\\s*(.*?)\\s*\\[ASSISTANT\\]', prompt, flags=re.S)\n",
    "        return m.group(1).strip() if m else ''\n",
    "\n",
    "train_examples = []\n",
    "for ex in datasets_dict['train']:\n",
    "    t = extract_title(ex['prompt'])\n",
    "    c = ex['target'] if isinstance(ex['target'], str) else str(ex['target'])\n",
    "    if t and c:\n",
    "        train_examples.append({'title': t, 'content': c})\n",
    "\n",
    "# Salvar JSONL e JSONL.GZ\n",
    "with open(TRAIN_SAVE_JSONL, 'w', encoding='utf-8') as f:\n",
    "    for obj in train_examples:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n",
    "with gzip.open(TRAIN_SAVE_GZ, 'wt', encoding='utf-8') as f:\n",
    "    for obj in train_examples:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n",
    "print(f'Dataset de treino salvo em:\\n- {TRAIN_SAVE_JSONL}\\n- {TRAIN_SAVE_GZ}\\nTotal exemplos: {len(train_examples)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8182e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Avaliação pós-FT e gráficos + salvamento de métricas (comparando com baseline de teste) — batelada\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reutiliza subset_test e generate_batch/GEN_BATCH_SIZE definidos no passo 4\n",
    "model.eval()\n",
    "\n",
    "def eval_dataset_after_ft(ds, batch_size=GEN_BATCH_SIZE):\n",
    "    refs, hyps = [], []\n",
    "    n = ds.num_rows\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = ds.select(range(i, min(i+batch_size, n)))\n",
    "        prompts = [ex['prompt'] for ex in batch]\n",
    "        hyps.extend(generate_batch(prompts))\n",
    "        refs.extend([ex['target'].strip() for ex in batch])\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "    rougeL = float(np.mean([scorer.score(r, h)['rougeLsum'].fmeasure for r, h in zip(refs, hyps)]))\n",
    "    return bleu, rougeL, refs, hyps\n",
    "\n",
    "bleu_ft, rougeL_ft, refs_test, hyps_ft = eval_dataset_after_ft(subset_test)\n",
    "\n",
    "print('\\nResumo comparativo (TEST):')\n",
    "print(f\"Baseline → BLEU={bleu_base_test:.2f} | ROUGE-L={rougeL_base_test:.3f}\")\n",
    "print(f\"Fine-tuned → BLEU={bleu_ft:.2f} | ROUGE-L={rougeL_ft:.3f}\")\n",
    "\n",
    "labels = ['BLEU','ROUGE-L']\n",
    "baseline_vals = [bleu_base_test, rougeL_base_test]\n",
    "ft_vals = [bleu_ft, rougeL_ft]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, baseline_vals, width, label='Baseline (test)')\n",
    "plt.bar(x + width/2, ft_vals, width, label='Fine-tuned (test)')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparação de métricas no conjunto de teste')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Salvar métricas e amostras no Drive\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(ART_DIR, 'oss_metrics.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'bleu_base_val': float(bleu_base_val),\n",
    "        'rougeL_base_val': float(rougeL_base_val),\n",
    "        'bleu_base_test': float(bleu_base_test),\n",
    "        'rougeL_base_test': float(rougeL_base_test),\n",
    "        'bleu_ft_test': float(bleu_ft),\n",
    "        'rougeL_ft_test': float(rougeL_ft)\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "samples_path = os.path.join(ART_DIR, 'qualitative_samples_test.jsonl')\n",
    "with open(samples_path, 'w', encoding='utf-8') as f:\n",
    "    for ex, r_base, r_ft in zip(subset_test, hyps_base_test, hyps_ft):\n",
    "        f.write(json.dumps({'prompt': ex['prompt'], 'hyp_base': r_base, 'hyp_ft': r_ft, 'ref': ex['target']}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Artefatos salvos em:', ART_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Testes adicionais de qualidade e robustez (métricas extras + análise pareada)\n",
    "from statistics import mean\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os, json\n",
    "\n",
    "# Usaremos refs_test (gold), hyps_base_test (baseline) e hyps_ft (fine-tuned) produzidos nos passos 4 e 6.\n",
    "# Caso esteja rodando diretamente este passo, garanta que 4 e 6 foram executados nesta sessão.\n",
    "\n",
    "def rouge_1_2(hypotheses, references):\n",
    "    from rouge_score import rouge_scorer\n",
    "    r1 = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    r2 = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "    r1_f = [r1.score(ref, hyp)['rouge1'].fmeasure for ref, hyp in zip(references, hypotheses)]\n",
    "    r2_f = [r2.score(ref, hyp)['rouge2'].fmeasure for ref, hyp in zip(references, hypotheses)]\n",
    "    return mean(r1_f), mean(r2_f)\n",
    "\n",
    "def meteor(hypotheses, references):\n",
    "    # Compatível com versões do NLTK que exigem entradas tokenizadas\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        try:\n",
    "            s = meteor_score([ref], hyp)  # algumas versões aceitam strings\n",
    "        except TypeError:\n",
    "            s = meteor_score([ref.split()], hyp.split())  # fallback: tokens\n",
    "        scores.append(s)\n",
    "    return mean(scores)\n",
    "\n",
    "def distinct_n(corpus, n=2):\n",
    "    # diversidade lexical: proporção de n-gramas únicos\n",
    "    ngrams = []\n",
    "    total = 0\n",
    "    for sent in corpus:\n",
    "        tokens = sent.split()\n",
    "        seq = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "        ngrams.extend(seq)\n",
    "        total += max(0, len(tokens) - n + 1)\n",
    "    unique = len(set(ngrams)) if ngrams else 0\n",
    "    return (unique / total) if total > 0 else 0.0\n",
    "\n",
    "def length_ratio(hypotheses, references):\n",
    "    # relação média de comprimento hyp/ref (em tokens)\n",
    "    import numpy as np\n",
    "    ratios = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        rlen = max(1, len(ref.split()))\n",
    "        hlen = len(hyp.split())\n",
    "        ratios.append(hlen / rlen)\n",
    "    return float(np.mean(ratios))\n",
    "\n",
    "def bootstrap_winrate(refs, base_hyps, ft_hyps, rounds=500, sample_size=50):\n",
    "    # sorteia subconjuntos e calcula taxa de vitórias do FT por ROUGE-Lsum\n",
    "    import random\n",
    "    from rouge_score import rouge_scorer\n",
    "    rng = random.Random(123)\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "    wins = 0\n",
    "    for _ in range(rounds):\n",
    "        idx = [rng.randrange(0, len(refs)) for __ in range(min(sample_size, len(refs)))]\n",
    "        base_scores = [scorer.score(refs[i], base_hyps[i])['rougeLsum'].fmeasure for i in idx]\n",
    "        ft_scores = [scorer.score(refs[i], ft_hyps[i])['rougeLsum'].fmeasure for i in idx]\n",
    "        if mean(ft_scores) > mean(base_scores):\n",
    "            wins += 1\n",
    "    return wins / rounds if rounds > 0 else 0.0\n",
    "\n",
    "print('Calculando métricas extras...')\n",
    "r1_base, r2_base = rouge_1_2(hyps_base_test, refs_test)\n",
    "r1_ft, r2_ft = rouge_1_2(hyps_ft, refs_test)\n",
    "meteor_base = meteor(hyps_base_test, refs_test)\n",
    "meteor_ft = meteor(hyps_ft, refs_test)\n",
    "\n",
    "P, R, F1_base = bert_score(hyps_base_test, refs_test, lang='pt', verbose=False)\n",
    "P, R, F1_ft = bert_score(hyps_ft, refs_test, lang='pt', verbose=False)\n",
    "bert_f1_base = float(F1_base.mean())\n",
    "bert_f1_ft = float(F1_ft.mean())\n",
    "\n",
    "distinct2_base = distinct_n(hyps_base_test, n=2)\n",
    "distinct2_ft = distinct_n(hyps_ft, n=2)\n",
    "len_ratio_base = length_ratio(hyps_base_test, refs_test)\n",
    "len_ratio_ft = length_ratio(hyps_ft, refs_test)\n",
    "winrate = bootstrap_winrate(refs_test, hyps_base_test, hyps_ft, rounds=300, sample_size=min(50, len(refs_test)))\n",
    "\n",
    "print('\\nMétricas adicionais (TEST):')\n",
    "print(f'ROUGE-1: base={r1_base:.3f} | ft={r1_ft:.3f}')\n",
    "print(f'ROUGE-2: base={r2_base:.3f} | ft={r2_ft:.3f}')\n",
    "print(f'METEOR : base={meteor_base:.3f} | ft={meteor_ft:.3f}')\n",
    "print(f'BERTScore-F1: base={bert_f1_base:.3f} | ft={bert_f1_ft:.3f}')\n",
    "print(f'Distinct-2: base={distinct2_base:.3f} | ft={distinct2_ft:.3f}')\n",
    "print(f'Length ratio: base={len_ratio_base:.3f} | ft={len_ratio_ft:.3f}')\n",
    "print(f'Bootstrap win-rate (ROUGE-Lsum): {winrate*100:.1f}% FT vence')\n",
    "\n",
    "# Persistir no artifacts\n",
    "extra = {\n",
    "    'rouge1_base': float(r1_base), 'rouge1_ft': float(r1_ft),\n",
    "    'rouge2_base': float(r2_base), 'rouge2_ft': float(r2_ft),\n",
    "    'meteor_base': float(meteor_base), 'meteor_ft': float(meteor_ft),\n",
    "    'bertscore_f1_base': float(bert_f1_base), 'bertscore_f1_ft': float(bert_f1_ft),\n",
    "    'distinct2_base': float(distinct2_base), 'distinct2_ft': float(distinct2_ft),\n",
    "    'length_ratio_base': float(len_ratio_base), 'length_ratio_ft': float(len_ratio_ft),\n",
    "    'bootstrap_winrate_rougeLsum': float(winrate)\n",
    "}\n",
    "with open(os.path.join(ART_DIR, 'oss_metrics_extra.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(extra, f, ensure_ascii=False, indent=2)\n",
    "print('Métricas extras salvas em:', ART_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808b456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c6efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f06e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89137774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42edbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a374f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a6f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c725666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11639e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ece841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7240a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
