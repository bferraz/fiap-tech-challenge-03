{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Parâmetros globais (edite aqui)\n",
    "import os\n",
    "\n",
    "# Caminhos\n",
    "DATA_PATH = os.getenv('DATA_PATH', '/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz')\n",
    "ART_DIR = os.getenv('ART_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/artifacts')\n",
    "OUTPUT_DIR = os.getenv('OSS_OUTPUT_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/tinyllama_amazon_finetuned')\n",
    "\n",
    "# Semente\n",
    "SEED = int(os.getenv('SEED', 42))\n",
    "\n",
    "# Leitura/Limpeza\n",
    "MAX_RECORDS = int(os.getenv('MAX_RECORDS_FOR_FT', 200_000))\n",
    "TITLE_MIN_LEN = int(os.getenv('TITLE_MIN_LEN', 2))\n",
    "CONTENT_MIN_LEN = int(os.getenv('CONTENT_MIN_LEN', 5))\n",
    "\n",
    "# Dataset/Splits\n",
    "TEST_SIZE = float(os.getenv('TEST_SIZE', 0.2))\n",
    "VAL_SIZE = float(os.getenv('VAL_SIZE', 0.5))\n",
    "\n",
    "# Modelo e geração\n",
    "BASE_OSS_MODEL = os.getenv('BASE_OSS_MODEL', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "OSS_MAX_SEQ_LEN = int(os.getenv('OSS_MAX_SEQ_LEN', 1024))\n",
    "OSS_SUBSET = int(os.getenv('OSS_SUBSET', 4000))\n",
    "OSS_EVAL_N = int(os.getenv('OSS_EVAL_N', 200))\n",
    "OSS_TEST_N = int(os.getenv('OSS_TEST_N', 200))\n",
    "GEN_MAX_NEW_TOKENS = int(os.getenv('GEN_MAX_NEW_TOKENS', 96))\n",
    "GEN_BATCH_SIZE = int(os.getenv('GEN_BATCH_SIZE', 8))\n",
    "GEN_DO_SAMPLE = bool(int(os.getenv('GEN_DO_SAMPLE', 1)))\n",
    "# test\n",
    "\n",
    "# Treino\n",
    "OSS_LR = float(os.getenv('OSS_LR', 2e-4))\n",
    "OSS_BATCH_SIZE = int(os.getenv('OSS_BATCH_SIZE', 4))\n",
    "OSS_GRAD_ACCUM = int(os.getenv('OSS_GRAD_ACCUM', 2))\n",
    "OSS_EPOCHS = int(os.getenv('OSS_EPOCHS', 2))\n",
    "OSS_WARMUP_STEPS = int(os.getenv('OSS_WARMUP_STEPS', 50))\n",
    "\n",
    "# LoRA\n",
    "LORA_R = int(os.getenv('LORA_R', 16))\n",
    "LORA_ALPHA = int(os.getenv('LORA_ALPHA', 16))\n",
    "LORA_DROPOUT = float(os.getenv('LORA_DROPOUT', 0.05))\n",
    "\n",
    "print('Parâmetros carregados:')\n",
    "print({k:v for k,v in globals().items() if k.isupper()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67223cff",
   "metadata": {},
   "source": [
    "# Tech Challenge (Colab GPU): Fine-tuning OSS com Unsloth + TinyLlama\n",
    "\n",
    "Pipeline completo para rodar no Google Colab com GPU (sem custo de API):\n",
    "\n",
    "1) Montar Google Drive e instalar dependências\n",
    "2) Carregar e limpar dados (title → content) a partir de `/content/drive/MyDrive/FineTunning/TechChallenge03/trn.json.gz`\n",
    "3) Preparar dataset para SFT (prompts + splits)\n",
    "4) Avaliação baseline (modelo base, sem FT)\n",
    "5) Treinamento LoRA com Unsloth + TinyLlama\n",
    "6) Avaliação pós-FT e gráficos\n",
    "7) Salvar artefatos (adapter, métricas, amostras) no Drive\n",
    "\n",
    "Observações:\n",
    "- Use GPU (Runtime > Change runtime type > T4/L4/A100).\n",
    "- bitsandbytes (4-bit) ajuda caber na VRAM.\n",
    "- Ajuste OSS_SUBSET/OSS_EVAL_N/OSS_EPOCHS conforme tempo e GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Montar Drive e instalar dependências (Colab)\n",
    "import sys, subprocess, os\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "PKGS = [\n",
    "    'unsloth>=2024.8.12',\n",
    "    'transformers>=4.43',\n",
    "    'accelerate>=0.33',\n",
    "    'datasets>=2.20',\n",
    "    'peft>=0.12.0',\n",
    "    'trl>=0.9.6',\n",
    "    'sacrebleu',\n",
    "    'rouge-score',\n",
    "    'matplotlib',\n",
    "    'tiktoken'\n",
    "]\n",
    "\n",
    "print('Instalando dependências...')\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', *PKGS])\n",
    "\n",
    "# bitsandbytes geralmente já existe no Colab; garantimos\n",
    "try:\n",
    "    import bitsandbytes as bnb  # type: ignore\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'bitsandbytes'])\n",
    "\n",
    "print('Setup concluído.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c53292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Carregar e limpar dados (title → content)\n",
    "import gzip, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f'Arquivo não encontrado: {DATA_PATH}'\n",
    "\n",
    "rows, count = [], 0\n",
    "with gzip.open(DATA_PATH, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        rows.append({'title': obj.get('title'), 'content': obj.get('content')})\n",
    "        count += 1\n",
    "        if count >= MAX_RECORDS:\n",
    "            break\n",
    "\n",
    "df_raw = pd.DataFrame(rows, columns=['title','content']).dropna(how='all')\n",
    "print('Lidas linhas:', len(df_raw))\n",
    "\n",
    "EMPTY_STRINGS = {'none','nan','null','na','n/a'}\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, float) and np.isnan(s):\n",
    "        return None\n",
    "    if isinstance(s, str) and s.strip().lower() in EMPTY_STRINGS:\n",
    "        return None\n",
    "    s = str(s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    s = re.sub(r'[`*_#>\\\"]', ' ', s)\n",
    "    s = s.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    s = re.sub(r'\\s+', ' ', s).strip(\" '\\\"\")\n",
    "    if not s or s.strip().lower() in EMPTY_STRINGS:\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "df = df_raw.copy()\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['title','content']).drop_duplicates(subset=['title','content'])\n",
    "# Filtros mínimos parametrizados\n",
    "before_len = len(df)\n",
    "df = df[df['title'].str.len() >= TITLE_MIN_LEN]\n",
    "df = df[df['content'].str.len() >= CONTENT_MIN_LEN]\n",
    "print(f'Após limpeza: {len(df)} (removidos {before-len(df)}; por tamanho {before_len-len(df)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Preparar dataset para SFT (prompts + splits)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "SYSTEM_PROMPT = os.getenv('OSS_SYSTEM_PROMPT', 'Você é um assistente que gera descrições detalhadas de produtos a partir do título.')\n",
    "PROMPT_COL = 'prompt'\n",
    "TARGET_COL = 'target'\n",
    "\n",
    "df_prep = df[['title','content']].copy()\n",
    "df_prep[PROMPT_COL] = df_prep['title'].apply(lambda t: f\"[SYSTEM]\\n{SYSTEM_PROMPT}\\n[USER]\\nTítulo: {t}\\n[ASSISTANT]\\n\")\n",
    "df_prep[TARGET_COL] = df_prep['content']\n",
    "\n",
    "test_size = float(os.getenv('TEST_SIZE', 0.2))\n",
    "val_size = float(os.getenv('VAL_SIZE', 0.5))\n",
    "\n",
    "train_df, temp_df = train_test_split(df_prep[[PROMPT_COL,TARGET_COL]], test_size=test_size, random_state=SEED, shuffle=True)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=SEED, shuffle=True)\n",
    "\n",
    "print(f\"Splits → train={len(train_df):,}, val={len(val_df):,}, test={len(test_df):,}\")\n",
    "\n",
    "datasets_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    'validation': Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    'test': Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfeca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Carregar modelo base (Unsloth + TinyLlama) e avaliação baseline (val e teste) — com geração em lote\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "MAX_SEQ_LEN = OSS_MAX_SEQ_LEN\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "print('Dispositivo:', device)\n",
    "\n",
    "print('Carregando modelo base...')\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_OSS_MODEL,\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True if device == 'cuda' else False,\n",
    ")\n",
    "\n",
    "# Subconjuntos para acelerar\n",
    "if datasets_dict['train'].num_rows > OSS_SUBSET:\n",
    "    datasets_dict['train'] = datasets_dict['train'].shuffle(seed=SEED).select(range(OSS_SUBSET))\n",
    "subset_val = datasets_dict['validation'].select(range(min(OSS_EVAL_N, datasets_dict['validation'].num_rows)))\n",
    "subset_test = datasets_dict['test'].select(range(min(OSS_TEST_N, datasets_dict['test'].num_rows)))\n",
    "\n",
    "model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Geração batelada e remoção de prompt precisa via input_length\n",
    "@torch.no_grad()\n",
    "def generate_batch(prompts, max_new_tokens=GEN_MAX_NEW_TOKENS, do_sample=GEN_DO_SAMPLE, temperature=0.7, top_p=0.9):\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=MAX_SEQ_LEN).to(model.device)\n",
    "    input_lengths = (inputs['input_ids'] != tokenizer.pad_token_id).sum(dim=1)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # Remover exatamente os tokens do prompt por amostra\n",
    "    result = []\n",
    "    for i, txt in enumerate(texts):\n",
    "        prompt_text = tokenizer.decode(outputs[i][:input_lengths[i]], skip_special_tokens=True)\n",
    "        result.append(txt[len(prompt_text):].strip())\n",
    "    return result\n",
    "\n",
    "# Helper para avaliação batelada\n",
    "def eval_dataset(ds, batch_size=GEN_BATCH_SIZE):\n",
    "    refs, hyps = [], []\n",
    "    n = ds.num_rows\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = ds.select(range(i, min(i+batch_size, n)))\n",
    "        prompts = [ex['prompt'] for ex in batch]\n",
    "        hyps.extend(generate_batch(prompts))\n",
    "        refs.extend([ex['target'].strip() for ex in batch])\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "    rougeL = float(np.mean([scorer.score(r, h)['rougeLsum'].fmeasure for r, h in zip(refs, hyps)]))\n",
    "    return bleu, rougeL, refs, hyps\n",
    "\n",
    "# Baseline (val e test)\n",
    "bleu_base_val, rougeL_base_val, refs_val, hyps_base_val = eval_dataset(subset_val)\n",
    "print(f'Baseline (val) → BLEU={bleu_base_val:.2f} | ROUGE-L={rougeL_base_val:.3f}')\n",
    "bleu_base_test, rougeL_base_test, refs_test_base, hyps_base_test = eval_dataset(subset_test)\n",
    "print(f'Baseline (test) → BLEU={bleu_base_test:.2f} | ROUGE-L={rougeL_base_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Treinamento LoRA (Unsloth + TRL SFTTrainer)\n",
    "from transformers import TrainingArguments, __version__ as HF_VER\n",
    "from trl import SFTTrainer\n",
    "import inspect\n",
    "\n",
    "LORA_R = int(os.getenv('LORA_R', 16))\n",
    "LORA_ALPHA = int(os.getenv('LORA_ALPHA', 16))\n",
    "LORA_DROPOUT = float(os.getenv('LORA_DROPOUT', 0.05))\n",
    "LR = float(os.getenv('OSS_LR', 2e-4))\n",
    "BATCH_SIZE = int(os.getenv('OSS_BATCH_SIZE', 4))\n",
    "GR_ACCUM = int(os.getenv('OSS_GRAD_ACCUM', 2))\n",
    "EPOCHS = int(os.getenv('OSS_EPOCHS', 2))\n",
    "WARMUP = int(os.getenv('OSS_WARMUP_STEPS', 50))\n",
    "OUTPUT_DIR = os.getenv('OSS_OUTPUT_DIR', '/content/drive/MyDrive/FineTunning/TechChallenge03/tinyllama_amazon_finetuned')\n",
    "\n",
    "# Aplicar LoRA (com Unsloth)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Preparar dataset tokenizado\n",
    "\n",
    "def format_row(ex):\n",
    "    return {'text': ex['prompt'] + ex['target']}\n",
    "\n",
    "train_ds = datasets_dict['train'].map(format_row, remove_columns=datasets_dict['train'].column_names)\n",
    "val_ds = datasets_dict['validation'].map(format_row, remove_columns=datasets_dict['validation'].column_names)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=['text'])\n",
    "val_tok = val_ds.map(tokenize, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Compatibilidade Transformers v4/v5 via inspeção da assinatura\n",
    "args_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GR_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_steps=WARMUP,\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "sig_params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
    "if 'eval_strategy' in sig_params:\n",
    "    args_kwargs['eval_strategy'] = 'steps'\n",
    "elif 'evaluation_strategy' in sig_params:\n",
    "    args_kwargs['evaluation_strategy'] = 'steps'\n",
    "else:\n",
    "    print('Aviso: parâmetro de avaliação por estratégia não encontrado; usaremos eval_steps para controlar a frequência.')\n",
    "\n",
    "if 'save_strategy' in sig_params:\n",
    "    args_kwargs['save_strategy'] = 'steps'\n",
    "\n",
    "args = TrainingArguments(**args_kwargs)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    args=args,\n",
    "    packing=True,\n",
    "    dataset_text_field=None\n",
    ")\n",
    "\n",
    "print('Treinando...')\n",
    "trainer.train()\n",
    "print('Treino concluído.')\n",
    "\n",
    "# Salvar adapter\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print('Adapter salvo em:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8182e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Avaliação pós-FT e gráficos + salvamento de métricas (comparando com baseline de teste) — batelada\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reutiliza subset_test e generate_batch/GEN_BATCH_SIZE definidos no passo 4\n",
    "model.eval()\n",
    "\n",
    "def eval_dataset_after_ft(ds, batch_size=GEN_BATCH_SIZE):\n",
    "    refs, hyps = [], []\n",
    "    n = ds.num_rows\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = ds.select(range(i, min(i+batch_size, n)))\n",
    "        prompts = [ex['prompt'] for ex in batch]\n",
    "        hyps.extend(generate_batch(prompts))\n",
    "        refs.extend([ex['target'].strip() for ex in batch])\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=True)\n",
    "    rougeL = float(np.mean([scorer.score(r, h)['rougeLsum'].fmeasure for r, h in zip(refs, hyps)]))\n",
    "    return bleu, rougeL, refs, hyps\n",
    "\n",
    "bleu_ft, rougeL_ft, refs_test, hyps_ft = eval_dataset_after_ft(subset_test)\n",
    "\n",
    "print('\\nResumo comparativo (TEST):')\n",
    "print(f\"Baseline → BLEU={bleu_base_test:.2f} | ROUGE-L={rougeL_base_test:.3f}\")\n",
    "print(f\"Fine-tuned → BLEU={bleu_ft:.2f} | ROUGE-L={rougeL_ft:.3f}\")\n",
    "\n",
    "labels = ['BLEU','ROUGE-L']\n",
    "baseline_vals = [bleu_base_test, rougeL_base_test]\n",
    "ft_vals = [bleu_ft, rougeL_ft]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, baseline_vals, width, label='Baseline (test)')\n",
    "plt.bar(x + width/2, ft_vals, width, label='Fine-tuned (test)')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparação de métricas no conjunto de teste')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Salvar métricas e amostras no Drive\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(ART_DIR, 'oss_metrics.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'bleu_base_val': float(bleu_base_val),\n",
    "        'rougeL_base_val': float(rougeL_base_val),\n",
    "        'bleu_base_test': float(bleu_base_test),\n",
    "        'rougeL_base_test': float(rougeL_base_test),\n",
    "        'bleu_ft_test': float(bleu_ft),\n",
    "        'rougeL_ft_test': float(rougeL_ft)\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "samples_path = os.path.join(ART_DIR, 'qualitative_samples_test.jsonl')\n",
    "with open(samples_path, 'w', encoding='utf-8') as f:\n",
    "    for ex, r_base, r_ft in zip(subset_test, hyps_base_test, hyps_ft):\n",
    "        f.write(json.dumps({'prompt': ex['prompt'], 'hyp_base': r_base, 'hyp_ft': r_ft, 'ref': ex['target']}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Artefatos salvos em:', ART_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808b456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c6efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f06e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89137774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42edbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a374f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a6f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c725666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11639e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ece841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7240a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
