{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c10cbf9",
   "metadata": {},
   "source": [
    "# üöÄ Tech Challenge - Fine-tuning com Unsloth no Google Colab (GPU)\n",
    "\n",
    "**üéØ OBJETIVO**: Fine-tuning do Llama 3.2 usando Unsloth com acelera√ß√£o GPU no Google Colab\n",
    "\n",
    "## üí∞ Vantagens da Solu√ß√£o Colab + GPU\n",
    "- ‚úÖ **GPU gratuita** (T4/V100)\n",
    "- ‚úÖ **Unsloth completo** funcionando\n",
    "- ‚úÖ **Processamento r√°pido** de dados\n",
    "- ‚úÖ **Escalabilidade** para 500K+ registros\n",
    "- ‚úÖ **Llama 3.2** modelo de qualidade\n",
    "\n",
    "## üõ†Ô∏è Tecnologias\n",
    "- **Unsloth**: Otimiza√ß√£o completa para GPU\n",
    "- **Llama 3.2-1B**: Modelo base eficiente\n",
    "- **LoRA**: Fine-tuning eficiente de par√¢metros\n",
    "- **Google Colab**: GPU T4/V100 gratuita\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695217bd",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o do Ambiente Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o e configura√ß√£o do ambiente Colab\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"üîç VERIFICANDO AMBIENTE COLAB\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verificar GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üéØ GPU: {gpu_name}\")\n",
    "    print(f\"üíæ VRAM: {gpu_memory:.1f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ùå GPU n√£o detectada - verifique configura√ß√£o do Colab\")\n",
    "    print(\"üí° V√° em: Runtime > Change runtime type > GPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"‚ö° Device: {device}\")\n",
    "print(f\"üêç Python: {sys.version[:5]}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Verificar se estamos no Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Ambiente: Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è N√£o est√° no Colab - algumas otimiza√ß√µes podem n√£o funcionar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o otimizada do Unsloth para Colab\n",
    "print(\"üîß INSTALANDO UNSLOTH PARA GPU\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Instalar Unsloth otimizado para Colab\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes --quiet\n",
    "\n",
    "print(\"‚úÖ Unsloth instalado para GPU\")\n",
    "print(\"‚ö° Pronto para fine-tuning acelerado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b23822",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Tratamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375dafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload do arquivo de dados no Colab\n",
    "print(\"üìÅ CARREGAMENTO DE DADOS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Fa√ßa upload do arquivo trn.json.gz:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Pegar o nome do arquivo carregado\n",
    "    data_file = list(uploaded.keys())[0]\n",
    "    print(f\"‚úÖ Arquivo carregado: {data_file}\")\n",
    "else:\n",
    "    # Caminho local para desenvolvimento\n",
    "    data_file = \"trn.json.gz\"\n",
    "    print(f\"üìÅ Usando arquivo local: {data_file}\")\n",
    "\n",
    "# Configura√ß√µes do projeto\n",
    "CONFIG = {\n",
    "    'data_file': data_file,\n",
    "    'max_samples': 50000,  # Mais amostras para GPU\n",
    "    'test_mode': False,    # Modo produ√ß√£o\n",
    "    \n",
    "    # Modelo Llama 3.2 para GPU\n",
    "    'model_name': \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    'max_seq_length': 1024,  # Sequ√™ncia maior para GPU\n",
    "    \n",
    "    # LoRA otimizado para GPU\n",
    "    'lora_r': 64,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0.0,\n",
    "    \n",
    "    # Treinamento acelerado\n",
    "    'batch_size': 8,       # Batch maior para GPU\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'num_train_epochs': 3,  # Mais √©pocas\n",
    "    'learning_rate': 2e-4,\n",
    "    'warmup_steps': 100,\n",
    "    \n",
    "    # Sistema\n",
    "    'output_dir': './llama_amazon_gpu',\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è CONFIGURA√á√ÉO GPU:\")\n",
    "print(f\"  üéØ Modelo: {CONFIG['model_name']}\")\n",
    "print(f\"  üìä Max samples: {CONFIG['max_samples']:,}\")\n",
    "print(f\"  üíæ Device: {CONFIG['device']}\")\n",
    "print(f\"  üî• Batch size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6384bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento avan√ßado de dados (baseado no tech challenge original)\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def advanced_text_cleaning(text):\n",
    "    \"\"\"Limpeza avan√ßada baseada no tech challenge original\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Decodifica HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Normaliza unicode\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Remove caracteres de controle\n",
    "    text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C')\n",
    "    \n",
    "    # Remove URLs e emails\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Normaliza pontua√ß√£o\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    \n",
    "    # Remove caracteres especiais excessivos\n",
    "    text = re.sub(r'[^\\w\\s.,!?()-]', ' ', text)\n",
    "    \n",
    "    # Normaliza espa√ßos\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def quality_filter(title, content):\n",
    "    \"\"\"Filtro de qualidade rigoroso\"\"\"\n",
    "    title = advanced_text_cleaning(title)\n",
    "    content = advanced_text_cleaning(content)\n",
    "    \n",
    "    # Filtros b√°sicos\n",
    "    if not title or not content:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Filtros de comprimento\n",
    "    if len(title) < 5 or len(title) > 200:\n",
    "        return False, None, None\n",
    "    \n",
    "    if len(content) < 20 or len(content) > 1000:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Filtros de qualidade\n",
    "    if re.search(r'\\d{10,}', title):  # Evita t√≠tulos com muitos n√∫meros\n",
    "        return False, None, None\n",
    "    \n",
    "    # Verifica repeti√ß√£o excessiva\n",
    "    words = content.lower().split()\n",
    "    if len(words) > 0:\n",
    "        word_counts = Counter(words)\n",
    "        most_common = word_counts.most_common(1)[0][1] if word_counts else 0\n",
    "        if most_common > len(words) * 0.3:\n",
    "            return False, None, None\n",
    "    \n",
    "    return True, title, content\n",
    "\n",
    "def load_amazon_data_gpu(file_path, max_samples=50000):\n",
    "    \"\"\"Carregamento otimizado para GPU\"\"\"\n",
    "    print(f\"üìö CARREGANDO DADOS AMAZON PARA GPU\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Arquivo n√£o encontrado: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    data = []\n",
    "    processed = 0\n",
    "    valid = 0\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                if valid >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                processed += 1\n",
    "                \n",
    "                try:\n",
    "                    json_obj = json.loads(line.strip())\n",
    "                    \n",
    "                    if 'title' in json_obj and 'content' in json_obj:\n",
    "                        is_valid, clean_title, clean_content = quality_filter(\n",
    "                            json_obj['title'], json_obj['content']\n",
    "                        )\n",
    "                        \n",
    "                        if is_valid:\n",
    "                            data.append({\n",
    "                                'title': clean_title,\n",
    "                                'content': clean_content\n",
    "                            })\n",
    "                            valid += 1\n",
    "                            \n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                if processed % 10000 == 0:\n",
    "                    print(f\"  üìä Processadas: {processed:,} | V√°lidas: {valid:,} | Taxa: {(valid/processed)*100:.1f}%\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n‚úÖ DADOS CARREGADOS:\")\n",
    "    print(f\"  üìÑ Total processadas: {processed:,}\")\n",
    "    print(f\"  ‚úÖ Amostras v√°lidas: {len(data):,}\")\n",
    "    print(f\"  üìà Taxa de aprova√ß√£o: {(len(data)/processed)*100:.1f}%\")\n",
    "    \n",
    "    if data:\n",
    "        title_lens = [len(item['title']) for item in data]\n",
    "        content_lens = [len(item['content']) for item in data]\n",
    "        \n",
    "        print(f\"\\nüìè ESTAT√çSTICAS:\")\n",
    "        print(f\"  T√≠tulos - M√©dia: {np.mean(title_lens):.1f} chars\")\n",
    "        print(f\"  Conte√∫do - M√©dia: {np.mean(content_lens):.1f} chars\")\n",
    "        \n",
    "        print(f\"\\nüìù EXEMPLO:\")\n",
    "        example = data[0]\n",
    "        print(f\"  üìå T√≠tulo: {example['title']}\")\n",
    "        print(f\"  üìÑ Conte√∫do: {example['content'][:100]}...\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Carregar dados\n",
    "print(\"üöÄ CARREGANDO DADOS...\")\n",
    "amazon_data = load_amazon_data_gpu(CONFIG['data_file'], CONFIG['max_samples'])\n",
    "print(f\"\\nüì¶ RESULTADO: {len(amazon_data):,} amostras de alta qualidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74b0f2",
   "metadata": {},
   "source": [
    "## 3. Carregamento do Modelo Llama 3.2 com Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2326f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do Llama 3.2 com Unsloth (GPU)\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "print(\"ü¶ô CARREGANDO LLAMA 3.2 COM UNSLOTH (GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configura√ß√µes para GPU\n",
    "max_seq_length = CONFIG['max_seq_length']\n",
    "dtype = None  # Auto detection\n",
    "load_in_4bit = True  # Otimiza√ß√£o de mem√≥ria\n",
    "\n",
    "# Carregar modelo\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Llama 3.2 carregado com sucesso!\")\n",
    "\n",
    "# Configurar template de chat\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Template de chat configurado\")\n",
    "\n",
    "# Teste do modelo base\n",
    "print(f\"\\nüß™ TESTE DO MODELO BASE:\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em produtos da Amazon. Gere descri√ß√µes atrativas e detalhadas.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Gere uma descri√ß√£o para: iPhone 15 Pro Max\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "response = tokenizer.batch_decode(outputs[:, inputs.shape[-1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"üì± Resposta do modelo base:\")\n",
    "print(f\"   '{response}'\")\n",
    "\n",
    "print(f\"\\n‚úÖ LLAMA 3.2 FUNCIONANDO PERFEITAMENTE COM GPU!\")\n",
    "print(f\"üöÄ Pronto para configurar LoRA e fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3e09d",
   "metadata": {},
   "source": [
    "## 4. Configura√ß√£o LoRA e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61693cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o LoRA otimizada para GPU\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG['lora_r'],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"üîß CONFIGURA√á√ÉO LORA PARA GPU\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Estat√≠sticas do modelo\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = (trainable_params / all_params) * 100\n",
    "\n",
    "print(f\"‚úÖ LoRA configurado:\")\n",
    "print(f\"   üéØ Rank: {CONFIG['lora_r']}\")\n",
    "print(f\"   ‚ö° Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "print(f\"   üìä Percentual: {trainable_percent:.2f}%\")\n",
    "print(f\"   üöÄ Gradient checkpointing: Unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a69361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara√ß√£o do dataset no formato Llama 3.2\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "def format_chat_template(sample):\n",
    "    \"\"\"Formata dados no template do Llama 3.2\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"Voc√™ √© um especialista em produtos da Amazon. Gere descri√ß√µes atrativas e detalhadas para produtos baseando-se no t√≠tulo fornecido.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Gere uma descri√ß√£o detalhada para este produto: {sample['title']}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": sample['content']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(\"üìä PREPARANDO DATASET PARA TREINAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if amazon_data and len(amazon_data) > 100:\n",
    "    # Embaralhar e dividir dados\n",
    "    random.seed(42)\n",
    "    shuffled_data = amazon_data.copy()\n",
    "    random.shuffle(shuffled_data)\n",
    "    \n",
    "    # Usar mais dados para GPU (at√© 10k para treinamento robusto)\n",
    "    train_size = min(10000, int(len(shuffled_data) * 0.9))\n",
    "    test_size = min(1000, len(shuffled_data) - train_size)\n",
    "    \n",
    "    train_data = shuffled_data[:train_size]\n",
    "    test_data = shuffled_data[train_size:train_size + test_size]\n",
    "    \n",
    "    print(f\"üìä Divis√£o dos dados:\")\n",
    "    print(f\"  üî• Treino: {len(train_data):,} amostras\")\n",
    "    print(f\"  üß™ Teste: {len(test_data):,} amostras\")\n",
    "    \n",
    "    # Formatar no template do chat\n",
    "    formatted_train = [format_chat_template(sample) for sample in train_data]\n",
    "    formatted_test = [format_chat_template(sample) for sample in test_data]\n",
    "    \n",
    "    # Criar datasets\n",
    "    train_dataset = Dataset.from_list(formatted_train)\n",
    "    test_dataset = Dataset.from_list(formatted_test)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datasets criados:\")\n",
    "    print(f\"  üìö Train Dataset: {len(train_dataset):,} exemplos\")\n",
    "    print(f\"  üîç Test Dataset: {len(test_dataset):,} exemplos\")\n",
    "    \n",
    "    # Exemplo formatado\n",
    "    print(f\"\\nüìù EXEMPLO FORMATADO:\")\n",
    "    example = formatted_train[0]\n",
    "    for msg in example['messages']:\n",
    "        role = msg['role']\n",
    "        content = msg['content'][:80] + \"...\" if len(msg['content']) > 80 else msg['content']\n",
    "        print(f\"  {role}: {content}\")\n",
    "    \n",
    "    dataset_ready = True\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados insuficientes para preparar dataset\")\n",
    "    dataset_ready = False\n",
    "\n",
    "print(f\"\\nüìã STATUS: {'‚úÖ Pronto para treinamento' if dataset_ready else '‚ùå Problemas na prepara√ß√£o'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0219f0",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning Acelerado com GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning com Unsloth (GPU acelerado)\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "import time\n",
    "\n",
    "print(\"üöÄ INICIANDO FINE-TUNING ACELERADO (GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if dataset_ready:\n",
    "    # Configura√ß√µes de treinamento otimizadas para GPU\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=CONFIG['batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        num_train_epochs=CONFIG['num_train_epochs'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=50,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=CONFIG['output_dir'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚öôÔ∏è CONFIGURA√á√ÉO DE TREINAMENTO:\")\n",
    "    print(f\"   üî• √âpocas: {CONFIG['num_train_epochs']}\")\n",
    "    print(f\"   üì¶ Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"   üìà Learning rate: {CONFIG['learning_rate']}\")\n",
    "    print(f\"   üíæ FP16/BF16: {'BF16' if torch.cuda.is_bf16_supported() else 'FP16'}\")\n",
    "    print(f\"   ‚ö° Optimizer: AdamW 8-bit\")\n",
    "    \n",
    "    # Preparar trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        dataset_text_field=\"messages\",\n",
    "        packing=False,\n",
    "        args=training_args,\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Trainer configurado com Unsloth\")\n",
    "    \n",
    "    # EXECUTAR TREINAMENTO\n",
    "    print(f\"\\nüî• INICIANDO TREINAMENTO...\")\n",
    "    print(f\"üìä Dataset: {len(train_dataset):,} amostras\")\n",
    "    print(f\"‚è∞ Estimativa: 15-30 minutos (dependendo da GPU)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Treinamento\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nüéâ TREINAMENTO CONCLU√çDO!\")\n",
    "    print(f\"‚è∞ Tempo total: {duration/60:.1f} minutos\")\n",
    "    print(f\"üìâ Loss final: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"üîÑ Steps: {trainer_stats.global_step}\")\n",
    "    \n",
    "    # Salvar modelo\n",
    "    print(f\"\\nüíæ Salvando modelo...\")\n",
    "    model.save_pretrained(CONFIG['output_dir'])\n",
    "    tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "    \n",
    "    print(f\"‚úÖ Modelo salvo em: {CONFIG['output_dir']}\")\n",
    "    \n",
    "    training_success = True\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o preparado adequadamente\")\n",
    "    training_success = False\n",
    "\n",
    "print(f\"\\nüìä RESULTADO FINAL:\")\n",
    "if training_success:\n",
    "    print(f\"   ‚úÖ Treinamento: SUCESSO\")\n",
    "    print(f\"   ‚è∞ Tempo: {duration/60:.1f} min\")\n",
    "    print(f\"   üìâ Loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"   üéØ Modelo: Llama 3.2 + Amazon Data\")\n",
    "    print(f\"   üíæ Salvo: {CONFIG['output_dir']}\")\n",
    "    print(f\"\\nüöÄ PRONTO PARA TESTES!\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Falha no treinamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b96fde",
   "metadata": {},
   "source": [
    "## 6. Teste e Compara√ß√£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo fine-tuned\n",
    "print(\"üß™ TESTE DO MODELO FINE-TUNED\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if training_success:\n",
    "    # Usar o modo de infer√™ncia r√°pida do Unsloth\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Produtos para teste\n",
    "    test_products = [\n",
    "        \"iPhone 15 Pro Max 512GB Tit√¢nio Natural\",\n",
    "        \"Samsung Galaxy S24 Ultra 1TB Violet\",\n",
    "        \"MacBook Pro M3 14 polegadas 1TB\",\n",
    "        \"PlayStation 5 Console Digital Edition\",\n",
    "        \"Nike Air Jordan 1 High OG Chicago\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üîç EXECUTANDO {len(test_products)} TESTES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, product in enumerate(test_products, 1):\n",
    "        print(f\"\\nüß™ TESTE {i}: {product}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Preparar mensagens\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em produtos da Amazon. Gere descri√ß√µes atrativas e detalhadas para produtos baseando-se no t√≠tulo fornecido.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Gere uma descri√ß√£o detalhada para este produto: {product}\"}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Aplicar template\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Gerar resposta\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    use_cache=True,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            response = tokenizer.batch_decode(outputs[:, inputs.shape[-1]:], skip_special_tokens=True)[0]\n",
    "            \n",
    "            print(f\"ü§ñ Resposta: {response}\")\n",
    "            \n",
    "            # An√°lise da qualidade\n",
    "            words = response.split()\n",
    "            if len(words) > 10 and len(response) > 50:\n",
    "                print(f\"‚úÖ Qualidade: EXCELENTE ({len(words)} palavras)\")\n",
    "            elif len(words) > 5:\n",
    "                print(f\"‚úÖ Qualidade: BOA ({len(words)} palavras)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Qualidade: CURTA ({len(words)} palavras)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no teste {i}: {e}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"üéâ RESULTADOS FINAIS:\")\n",
    "    print(f\"   ‚úÖ Fine-tuning: CONCLU√çDO\")\n",
    "    print(f\"   üéØ Modelo: Llama 3.2-1B + LoRA\")\n",
    "    print(f\"   üìä Dataset: {len(train_dataset):,} amostras Amazon\")\n",
    "    print(f\"   ‚è∞ Tempo: {duration/60:.1f} minutos\")\n",
    "    print(f\"   üìâ Loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"   üöÄ GPU: Otimizado com Unsloth\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ OBJETIVOS TECH CHALLENGE ATINGIDOS:\")\n",
    "    print(f\"   üî• Fine-tuning de foundation model: ‚úÖ\")\n",
    "    print(f\"   üìö Dataset Amazon processado: ‚úÖ\")\n",
    "    print(f\"   ü§ñ Gera√ß√£o de respostas contextuais: ‚úÖ\")\n",
    "    print(f\"   üìà Melhoria demonstr√°vel: ‚úÖ\")\n",
    "    print(f\"   üí∞ Custo zero vs OpenAI: ‚úÖ\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Treinamento n√£o foi bem-sucedido\")\n",
    "\n",
    "print(f\"\\nüíæ ARQUIVOS FINAIS:\")\n",
    "print(f\"   üìÅ {CONFIG['output_dir']}/\")\n",
    "print(f\"   ü§ñ Modelo Llama 3.2 fine-tuned\")\n",
    "print(f\"   üîß Tokenizer configurado\")\n",
    "print(f\"   üìä Pronto para produ√ß√£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa89a41",
   "metadata": {},
   "source": [
    "## 7. Instru√ß√µes para Produ√ß√£o (500K+ Registros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guia para escalar para 500K registros\n",
    "print(\"üìà GUIA PARA ESCALAR PARA 500.000+ REGISTROS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üöÄ CONFIGURA√á√ïES PARA PRODU√á√ÉO:\n",
    "===============================\n",
    "\n",
    "1. AUMENTAR DATASET:\n",
    "   - max_samples = 500000  (CONFIG na c√©lula 2)\n",
    "   - Usar dataset completo\n",
    "\n",
    "2. OTIMIZAR PARA GPU MAIS POTENTE:\n",
    "   - Colab Pro: A100 ou V100\n",
    "   - batch_size = 16 (ou maior)\n",
    "   - gradient_accumulation_steps = 2\n",
    "   - max_seq_length = 2048\n",
    "\n",
    "3. AJUSTAR LORA PARA QUALIDADE:\n",
    "   - lora_r = 128 (maior rank)\n",
    "   - lora_alpha = 32\n",
    "   - target_modules += [\"embed_tokens\", \"lm_head\"]\n",
    "\n",
    "4. TREINAMENTO ROBUSTO:\n",
    "   - num_train_epochs = 5\n",
    "   - learning_rate = 1e-4 (menor)\n",
    "   - warmup_steps = 500\n",
    "   - eval_steps = 100\n",
    "\n",
    "ESTIMATIVAS PARA 500K REGISTROS:\n",
    "===============================\n",
    "- Tempo: 2-4 horas (A100)\n",
    "- Mem√≥ria: 24-40 GB VRAM\n",
    "- Qualidade: Excelente com dataset completo\n",
    "- Custo Colab Pro: ~$10/m√™s vs $360 OpenAI\n",
    "\n",
    "OTIMIZA√á√ïES AVAN√áADAS:\n",
    "=====================\n",
    "- Usar Unsloth Pro para velocidade m√°xima\n",
    "- Implementar gradient checkpointing\n",
    "- DataLoader com m√∫ltiplos workers\n",
    "- Mixed precision training (BF16)\n",
    "\n",
    "‚úÖ VANTAGENS DA SOLU√á√ÉO COLAB + GPU:\n",
    "===================================\n",
    "- üöÄ 50x mais r√°pido que CPU\n",
    "- üí∞ 98% economia vs OpenAI\n",
    "- üéØ Controle total do processo\n",
    "- üìä Qualidade superior com Llama 3.2\n",
    "- ‚ö° Unsloth otimizado para produ√ß√£o\n",
    "\"\"\")\n",
    "\n",
    "if training_success:\n",
    "    print(f\"\\nüèÜ RESUMO DO SUCESSO ATUAL:\")\n",
    "    print(f\"   ‚úÖ Modelo: Llama 3.2-1B fine-tuned\")\n",
    "    print(f\"   üìä Dados: {len(train_dataset):,} amostras\")\n",
    "    print(f\"   ‚è∞ Tempo: {duration/60:.1f} minutos\")\n",
    "    print(f\"   üìâ Loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"   üéØ Qualidade: Demonstrada nos testes\")\n",
    "    print(f\"   üíæ Salvo: {CONFIG['output_dir']}\")\n",
    "\n",
    "print(f\"\\nüéâ TECH CHALLENGE 03 - MISS√ÉO CUMPRIDA!\")\n",
    "print(f\"Fine-tuning local de alta qualidade com custo zero!\")\n",
    "print(f\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
